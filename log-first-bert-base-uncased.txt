2019-02-12 17:48:39,723 : ********************************************************************************
2019-02-12 17:48:39,723 : ********************************************************************************
2019-02-12 17:48:39,723 : ********************************************************************************
2019-02-12 17:48:39,723 : layer 0
2019-02-12 17:48:39,724 : ********************************************************************************
2019-02-12 17:48:39,724 : ********************************************************************************
2019-02-12 17:48:39,724 : ********************************************************************************
2019-02-12 17:48:39,724 : ***** Transfer task : STS12 *****


2019-02-12 17:48:39,758 : loading BERT mode bert-base-uncased
2019-02-12 17:48:39,759 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:48:39,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:48:39,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq27sa3jk
2019-02-12 17:48:42,186 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:48:48,462 : MSRpar : pearson = nan, spearman = nan
2019-02-12 17:48:49,392 : MSRvid : pearson = nan, spearman = nan
2019-02-12 17:48:50,134 : SMTeuroparl : pearson = nan, spearman = nan
2019-02-12 17:48:51,448 : surprise.OnWN : pearson = nan, spearman = nan
2019-02-12 17:48:52,168 : surprise.SMTnews : pearson = nan, spearman = nan
2019-02-12 17:48:52,168 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-12 17:48:52,168 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-12 17:48:52,168 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 17:48:52,177 : loading BERT mode bert-base-uncased
2019-02-12 17:48:52,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:48:52,195 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:48:52,195 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphfvitof6
2019-02-12 17:48:54,595 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:48:56,764 : FNWN : pearson = nan, spearman = nan
2019-02-12 17:48:57,804 : headlines : pearson = nan, spearman = nan
2019-02-12 17:48:58,587 : OnWN : pearson = nan, spearman = nan
2019-02-12 17:48:58,587 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-12 17:48:58,587 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-12 17:48:58,588 : ***** Transfer task : STS14 *****


2019-02-12 17:48:58,606 : loading BERT mode bert-base-uncased
2019-02-12 17:48:58,606 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:48:58,655 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:48:58,655 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpex0nsj1u
2019-02-12 17:49:01,055 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:49:03,150 : deft-forum : pearson = nan, spearman = nan
2019-02-12 17:49:04,030 : deft-news : pearson = nan, spearman = nan
2019-02-12 17:49:05,674 : headlines : pearson = nan, spearman = nan
2019-02-12 17:49:07,476 : images : pearson = nan, spearman = nan
2019-02-12 17:49:09,431 : OnWN : pearson = nan, spearman = nan
2019-02-12 17:49:11,920 : tweet-news : pearson = nan, spearman = nan
2019-02-12 17:49:11,920 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-12 17:49:11,920 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-12 17:49:11,920 : ***** Transfer task : STS15 *****


2019-02-12 17:49:11,954 : loading BERT mode bert-base-uncased
2019-02-12 17:49:11,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:49:11,971 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:49:11,972 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_uym24ir
2019-02-12 17:49:14,309 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:49:17,043 : answers-forums : pearson = nan, spearman = nan
2019-02-12 17:49:18,545 : answers-students : pearson = nan, spearman = nan
2019-02-12 17:49:19,472 : belief : pearson = nan, spearman = nan
2019-02-12 17:49:20,646 : headlines : pearson = nan, spearman = nan
2019-02-12 17:49:21,775 : images : pearson = nan, spearman = nan
2019-02-12 17:49:21,775 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-12 17:49:21,775 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-12 17:49:21,775 : ***** Transfer task : STS16 *****


2019-02-12 17:49:21,845 : loading BERT mode bert-base-uncased
2019-02-12 17:49:21,845 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:49:21,862 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:49:21,862 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbmw3napt
2019-02-12 17:49:24,255 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:49:25,962 : answer-answer : pearson = nan, spearman = nan
2019-02-12 17:49:26,224 : headlines : pearson = nan, spearman = nan
2019-02-12 17:49:26,560 : plagiarism : pearson = nan, spearman = nan
2019-02-12 17:49:27,119 : postediting : pearson = nan, spearman = nan
2019-02-12 17:49:27,518 : question-question : pearson = nan, spearman = nan
2019-02-12 17:49:27,518 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-12 17:49:27,518 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-12 17:49:27,518 : ***** Transfer task : MR *****


2019-02-12 17:49:27,538 : loading BERT mode bert-base-uncased
2019-02-12 17:49:27,538 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:49:27,556 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:49:27,556 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd35vwt1h
2019-02-12 17:49:29,896 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:49:31,328 : Generating sentence embeddings
2019-02-12 17:49:45,597 : Generated sentence embeddings
2019-02-12 17:49:45,597 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:49:57,118 : Best param found at split 1: l2reg = 1e-05                 with score 50.0
2019-02-12 17:50:15,333 : Best param found at split 2: l2reg = 1e-05                 with score 50.0
2019-02-12 17:50:33,427 : Best param found at split 3: l2reg = 1e-05                 with score 50.0
2019-02-12 17:50:47,254 : Best param found at split 4: l2reg = 1e-05                 with score 50.0
2019-02-12 17:51:14,314 : Best param found at split 5: l2reg = 1e-05                 with score 50.0
2019-02-12 17:51:14,977 : Dev acc : 50.0 Test acc : 50.0

2019-02-12 17:51:14,978 : ***** Transfer task : CR *****


2019-02-12 17:51:14,985 : loading BERT mode bert-base-uncased
2019-02-12 17:51:14,985 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:51:15,005 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:51:15,005 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5iubtcl7
2019-02-12 17:51:17,450 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:51:18,894 : Generating sentence embeddings
2019-02-12 17:51:23,526 : Generated sentence embeddings
2019-02-12 17:51:23,526 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:51:27,931 : Best param found at split 1: l2reg = 1e-05                 with score 63.76
2019-02-12 17:51:32,419 : Best param found at split 2: l2reg = 1e-05                 with score 63.76
2019-02-12 17:51:36,817 : Best param found at split 3: l2reg = 1e-05                 with score 63.77
2019-02-12 17:51:40,725 : Best param found at split 4: l2reg = 0.01                 with score 63.75
2019-02-12 17:51:44,514 : Best param found at split 5: l2reg = 1e-05                 with score 63.75
2019-02-12 17:51:44,754 : Dev acc : 63.76 Test acc : 63.76

2019-02-12 17:51:44,754 : ***** Transfer task : MPQA *****


2019-02-12 17:51:44,760 : loading BERT mode bert-base-uncased
2019-02-12 17:51:44,760 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:51:44,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:51:44,779 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ct5ru4h
2019-02-12 17:51:47,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:51:48,727 : Generating sentence embeddings
2019-02-12 17:51:55,681 : Generated sentence embeddings
2019-02-12 17:51:55,681 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:52:14,116 : Best param found at split 1: l2reg = 1e-05                 with score 68.78
2019-02-12 17:52:26,996 : Best param found at split 2: l2reg = 0.01                 with score 53.76
2019-02-12 17:52:40,490 : Best param found at split 3: l2reg = 1e-05                 with score 68.77
2019-02-12 17:52:56,487 : Best param found at split 4: l2reg = 1e-05                 with score 68.77
2019-02-12 17:53:15,355 : Best param found at split 5: l2reg = 1e-05                 with score 68.77
2019-02-12 17:53:16,272 : Dev acc : 65.77 Test acc : 68.77

2019-02-12 17:53:16,273 : ***** Transfer task : SUBJ *****


2019-02-12 17:53:16,290 : loading BERT mode bert-base-uncased
2019-02-12 17:53:16,290 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:53:16,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:53:16,310 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8tjltq7u
2019-02-12 17:53:18,713 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:53:20,240 : Generating sentence embeddings
2019-02-12 17:53:40,672 : Generated sentence embeddings
2019-02-12 17:53:40,673 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:53:59,571 : Best param found at split 1: l2reg = 1e-05                 with score 50.0
2019-02-12 17:54:19,932 : Best param found at split 2: l2reg = 1e-05                 with score 50.0
2019-02-12 17:54:40,809 : Best param found at split 3: l2reg = 1e-05                 with score 50.0
2019-02-12 17:55:00,053 : Best param found at split 4: l2reg = 1e-05                 with score 50.0
2019-02-12 17:55:10,744 : Best param found at split 5: l2reg = 1e-05                 with score 50.0
2019-02-12 17:55:11,297 : Dev acc : 50.0 Test acc : 50.0

2019-02-12 17:55:11,298 : ***** Transfer task : SST Binary classification *****


2019-02-12 17:55:11,428 : loading BERT mode bert-base-uncased
2019-02-12 17:55:11,428 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:55:11,451 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:55:11,451 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp082eaxkr
2019-02-12 17:55:13,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:55:15,343 : Computing embedding for train
2019-02-12 17:56:24,089 : Computed train embeddings
2019-02-12 17:56:24,089 : Computing embedding for dev
2019-02-12 17:56:25,989 : Computed dev embeddings
2019-02-12 17:56:25,989 : Computing embedding for test
2019-02-12 17:56:28,487 : Computed test embeddings
2019-02-12 17:56:28,487 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 17:57:04,062 : [('reg:1e-05', 50.92), ('reg:0.0001', 50.92), ('reg:0.001', 50.92), ('reg:0.01', 50.92)]
2019-02-12 17:57:04,062 : Validation : best param found is reg = 1e-05 with score             50.92
2019-02-12 17:57:04,062 : Evaluating...
2019-02-12 17:57:14,568 : 
Dev acc : 50.92 Test acc : 49.92 for             SST Binary classification

2019-02-12 17:57:14,573 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 17:57:14,622 : loading BERT mode bert-base-uncased
2019-02-12 17:57:14,623 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:57:14,644 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:57:14,644 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpurz41wli
2019-02-12 17:57:17,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:57:18,541 : Computing embedding for train
2019-02-12 17:57:33,178 : Computed train embeddings
2019-02-12 17:57:33,178 : Computing embedding for dev
2019-02-12 17:57:35,135 : Computed dev embeddings
2019-02-12 17:57:35,136 : Computing embedding for test
2019-02-12 17:57:39,098 : Computed test embeddings
2019-02-12 17:57:39,099 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 17:57:43,996 : [('reg:1e-05', 26.25), ('reg:0.0001', 26.25), ('reg:0.001', 25.34), ('reg:0.01', 25.34)]
2019-02-12 17:57:43,996 : Validation : best param found is reg = 1e-05 with score             26.25
2019-02-12 17:57:43,996 : Evaluating...
2019-02-12 17:57:45,188 : 
Dev acc : 26.25 Test acc : 28.64 for             SST Fine-Grained classification

2019-02-12 17:57:45,188 : ***** Transfer task : TREC *****


2019-02-12 17:57:45,202 : loading BERT mode bert-base-uncased
2019-02-12 17:57:45,202 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:57:45,221 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:57:45,221 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6e5vtkq2
2019-02-12 17:57:47,672 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:57:57,082 : Computed train embeddings
2019-02-12 17:57:57,794 : Computed test embeddings
2019-02-12 17:57:57,795 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 17:58:12,609 : [('reg:1e-05', 22.74), ('reg:0.0001', 22.74), ('reg:0.001', 22.74), ('reg:0.01', 22.74)]
2019-02-12 17:58:12,609 : Cross-validation : best param found is reg = 1e-05             with score 22.74
2019-02-12 17:58:12,609 : Evaluating...
2019-02-12 17:58:13,504 : 
Dev acc : 22.74 Test acc : 18.8             for TREC

2019-02-12 17:58:13,504 : ***** Transfer task : MRPC *****


2019-02-12 17:58:13,564 : loading BERT mode bert-base-uncased
2019-02-12 17:58:13,564 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:58:13,583 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:58:13,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkjqwibu9
2019-02-12 17:58:16,073 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:58:17,554 : Computing embedding for train
2019-02-12 17:58:31,698 : Computed train embeddings
2019-02-12 17:58:31,698 : Computing embedding for test
2019-02-12 17:58:37,837 : Computed test embeddings
2019-02-12 17:58:37,854 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 17:58:50,762 : [('reg:1e-05', 67.54), ('reg:0.0001', 67.54), ('reg:0.001', 67.54), ('reg:0.01', 67.54)]
2019-02-12 17:58:50,762 : Cross-validation : best param found is reg = 1e-05             with score 67.54
2019-02-12 17:58:50,762 : Evaluating...
2019-02-12 17:58:51,336 : Dev acc : 67.54 Test acc 66.49; Test F1 79.87 for MRPC.

2019-02-12 17:58:51,336 : ***** Transfer task : SICK-Entailment*****


2019-02-12 17:58:51,360 : loading BERT mode bert-base-uncased
2019-02-12 17:58:51,360 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:58:51,381 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:58:51,381 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2lhde_m8
2019-02-12 17:58:53,821 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:58:55,310 : Computing embedding for train
2019-02-12 17:59:05,554 : Computed train embeddings
2019-02-12 17:59:05,554 : Computing embedding for dev
2019-02-12 17:59:06,859 : Computed dev embeddings
2019-02-12 17:59:06,859 : Computing embedding for test
2019-02-12 17:59:19,054 : Computed test embeddings
2019-02-12 17:59:19,082 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 17:59:22,202 : [('reg:1e-05', 56.4), ('reg:0.0001', 56.4), ('reg:0.001', 56.4), ('reg:0.01', 56.4)]
2019-02-12 17:59:22,202 : Validation : best param found is reg = 1e-05 with score             56.4
2019-02-12 17:59:22,202 : Evaluating...
2019-02-12 17:59:23,022 : 
Dev acc : 56.4 Test acc : 56.69 for                        SICK entailment

2019-02-12 17:59:23,023 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 17:59:23,049 : loading BERT mode bert-base-uncased
2019-02-12 17:59:23,049 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:59:23,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:59:23,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8j0syy2l
2019-02-12 17:59:25,495 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:59:27,022 : Computing embedding for train
2019-02-12 17:59:40,101 : Computed train embeddings
2019-02-12 17:59:40,102 : Computing embedding for dev
2019-02-12 17:59:41,784 : Computed dev embeddings
2019-02-12 17:59:41,784 : Computing embedding for test
2019-02-12 17:59:57,440 : Computed test embeddings
2019-02-12 18:00:32,043 : Dev : Pearson 0
2019-02-12 18:00:32,043 : Test : Pearson 0 Spearman 0 MSE 1.020079206584388                        for SICK Relatedness

2019-02-12 18:00:32,044 : 

***** Transfer task : STSBenchmark*****


2019-02-12 18:00:32,122 : loading BERT mode bert-base-uncased
2019-02-12 18:00:32,122 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:00:32,141 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:00:32,141 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7wflaq3_
2019-02-12 18:00:34,564 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:00:36,039 : Computing embedding for train
2019-02-12 18:00:50,468 : Computed train embeddings
2019-02-12 18:00:50,468 : Computing embedding for dev
2019-02-12 18:00:56,752 : Computed dev embeddings
2019-02-12 18:00:56,752 : Computing embedding for test
2019-02-12 18:01:02,398 : Computed test embeddings
2019-02-12 18:02:11,494 : Dev : Pearson 0
2019-02-12 18:02:11,494 : Test : Pearson 0 Spearman 0 MSE 2.474329571821447                        for SICK Relatedness

2019-02-12 18:02:11,494 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 18:02:11,743 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 18:02:11,753 : loading BERT mode bert-base-uncased
2019-02-12 18:02:11,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:02:11,845 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:02:11,845 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw4ckyvsq
2019-02-12 18:02:14,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:02:15,708 : Computing embeddings for train/dev/test
2019-02-12 18:04:54,561 : Computed embeddings
2019-02-12 18:04:54,561 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:05:45,567 : [('reg:1e-05', 16.67), ('reg:0.0001', 16.67), ('reg:0.001', 16.67), ('reg:0.01', 16.67)]
2019-02-12 18:05:45,567 : Validation : best param found is reg = 1e-05 with score             16.67
2019-02-12 18:05:45,567 : Evaluating...
2019-02-12 18:05:58,621 : 
Dev acc : 16.7 Test acc : 16.7 for LENGTH classification

2019-02-12 18:05:58,629 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 18:05:59,120 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 18:05:59,166 : loading BERT mode bert-base-uncased
2019-02-12 18:05:59,166 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:05:59,195 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:05:59,195 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqfbxu7_0
2019-02-12 18:06:01,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:06:03,187 : Computing embeddings for train/dev/test
2019-02-12 18:09:07,285 : Computed embeddings
2019-02-12 18:09:07,285 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:09:49,179 : [('reg:1e-05', 0.1), ('reg:0.0001', 0.1), ('reg:0.001', 0.1), ('reg:0.01', 0.1)]
2019-02-12 18:09:49,179 : Validation : best param found is reg = 1e-05 with score             0.1
2019-02-12 18:09:49,179 : Evaluating...
2019-02-12 18:09:59,970 : 
Dev acc : 0.1 Test acc : 0.1 for WORDCONTENT classification

2019-02-12 18:09:59,978 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 18:10:00,313 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 18:10:00,376 : loading BERT mode bert-base-uncased
2019-02-12 18:10:00,377 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:10:00,469 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:10:00,469 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjzn_3nij
2019-02-12 18:10:02,900 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:10:04,347 : Computing embeddings for train/dev/test
2019-02-12 18:13:00,261 : Computed embeddings
2019-02-12 18:13:00,261 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:13:57,939 : [('reg:1e-05', 18.07), ('reg:0.0001', 18.07), ('reg:0.001', 18.07), ('reg:0.01', 18.07)]
2019-02-12 18:13:57,939 : Validation : best param found is reg = 1e-05 with score             18.07
2019-02-12 18:13:57,939 : Evaluating...
2019-02-12 18:14:11,900 : 
Dev acc : 18.1 Test acc : 17.9 for DEPTH classification

2019-02-12 18:14:11,907 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 18:14:12,453 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 18:14:12,514 : loading BERT mode bert-base-uncased
2019-02-12 18:14:12,514 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:14:12,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:14:12,539 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf62zy1ij
2019-02-12 18:14:14,955 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:14:16,416 : Computing embeddings for train/dev/test
2019-02-12 18:16:47,376 : Computed embeddings
2019-02-12 18:16:47,376 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:17:42,851 : [('reg:1e-05', 5.0), ('reg:0.0001', 5.0), ('reg:0.001', 5.0), ('reg:0.01', 5.0)]
2019-02-12 18:17:42,851 : Validation : best param found is reg = 1e-05 with score             5.0
2019-02-12 18:17:42,851 : Evaluating...
2019-02-12 18:18:01,456 : 
Dev acc : 5.0 Test acc : 5.0 for TOPCONSTITUENTS classification

2019-02-12 18:18:01,463 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 18:18:01,988 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 18:18:02,053 : loading BERT mode bert-base-uncased
2019-02-12 18:18:02,053 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:18:02,082 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:18:02,082 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppcwegjqc
2019-02-12 18:18:04,474 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:18:06,036 : Computing embeddings for train/dev/test
2019-02-12 18:20:45,687 : Computed embeddings
2019-02-12 18:20:45,688 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:21:41,866 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-12 18:21:41,866 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-12 18:21:41,866 : Evaluating...
2019-02-12 18:22:01,977 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-02-12 18:22:01,984 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 18:22:02,369 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 18:22:02,433 : loading BERT mode bert-base-uncased
2019-02-12 18:22:02,433 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:22:02,461 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:22:02,461 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4idcednj
2019-02-12 18:22:04,849 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:22:06,385 : Computing embeddings for train/dev/test
2019-02-12 18:24:48,699 : Computed embeddings
2019-02-12 18:24:48,699 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:25:36,868 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-12 18:25:36,869 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-12 18:25:36,869 : Evaluating...
2019-02-12 18:25:55,570 : 
Dev acc : 50.0 Test acc : 50.0 for TENSE classification

2019-02-12 18:25:55,578 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 18:25:55,971 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 18:25:56,032 : loading BERT mode bert-base-uncased
2019-02-12 18:25:56,032 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:25:56,144 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:25:56,144 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx06is4ly
2019-02-12 18:25:58,534 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:26:00,073 : Computing embeddings for train/dev/test
2019-02-12 18:29:09,808 : Computed embeddings
2019-02-12 18:29:09,808 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:29:42,802 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-12 18:29:42,802 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-12 18:29:42,802 : Evaluating...
2019-02-12 18:29:59,212 : 
Dev acc : 50.0 Test acc : 50.0 for SUBJNUMBER classification

2019-02-12 18:29:59,220 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 18:29:59,606 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 18:29:59,671 : loading BERT mode bert-base-uncased
2019-02-12 18:29:59,671 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:29:59,783 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:29:59,783 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcwrvk7z1
2019-02-12 18:30:02,196 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:30:03,733 : Computing embeddings for train/dev/test
2019-02-12 18:33:22,265 : Computed embeddings
2019-02-12 18:33:22,265 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:33:55,481 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-12 18:33:55,481 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-12 18:33:55,481 : Evaluating...
2019-02-12 18:34:04,017 : 
Dev acc : 50.0 Test acc : 50.0 for OBJNUMBER classification

2019-02-12 18:34:04,025 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 18:34:04,584 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 18:34:04,651 : loading BERT mode bert-base-uncased
2019-02-12 18:34:04,652 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:34:04,679 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:34:04,680 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl06cfhjm
2019-02-12 18:34:07,093 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:34:08,496 : Computing embeddings for train/dev/test
2019-02-12 18:37:05,272 : Computed embeddings
2019-02-12 18:37:05,273 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:37:41,950 : [('reg:1e-05', 50.19), ('reg:0.0001', 50.19), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-02-12 18:37:41,950 : Validation : best param found is reg = 1e-05 with score             50.19
2019-02-12 18:37:41,950 : Evaluating...
2019-02-12 18:37:51,497 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-02-12 18:37:51,506 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 18:37:52,093 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 18:37:52,168 : loading BERT mode bert-base-uncased
2019-02-12 18:37:52,169 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:37:52,197 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:37:52,197 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpivv0wm9s
2019-02-12 18:37:54,631 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:37:56,050 : Computing embeddings for train/dev/test
2019-02-12 18:40:26,096 : Computed embeddings
2019-02-12 18:40:26,097 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:41:15,663 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-12 18:41:15,663 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-12 18:41:15,663 : Evaluating...
2019-02-12 18:41:23,894 : 
Dev acc : 50.0 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-02-12 18:41:23,904 : {'STS12': {'MSRpar': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'MSRvid': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 399}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS13': {'FNWN': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 189}, 'headlines': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'OnWN': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 561}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS14': {'deft-forum': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 450}, 'deft-news': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 300}, 'headlines': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'images': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'OnWN': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'tweet-news': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS15': {'answers-forums': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 375}, 'answers-students': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'belief': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 375}, 'headlines': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'images': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS16': {'answer-answer': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 254}, 'headlines': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 249}, 'plagiarism': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 230}, 'postediting': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 244}, 'question-question': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 209}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'MR': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 63.76, 'acc': 63.76, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 65.77, 'acc': 68.77, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 50.92, 'acc': 49.92, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 26.25, 'acc': 28.64, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 22.74, 'acc': 18.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 67.54, 'acc': 66.49, 'f1': 79.87, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 56.4, 'acc': 56.69, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0, 'pearson': 0, 'spearman': 0, 'mse': 1.020079206584388, 'yhat': array([3.57972671, 3.57972671, 3.57972671, ..., 3.57972671, 3.57972671,
       3.57972671]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0, 'pearson': 0, 'spearman': 0, 'mse': 2.474329571821447, 'yhat': array([2.99368074, 2.99368074, 2.99368074, ..., 2.99368074, 2.99368074,
       2.99368074]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 16.67, 'acc': 16.67, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.1, 'acc': 0.1, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 18.07, 'acc': 17.88, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 5.0, 'acc': 5.0, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.19, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 18:41:23,904 : ********************************************************************************
2019-02-12 18:41:23,904 : ********************************************************************************
2019-02-12 18:41:23,904 : ********************************************************************************
2019-02-12 18:41:23,904 : layer 1
2019-02-12 18:41:23,904 : ********************************************************************************
2019-02-12 18:41:23,904 : ********************************************************************************
2019-02-12 18:41:23,904 : ********************************************************************************
2019-02-12 18:41:23,988 : ***** Transfer task : STS12 *****


2019-02-12 18:41:24,001 : loading BERT mode bert-base-uncased
2019-02-12 18:41:24,001 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:41:24,018 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:41:24,018 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb1iaqus6
2019-02-12 18:41:26,462 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:41:29,833 : MSRpar : pearson = 0.1038, spearman = 0.1476
2019-02-12 18:41:30,788 : MSRvid : pearson = 0.1145, spearman = 0.1538
2019-02-12 18:41:31,558 : SMTeuroparl : pearson = 0.2852, spearman = 0.5018
2019-02-12 18:41:33,453 : surprise.OnWN : pearson = 0.0625, spearman = 0.1267
2019-02-12 18:41:34,750 : surprise.SMTnews : pearson = 0.3811, spearman = 0.2912
2019-02-12 18:41:34,750 : ALL (weighted average) : Pearson = 0.1588,             Spearman = 0.2148
2019-02-12 18:41:34,750 : ALL (average) : Pearson = 0.1894,             Spearman = 0.2442

2019-02-12 18:41:34,750 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 18:41:34,760 : loading BERT mode bert-base-uncased
2019-02-12 18:41:34,760 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:41:34,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:41:34,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw07frjpc
2019-02-12 18:41:37,191 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:41:39,594 : FNWN : pearson = 0.1196, spearman = 0.1837
2019-02-12 18:41:42,348 : headlines : pearson = 0.0731, spearman = 0.2303
2019-02-12 18:41:44,185 : OnWN : pearson = 0.0764, spearman = 0.0332
2019-02-12 18:41:44,185 : ALL (weighted average) : Pearson = 0.0802,             Spearman = 0.1507
2019-02-12 18:41:44,185 : ALL (average) : Pearson = 0.0897,             Spearman = 0.1491

2019-02-12 18:41:44,185 : ***** Transfer task : STS14 *****


2019-02-12 18:41:44,203 : loading BERT mode bert-base-uncased
2019-02-12 18:41:44,204 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:41:44,221 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:41:44,221 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2p7ye_un
2019-02-12 18:41:46,646 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:41:49,500 : deft-forum : pearson = 0.0830, spearman = 0.1252
2019-02-12 18:41:50,380 : deft-news : pearson = 0.2601, spearman = 0.4118
2019-02-12 18:41:51,775 : headlines : pearson = 0.0891, spearman = 0.1901
2019-02-12 18:41:53,130 : images : pearson = 0.0273, spearman = 0.1951
2019-02-12 18:41:54,496 : OnWN : pearson = 0.0961, spearman = 0.1133
2019-02-12 18:41:56,169 : tweet-news : pearson = 0.1473, spearman = 0.2135
2019-02-12 18:41:56,170 : ALL (weighted average) : Pearson = 0.1027,             Spearman = 0.1904
2019-02-12 18:41:56,170 : ALL (average) : Pearson = 0.1172,             Spearman = 0.2082

2019-02-12 18:41:56,170 : ***** Transfer task : STS15 *****


2019-02-12 18:41:56,207 : loading BERT mode bert-base-uncased
2019-02-12 18:41:56,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:41:56,225 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:41:56,225 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfzxfnpej
2019-02-12 18:41:58,657 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:42:01,150 : answers-forums : pearson = 0.1216, spearman = 0.1637
2019-02-12 18:42:02,513 : answers-students : pearson = 0.2266, spearman = 0.2630
2019-02-12 18:42:03,600 : belief : pearson = 0.0679, spearman = 0.1827
2019-02-12 18:42:05,037 : headlines : pearson = 0.1860, spearman = 0.3372
2019-02-12 18:42:06,430 : images : pearson = -0.0243, spearman = 0.2210
2019-02-12 18:42:06,430 : ALL (weighted average) : Pearson = 0.1207,             Spearman = 0.2486
2019-02-12 18:42:06,430 : ALL (average) : Pearson = 0.1155,             Spearman = 0.2335

2019-02-12 18:42:06,430 : ***** Transfer task : STS16 *****


2019-02-12 18:42:06,467 : loading BERT mode bert-base-uncased
2019-02-12 18:42:06,468 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:42:06,486 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:42:06,486 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm5t5juut
2019-02-12 18:42:08,926 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:42:10,917 : answer-answer : pearson = 0.0996, spearman = 0.2157
2019-02-12 18:42:11,361 : headlines : pearson = 0.2045, spearman = 0.3990
2019-02-12 18:42:11,876 : plagiarism : pearson = 0.1424, spearman = 0.3428
2019-02-12 18:42:12,640 : postediting : pearson = 0.1802, spearman = 0.6355
2019-02-12 18:42:13,037 : question-question : pearson = -0.1246, spearman = -0.1493
2019-02-12 18:42:13,037 : ALL (weighted average) : Pearson = 0.1070,             Spearman = 0.3009
2019-02-12 18:42:13,037 : ALL (average) : Pearson = 0.1004,             Spearman = 0.2888

2019-02-12 18:42:13,037 : ***** Transfer task : MR *****


2019-02-12 18:42:13,087 : loading BERT mode bert-base-uncased
2019-02-12 18:42:13,088 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:42:13,108 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:42:13,108 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjrmrmb8d
2019-02-12 18:42:15,553 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:42:17,000 : Generating sentence embeddings
2019-02-12 18:42:32,923 : Generated sentence embeddings
2019-02-12 18:42:32,923 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:42:59,393 : Best param found at split 1: l2reg = 1e-05                 with score 57.9
2019-02-12 18:43:29,095 : Best param found at split 2: l2reg = 1e-05                 with score 57.21
2019-02-12 18:44:01,756 : Best param found at split 3: l2reg = 1e-05                 with score 56.86
2019-02-12 18:44:38,570 : Best param found at split 4: l2reg = 1e-05                 with score 53.77
2019-02-12 18:45:13,672 : Best param found at split 5: l2reg = 1e-05                 with score 57.67
2019-02-12 18:45:15,087 : Dev acc : 56.68 Test acc : 54.12

2019-02-12 18:45:15,088 : ***** Transfer task : CR *****


2019-02-12 18:45:15,095 : loading BERT mode bert-base-uncased
2019-02-12 18:45:15,095 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:45:15,115 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:45:15,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1agmqjfh
2019-02-12 18:45:17,542 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:45:18,985 : Generating sentence embeddings
2019-02-12 18:45:23,738 : Generated sentence embeddings
2019-02-12 18:45:23,739 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:45:28,162 : Best param found at split 1: l2reg = 0.01                 with score 65.55
2019-02-12 18:45:32,992 : Best param found at split 2: l2reg = 0.001                 with score 64.46
2019-02-12 18:45:38,140 : Best param found at split 3: l2reg = 1e-05                 with score 65.86
2019-02-12 18:45:43,510 : Best param found at split 4: l2reg = 0.001                 with score 65.74
2019-02-12 18:45:48,710 : Best param found at split 5: l2reg = 0.001                 with score 65.18
2019-02-12 18:45:49,113 : Dev acc : 65.36 Test acc : 64.16

2019-02-12 18:45:49,114 : ***** Transfer task : MPQA *****


2019-02-12 18:45:49,119 : loading BERT mode bert-base-uncased
2019-02-12 18:45:49,120 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:45:49,139 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:45:49,139 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj447aok4
2019-02-12 18:45:51,586 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:45:53,074 : Generating sentence embeddings
2019-02-12 18:46:03,514 : Generated sentence embeddings
2019-02-12 18:46:03,514 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:46:33,079 : Best param found at split 1: l2reg = 0.0001                 with score 80.6
2019-02-12 18:47:01,214 : Best param found at split 2: l2reg = 0.001                 with score 80.29
2019-02-12 18:47:37,086 : Best param found at split 3: l2reg = 0.001                 with score 79.26
2019-02-12 18:48:18,674 : Best param found at split 4: l2reg = 1e-05                 with score 81.15
2019-02-12 18:48:45,002 : Best param found at split 5: l2reg = 1e-05                 with score 80.05
2019-02-12 18:48:46,163 : Dev acc : 80.27 Test acc : 81.85

2019-02-12 18:48:46,164 : ***** Transfer task : SUBJ *****


2019-02-12 18:48:46,179 : loading BERT mode bert-base-uncased
2019-02-12 18:48:46,179 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:48:46,233 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:48:46,233 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe0wq5bc1
2019-02-12 18:48:48,679 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:48:50,111 : Generating sentence embeddings
2019-02-12 18:49:06,217 : Generated sentence embeddings
2019-02-12 18:49:06,218 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:49:31,987 : Best param found at split 1: l2reg = 1e-05                 with score 80.02
2019-02-12 18:49:52,252 : Best param found at split 2: l2reg = 1e-05                 with score 81.14
2019-02-12 18:50:11,242 : Best param found at split 3: l2reg = 1e-05                 with score 81.91
2019-02-12 18:50:43,429 : Best param found at split 4: l2reg = 1e-05                 with score 80.69
2019-02-12 18:51:14,338 : Best param found at split 5: l2reg = 1e-05                 with score 78.76
2019-02-12 18:51:16,363 : Dev acc : 80.5 Test acc : 82.41

2019-02-12 18:51:16,364 : ***** Transfer task : SST Binary classification *****


2019-02-12 18:51:16,501 : loading BERT mode bert-base-uncased
2019-02-12 18:51:16,502 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:51:16,524 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:51:16,524 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5wwe1rxw
2019-02-12 18:51:18,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:51:20,432 : Computing embedding for train
2019-02-12 18:52:53,427 : Computed train embeddings
2019-02-12 18:52:53,427 : Computing embedding for dev
2019-02-12 18:52:54,643 : Computed dev embeddings
2019-02-12 18:52:54,643 : Computing embedding for test
2019-02-12 18:52:57,132 : Computed test embeddings
2019-02-12 18:52:57,133 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:53:20,697 : [('reg:1e-05', 68.46), ('reg:0.0001', 68.69), ('reg:0.001', 63.76), ('reg:0.01', 52.64)]
2019-02-12 18:53:20,697 : Validation : best param found is reg = 0.0001 with score             68.69
2019-02-12 18:53:20,698 : Evaluating...
2019-02-12 18:53:27,683 : 
Dev acc : 68.69 Test acc : 70.57 for             SST Binary classification

2019-02-12 18:53:27,688 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 18:53:27,742 : loading BERT mode bert-base-uncased
2019-02-12 18:53:27,743 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:53:27,762 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:53:27,763 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe7qxugb2
2019-02-12 18:53:30,197 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:53:31,651 : Computing embedding for train
2019-02-12 18:53:43,482 : Computed train embeddings
2019-02-12 18:53:43,482 : Computing embedding for dev
2019-02-12 18:53:45,025 : Computed dev embeddings
2019-02-12 18:53:45,025 : Computing embedding for test
2019-02-12 18:53:48,080 : Computed test embeddings
2019-02-12 18:53:48,080 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:53:51,768 : [('reg:1e-05', 31.88), ('reg:0.0001', 29.61), ('reg:0.001', 31.97), ('reg:0.01', 26.25)]
2019-02-12 18:53:51,768 : Validation : best param found is reg = 0.001 with score             31.97
2019-02-12 18:53:51,768 : Evaluating...
2019-02-12 18:53:52,581 : 
Dev acc : 31.97 Test acc : 30.72 for             SST Fine-Grained classification

2019-02-12 18:53:52,581 : ***** Transfer task : TREC *****


2019-02-12 18:53:52,594 : loading BERT mode bert-base-uncased
2019-02-12 18:53:52,594 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:53:52,614 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:53:52,614 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu29b8lqz
2019-02-12 18:53:55,052 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:54:01,977 : Computed train embeddings
2019-02-12 18:54:02,346 : Computed test embeddings
2019-02-12 18:54:02,346 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 18:54:21,788 : [('reg:1e-05', 50.34), ('reg:0.0001', 49.77), ('reg:0.001', 48.45), ('reg:0.01', 36.59)]
2019-02-12 18:54:21,788 : Cross-validation : best param found is reg = 1e-05             with score 50.34
2019-02-12 18:54:21,788 : Evaluating...
2019-02-12 18:54:22,897 : 
Dev acc : 50.34 Test acc : 62.2             for TREC

2019-02-12 18:54:22,898 : ***** Transfer task : MRPC *****


2019-02-12 18:54:22,918 : loading BERT mode bert-base-uncased
2019-02-12 18:54:22,918 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:54:22,941 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:54:22,941 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu_zfhory
2019-02-12 18:54:25,376 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:54:26,858 : Computing embedding for train
2019-02-12 18:54:40,472 : Computed train embeddings
2019-02-12 18:54:40,473 : Computing embedding for test
2019-02-12 18:54:46,495 : Computed test embeddings
2019-02-12 18:54:46,511 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 18:54:58,478 : [('reg:1e-05', 68.65), ('reg:0.0001', 68.52), ('reg:0.001', 68.06), ('reg:0.01', 67.66)]
2019-02-12 18:54:58,478 : Cross-validation : best param found is reg = 1e-05             with score 68.65
2019-02-12 18:54:58,478 : Evaluating...
2019-02-12 18:54:59,167 : Dev acc : 68.65 Test acc 68.93; Test F1 80.03 for MRPC.

2019-02-12 18:54:59,167 : ***** Transfer task : SICK-Entailment*****


2019-02-12 18:54:59,190 : loading BERT mode bert-base-uncased
2019-02-12 18:54:59,190 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:54:59,246 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:54:59,246 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqbu7dplv
2019-02-12 18:55:01,673 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:55:03,167 : Computing embedding for train
2019-02-12 18:55:15,325 : Computed train embeddings
2019-02-12 18:55:15,325 : Computing embedding for dev
2019-02-12 18:55:16,829 : Computed dev embeddings
2019-02-12 18:55:16,829 : Computing embedding for test
2019-02-12 18:55:30,726 : Computed test embeddings
2019-02-12 18:55:30,754 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:55:34,028 : [('reg:1e-05', 71.8), ('reg:0.0001', 71.8), ('reg:0.001', 71.4), ('reg:0.01', 62.8)]
2019-02-12 18:55:34,028 : Validation : best param found is reg = 1e-05 with score             71.8
2019-02-12 18:55:34,028 : Evaluating...
2019-02-12 18:55:34,866 : 
Dev acc : 71.8 Test acc : 68.95 for                        SICK entailment

2019-02-12 18:55:34,866 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 18:55:34,892 : loading BERT mode bert-base-uncased
2019-02-12 18:55:34,893 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:55:34,912 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:55:34,913 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp2jbh6u3
2019-02-12 18:55:37,349 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:55:38,801 : Computing embedding for train
2019-02-12 18:55:51,211 : Computed train embeddings
2019-02-12 18:55:51,211 : Computing embedding for dev
2019-02-12 18:55:52,457 : Computed dev embeddings
2019-02-12 18:55:52,457 : Computing embedding for test
2019-02-12 18:56:04,737 : Computed test embeddings
2019-02-12 18:57:35,863 : Dev : Pearson 0.697939563530309
2019-02-12 18:57:35,863 : Test : Pearson 0.7213960553443098 Spearman 0.6669834363031751 MSE 0.4894853563904378                        for SICK Relatedness

2019-02-12 18:57:35,864 : 

***** Transfer task : STSBenchmark*****


2019-02-12 18:57:35,933 : loading BERT mode bert-base-uncased
2019-02-12 18:57:35,933 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:57:35,951 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:57:35,952 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd0awr77x
2019-02-12 18:57:38,378 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:57:39,833 : Computing embedding for train
2019-02-12 18:57:54,339 : Computed train embeddings
2019-02-12 18:57:54,339 : Computing embedding for dev
2019-02-12 18:57:58,465 : Computed dev embeddings
2019-02-12 18:57:58,465 : Computing embedding for test
2019-02-12 18:58:02,105 : Computed test embeddings
2019-02-12 18:59:32,978 : Dev : Pearson 0.5584027781621694
2019-02-12 18:59:32,978 : Test : Pearson 0.4944005510509023 Spearman 0.4807060159651944 MSE 1.9982841152386206                        for SICK Relatedness

2019-02-12 18:59:32,979 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 18:59:33,227 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 18:59:33,237 : loading BERT mode bert-base-uncased
2019-02-12 18:59:33,237 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:59:33,330 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:59:33,330 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq90oit5g
2019-02-12 18:59:35,768 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:59:37,204 : Computing embeddings for train/dev/test
2019-02-12 19:02:11,266 : Computed embeddings
2019-02-12 19:02:11,266 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:03:33,686 : [('reg:1e-05', 71.76), ('reg:0.0001', 70.3), ('reg:0.001', 63.13), ('reg:0.01', 42.64)]
2019-02-12 19:03:33,686 : Validation : best param found is reg = 1e-05 with score             71.76
2019-02-12 19:03:33,686 : Evaluating...
2019-02-12 19:03:47,391 : 
Dev acc : 71.8 Test acc : 71.9 for LENGTH classification

2019-02-12 19:03:47,400 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 19:03:47,868 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 19:03:47,913 : loading BERT mode bert-base-uncased
2019-02-12 19:03:47,913 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:03:47,941 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:03:47,941 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi86f2tsh
2019-02-12 19:03:50,355 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:03:51,767 : Computing embeddings for train/dev/test
2019-02-12 19:06:06,816 : Computed embeddings
2019-02-12 19:06:06,816 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:07:20,407 : [('reg:1e-05', 0.81), ('reg:0.0001', 0.25), ('reg:0.001', 0.15), ('reg:0.01', 0.1)]
2019-02-12 19:07:20,407 : Validation : best param found is reg = 1e-05 with score             0.81
2019-02-12 19:07:20,407 : Evaluating...
2019-02-12 19:07:37,131 : 
Dev acc : 0.8 Test acc : 0.8 for WORDCONTENT classification

2019-02-12 19:07:37,140 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 19:07:37,511 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 19:07:37,575 : loading BERT mode bert-base-uncased
2019-02-12 19:07:37,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:07:37,600 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:07:37,600 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe2gipely
2019-02-12 19:07:40,014 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:07:41,483 : Computing embeddings for train/dev/test
2019-02-12 19:10:27,352 : Computed embeddings
2019-02-12 19:10:27,352 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:11:17,913 : [('reg:1e-05', 25.72), ('reg:0.0001', 25.84), ('reg:0.001', 23.09), ('reg:0.01', 22.56)]
2019-02-12 19:11:17,913 : Validation : best param found is reg = 0.0001 with score             25.84
2019-02-12 19:11:17,913 : Evaluating...
2019-02-12 19:11:30,873 : 
Dev acc : 25.8 Test acc : 26.1 for DEPTH classification

2019-02-12 19:11:30,881 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 19:11:31,267 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 19:11:31,330 : loading BERT mode bert-base-uncased
2019-02-12 19:11:31,330 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:11:31,359 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:11:31,359 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9nvwxopk
2019-02-12 19:11:33,787 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:11:35,274 : Computing embeddings for train/dev/test
2019-02-12 19:14:13,108 : Computed embeddings
2019-02-12 19:14:13,108 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:15:43,474 : [('reg:1e-05', 50.57), ('reg:0.0001', 44.22), ('reg:0.001', 31.25), ('reg:0.01', 16.24)]
2019-02-12 19:15:43,474 : Validation : best param found is reg = 1e-05 with score             50.57
2019-02-12 19:15:43,474 : Evaluating...
2019-02-12 19:16:08,994 : 
Dev acc : 50.6 Test acc : 51.0 for TOPCONSTITUENTS classification

2019-02-12 19:16:09,002 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 19:16:09,358 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 19:16:09,422 : loading BERT mode bert-base-uncased
2019-02-12 19:16:09,423 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:16:09,452 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:16:09,452 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpakfaw317
2019-02-12 19:16:11,872 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:16:13,315 : Computing embeddings for train/dev/test
2019-02-12 19:18:43,461 : Computed embeddings
2019-02-12 19:18:43,461 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:19:58,018 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-12 19:19:58,018 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-12 19:19:58,018 : Evaluating...
2019-02-12 19:20:11,804 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-02-12 19:20:11,812 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 19:20:12,192 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 19:20:12,256 : loading BERT mode bert-base-uncased
2019-02-12 19:20:12,257 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:20:12,370 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:20:12,370 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0fgxkzre
2019-02-12 19:20:14,777 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:20:16,244 : Computing embeddings for train/dev/test
2019-02-12 19:22:39,967 : Computed embeddings
2019-02-12 19:22:39,967 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:24:37,112 : [('reg:1e-05', 79.94), ('reg:0.0001', 79.73), ('reg:0.001', 78.33), ('reg:0.01', 68.1)]
2019-02-12 19:24:37,112 : Validation : best param found is reg = 1e-05 with score             79.94
2019-02-12 19:24:37,112 : Evaluating...
2019-02-12 19:25:00,156 : 
Dev acc : 79.9 Test acc : 78.2 for TENSE classification

2019-02-12 19:25:00,165 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 19:25:00,560 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 19:25:00,624 : loading BERT mode bert-base-uncased
2019-02-12 19:25:00,624 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:25:00,740 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:25:00,741 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7i0_2a6f
2019-02-12 19:25:03,178 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:25:04,690 : Computing embeddings for train/dev/test
2019-02-12 19:27:25,635 : Computed embeddings
2019-02-12 19:27:25,635 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:29:17,640 : [('reg:1e-05', 74.42), ('reg:0.0001', 73.9), ('reg:0.001', 71.77), ('reg:0.01', 56.24)]
2019-02-12 19:29:17,640 : Validation : best param found is reg = 1e-05 with score             74.42
2019-02-12 19:29:17,640 : Evaluating...
2019-02-12 19:29:39,654 : 
Dev acc : 74.4 Test acc : 73.4 for SUBJNUMBER classification

2019-02-12 19:29:39,663 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 19:29:40,236 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 19:29:40,302 : loading BERT mode bert-base-uncased
2019-02-12 19:29:40,302 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:29:40,329 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:29:40,330 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp78m_8ytk
2019-02-12 19:29:42,759 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:29:44,231 : Computing embeddings for train/dev/test
2019-02-12 19:31:52,203 : Computed embeddings
2019-02-12 19:31:52,204 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:33:27,477 : [('reg:1e-05', 71.16), ('reg:0.0001', 70.62), ('reg:0.001', 62.48), ('reg:0.01', 60.82)]
2019-02-12 19:33:27,478 : Validation : best param found is reg = 1e-05 with score             71.16
2019-02-12 19:33:27,478 : Evaluating...
2019-02-12 19:33:40,668 : 
Dev acc : 71.2 Test acc : 71.9 for OBJNUMBER classification

2019-02-12 19:33:40,677 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 19:33:41,079 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 19:33:41,147 : loading BERT mode bert-base-uncased
2019-02-12 19:33:41,147 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:33:41,176 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:33:41,176 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgd2vehcb
2019-02-12 19:33:43,614 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:33:45,030 : Computing embeddings for train/dev/test
2019-02-12 19:36:07,509 : Computed embeddings
2019-02-12 19:36:07,509 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:37:06,202 : [('reg:1e-05', 50.19), ('reg:0.0001', 50.19), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-02-12 19:37:06,202 : Validation : best param found is reg = 1e-05 with score             50.19
2019-02-12 19:37:06,202 : Evaluating...
2019-02-12 19:37:15,906 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-02-12 19:37:15,916 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 19:37:16,328 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 19:37:16,403 : loading BERT mode bert-base-uncased
2019-02-12 19:37:16,403 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:37:16,433 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:37:16,433 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi3bkqiha
2019-02-12 19:37:18,908 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:37:20,335 : Computing embeddings for train/dev/test
2019-02-12 19:39:41,429 : Computed embeddings
2019-02-12 19:39:41,429 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:40:42,647 : [('reg:1e-05', 52.13), ('reg:0.0001', 50.03), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-12 19:40:42,647 : Validation : best param found is reg = 1e-05 with score             52.13
2019-02-12 19:40:42,647 : Evaluating...
2019-02-12 19:40:59,126 : 
Dev acc : 52.1 Test acc : 51.8 for COORDINATIONINVERSION classification

2019-02-12 19:40:59,137 : {'STS12': {'MSRpar': {'pearson': (0.10380424942562065, 0.004430754024065819), 'spearman': SpearmanrResult(correlation=0.14761811776682998, pvalue=4.945273020979249e-05), 'nsamples': 750}, 'MSRvid': {'pearson': (0.11454030961837812, 0.001678384357147286), 'spearman': SpearmanrResult(correlation=0.15381061031829224, pvalue=2.3322475054148977e-05), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.28517971103148354, 4.883730298968081e-10), 'spearman': SpearmanrResult(correlation=0.5017880205922501, pvalue=1.2096690067571207e-30), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.06245784237201789, 0.08739640410855704), 'spearman': SpearmanrResult(correlation=0.12674077352775195, pvalue=0.0005029336895913375), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.38109829191164263, 3.0657187916444786e-15), 'spearman': SpearmanrResult(correlation=0.2911581198574834, pvalue=3.094281818427784e-09), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.18941608087182854, 'wmean': 0.15880228664678533}, 'spearman': {'mean': 0.24422312841252153, 'wmean': 0.21480692325760434}}}, 'STS13': {'FNWN': {'pearson': (0.11963480087641581, 0.10106964967955051), 'spearman': SpearmanrResult(correlation=0.18372020206777637, pvalue=0.011388165408677183), 'nsamples': 189}, 'headlines': {'pearson': (0.07309505026120544, 0.0453784961388967), 'spearman': SpearmanrResult(correlation=0.23026393007211166, pvalue=1.7540278049854389e-10), 'nsamples': 750}, 'OnWN': {'pearson': (0.07640805050606723, 0.07054957054143243), 'spearman': SpearmanrResult(correlation=0.03317657890478938, pvalue=0.4328846102314525), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.0897126338812295, 'wmean': 0.08019812093030027}, 'spearman': {'mean': 0.1490535703482258, 'wmean': 0.1506887510069869}}}, 'STS14': {'deft-forum': {'pearson': (0.0830406025191273, 0.07845902357732634), 'spearman': SpearmanrResult(correlation=0.1251657667948887, pvalue=0.007854918528360094), 'nsamples': 450}, 'deft-news': {'pearson': (0.26012087363413466, 4.984269664556529e-06), 'spearman': SpearmanrResult(correlation=0.4117529233836087, pvalue=1.0506523650820577e-13), 'nsamples': 300}, 'headlines': {'pearson': (0.08909981973681941, 0.014651306531006035), 'spearman': SpearmanrResult(correlation=0.1901364319417861, pvalue=1.5514456990945782e-07), 'nsamples': 750}, 'images': {'pearson': (0.027269066687729573, 0.45585639072982576), 'spearman': SpearmanrResult(correlation=0.19510216928988197, pvalue=7.203406241497916e-08), 'nsamples': 750}, 'OnWN': {'pearson': (0.09607507735599727, 0.00846771112077922), 'spearman': SpearmanrResult(correlation=0.1133339946004993, pvalue=0.0018796116330401012), 'nsamples': 750}, 'tweet-news': {'pearson': (0.1473089606456684, 5.13040774787193e-05), 'spearman': SpearmanrResult(correlation=0.2134627407786209, pvalue=3.5403738476846613e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.11715240009657941, 'wmean': 0.10272512707826899}, 'spearman': {'mean': 0.20815900446488092, 'wmean': 0.19036719320823298}}}, 'STS15': {'answers-forums': {'pearson': (0.12158114216686705, 0.018508138202662203), 'spearman': SpearmanrResult(correlation=0.16374204764325878, pvalue=0.001463798670735793), 'nsamples': 375}, 'answers-students': {'pearson': (0.22656941258579405, 3.467128075018036e-10), 'spearman': SpearmanrResult(correlation=0.26299109969089624, pvalue=2.4885223708616667e-13), 'nsamples': 750}, 'belief': {'pearson': (0.0678607706750548, 0.1897756752333678), 'spearman': SpearmanrResult(correlation=0.1826704910069743, pvalue=0.00037718792800990966), 'nsamples': 375}, 'headlines': {'pearson': (0.18598490301931533, 2.9015547628717895e-07), 'spearman': SpearmanrResult(correlation=0.33717873902465495, pvalue=2.136511030457333e-21), 'nsamples': 750}, 'images': {'pearson': (-0.024335388238084395, 0.5057697497834123), 'spearman': SpearmanrResult(correlation=0.22103565473014256, pvalue=9.413150963712061e-10), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.11553216804178938, 'wmean': 0.12073497094699648}, 'spearman': {'mean': 0.23352360641918538, 'wmean': 0.24860294069270256}}}, 'STS16': {'answer-answer': {'pearson': (0.0996322199747233, 0.11319648980704425), 'spearman': SpearmanrResult(correlation=0.2157206466623911, pvalue=0.0005363502974616452), 'nsamples': 254}, 'headlines': {'pearson': (0.20449626093914425, 0.001174422538880629), 'spearman': SpearmanrResult(correlation=0.3990100203819182, pvalue=6.22834103527279e-11), 'nsamples': 249}, 'plagiarism': {'pearson': (0.1424344034537375, 0.030820925710232608), 'spearman': SpearmanrResult(correlation=0.34282705286133175, pvalue=9.638568668189056e-08), 'nsamples': 230}, 'postediting': {'pearson': (0.1802028795897441, 0.004749416093869213), 'spearman': SpearmanrResult(correlation=0.6355357099108896, pvalue=5.211609268901804e-29), 'nsamples': 244}, 'question-question': {'pearson': (-0.12464601726875066, 0.07214554603331674), 'spearman': SpearmanrResult(correlation=-0.14928117797745077, pvalue=0.03098288398281293), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.1004239493377197, 'wmean': 0.10700215063449825}, 'spearman': {'mean': 0.288762450367816, 'wmean': 0.3009002601234579}}}, 'MR': {'devacc': 56.68, 'acc': 54.12, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 65.36, 'acc': 64.16, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.27, 'acc': 81.85, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 80.5, 'acc': 82.41, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 68.69, 'acc': 70.57, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 31.97, 'acc': 30.72, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 50.34, 'acc': 62.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.65, 'acc': 68.93, 'f1': 80.03, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.8, 'acc': 68.95, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.697939563530309, 'pearson': 0.7213960553443098, 'spearman': 0.6669834363031751, 'mse': 0.4894853563904378, 'yhat': array([3.13010049, 4.23965664, 1.71305823, ..., 3.43935882, 4.44825269,
       4.60157129]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5584027781621694, 'pearson': 0.4944005510509023, 'spearman': 0.4807060159651944, 'mse': 1.9982841152386206, 'yhat': array([2.11734551, 2.34330568, 2.58218821, ..., 3.41403858, 3.19600335,
       3.21360939]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 71.76, 'acc': 71.9, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.81, 'acc': 0.79, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.84, 'acc': 26.12, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 50.57, 'acc': 50.99, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 79.94, 'acc': 78.2, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 74.42, 'acc': 73.41, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 71.16, 'acc': 71.87, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.19, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 52.13, 'acc': 51.83, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 19:40:59,137 : ********************************************************************************
2019-02-12 19:40:59,137 : ********************************************************************************
2019-02-12 19:40:59,137 : ********************************************************************************
2019-02-12 19:40:59,137 : layer 2
2019-02-12 19:40:59,137 : ********************************************************************************
2019-02-12 19:40:59,137 : ********************************************************************************
2019-02-12 19:40:59,137 : ********************************************************************************
2019-02-12 19:40:59,224 : ***** Transfer task : STS12 *****


2019-02-12 19:40:59,237 : loading BERT mode bert-base-uncased
2019-02-12 19:40:59,237 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:40:59,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:40:59,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxk542bxb
2019-02-12 19:41:01,674 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:41:05,613 : MSRpar : pearson = 0.2096, spearman = 0.2474
2019-02-12 19:41:07,357 : MSRvid : pearson = 0.1847, spearman = 0.2012
2019-02-12 19:41:08,698 : SMTeuroparl : pearson = 0.4128, spearman = 0.5718
2019-02-12 19:41:11,052 : surprise.OnWN : pearson = 0.1799, spearman = 0.2066
2019-02-12 19:41:12,359 : surprise.SMTnews : pearson = 0.4149, spearman = 0.3424
2019-02-12 19:41:12,359 : ALL (weighted average) : Pearson = 0.2528,             Spearman = 0.2865
2019-02-12 19:41:12,359 : ALL (average) : Pearson = 0.2804,             Spearman = 0.3139

2019-02-12 19:41:12,360 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 19:41:12,369 : loading BERT mode bert-base-uncased
2019-02-12 19:41:12,369 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:41:12,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:41:12,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8et18sjt
2019-02-12 19:41:14,816 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:41:16,952 : FNWN : pearson = 0.2459, spearman = 0.2579
2019-02-12 19:41:18,244 : headlines : pearson = 0.0788, spearman = 0.2285
2019-02-12 19:41:19,221 : OnWN : pearson = 0.1281, spearman = 0.1231
2019-02-12 19:41:19,221 : ALL (weighted average) : Pearson = 0.1183,             Spearman = 0.1928
2019-02-12 19:41:19,222 : ALL (average) : Pearson = 0.1509,             Spearman = 0.2032

2019-02-12 19:41:19,222 : ***** Transfer task : STS14 *****


2019-02-12 19:41:19,237 : loading BERT mode bert-base-uncased
2019-02-12 19:41:19,237 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:41:19,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:41:19,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpccvmpq0e
2019-02-12 19:41:21,689 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:41:24,026 : deft-forum : pearson = 0.1154, spearman = 0.1576
2019-02-12 19:41:24,909 : deft-news : pearson = 0.3808, spearman = 0.4890
2019-02-12 19:41:26,297 : headlines : pearson = 0.1046, spearman = 0.1956
2019-02-12 19:41:27,647 : images : pearson = 0.1633, spearman = 0.2808
2019-02-12 19:41:29,017 : OnWN : pearson = 0.1605, spearman = 0.1957
2019-02-12 19:41:30,697 : tweet-news : pearson = 0.2102, spearman = 0.2721
2019-02-12 19:41:30,697 : ALL (weighted average) : Pearson = 0.1720,             Spearman = 0.2469
2019-02-12 19:41:30,697 : ALL (average) : Pearson = 0.1891,             Spearman = 0.2651

2019-02-12 19:41:30,697 : ***** Transfer task : STS15 *****


2019-02-12 19:41:30,732 : loading BERT mode bert-base-uncased
2019-02-12 19:41:30,732 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:41:30,749 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:41:30,749 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa5mm5e6v
2019-02-12 19:41:33,222 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:41:35,622 : answers-forums : pearson = 0.2032, spearman = 0.2481
2019-02-12 19:41:37,188 : answers-students : pearson = 0.2900, spearman = 0.3214
2019-02-12 19:41:38,461 : belief : pearson = 0.2431, spearman = 0.3387
2019-02-12 19:41:40,078 : headlines : pearson = 0.1952, spearman = 0.3342
2019-02-12 19:41:41,637 : images : pearson = 0.0614, spearman = 0.3329
2019-02-12 19:41:41,637 : ALL (weighted average) : Pearson = 0.1924,             Spearman = 0.3205
2019-02-12 19:41:41,637 : ALL (average) : Pearson = 0.1986,             Spearman = 0.3151

2019-02-12 19:41:41,637 : ***** Transfer task : STS16 *****


2019-02-12 19:41:41,705 : loading BERT mode bert-base-uncased
2019-02-12 19:41:41,705 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:41:41,722 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:41:41,722 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5bp5ld77
2019-02-12 19:41:44,154 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:41:46,227 : answer-answer : pearson = 0.1994, spearman = 0.2954
2019-02-12 19:41:46,723 : headlines : pearson = 0.2209, spearman = 0.4161
2019-02-12 19:41:47,297 : plagiarism : pearson = 0.3155, spearman = 0.4268
2019-02-12 19:41:48,160 : postediting : pearson = 0.4109, spearman = 0.7151
2019-02-12 19:41:48,648 : question-question : pearson = -0.0203, spearman = -0.0059
2019-02-12 19:41:48,648 : ALL (weighted average) : Pearson = 0.2312,             Spearman = 0.3795
2019-02-12 19:41:48,648 : ALL (average) : Pearson = 0.2253,             Spearman = 0.3695

2019-02-12 19:41:48,649 : ***** Transfer task : MR *****


2019-02-12 19:41:48,666 : loading BERT mode bert-base-uncased
2019-02-12 19:41:48,666 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:41:48,684 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:41:48,685 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa07sxs4m
2019-02-12 19:41:51,105 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:41:52,582 : Generating sentence embeddings
2019-02-12 19:42:11,729 : Generated sentence embeddings
2019-02-12 19:42:11,729 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:42:39,771 : Best param found at split 1: l2reg = 1e-05                 with score 66.87
2019-02-12 19:43:06,172 : Best param found at split 2: l2reg = 1e-05                 with score 63.61
2019-02-12 19:43:31,993 : Best param found at split 3: l2reg = 0.001                 with score 62.61
2019-02-12 19:44:06,550 : Best param found at split 4: l2reg = 0.0001                 with score 64.14
2019-02-12 19:44:41,141 : Best param found at split 5: l2reg = 1e-05                 with score 61.66
2019-02-12 19:44:43,107 : Dev acc : 63.78 Test acc : 63.21

2019-02-12 19:44:43,108 : ***** Transfer task : CR *****


2019-02-12 19:44:43,115 : loading BERT mode bert-base-uncased
2019-02-12 19:44:43,115 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:44:43,135 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:44:43,135 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzdhx2giu
2019-02-12 19:44:45,562 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:44:47,063 : Generating sentence embeddings
2019-02-12 19:44:52,779 : Generated sentence embeddings
2019-02-12 19:44:52,779 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:45:01,851 : Best param found at split 1: l2reg = 0.0001                 with score 66.51
2019-02-12 19:45:17,959 : Best param found at split 2: l2reg = 0.001                 with score 66.68
2019-02-12 19:45:24,824 : Best param found at split 3: l2reg = 1e-05                 with score 69.44
2019-02-12 19:45:29,852 : Best param found at split 4: l2reg = 1e-05                 with score 68.22
2019-02-12 19:45:38,049 : Best param found at split 5: l2reg = 0.001                 with score 67.06
2019-02-12 19:45:38,500 : Dev acc : 67.58 Test acc : 65.75

2019-02-12 19:45:38,501 : ***** Transfer task : MPQA *****


2019-02-12 19:45:38,506 : loading BERT mode bert-base-uncased
2019-02-12 19:45:38,506 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:45:38,525 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:45:38,525 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprv7ilibl
2019-02-12 19:45:40,981 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:45:42,552 : Generating sentence embeddings
2019-02-12 19:45:50,977 : Generated sentence embeddings
2019-02-12 19:45:50,978 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:46:10,628 : Best param found at split 1: l2reg = 0.0001                 with score 78.3
2019-02-12 19:46:24,950 : Best param found at split 2: l2reg = 0.001                 with score 81.03
2019-02-12 19:46:51,633 : Best param found at split 3: l2reg = 0.0001                 with score 80.67
2019-02-12 19:47:28,880 : Best param found at split 4: l2reg = 1e-05                 with score 80.0
2019-02-12 19:48:00,793 : Best param found at split 5: l2reg = 0.001                 with score 81.62
2019-02-12 19:48:02,669 : Dev acc : 80.32 Test acc : 82.04

2019-02-12 19:48:02,669 : ***** Transfer task : SUBJ *****


2019-02-12 19:48:02,685 : loading BERT mode bert-base-uncased
2019-02-12 19:48:02,685 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:48:02,704 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:48:02,704 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn_l6amqq
2019-02-12 19:48:05,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:48:06,572 : Generating sentence embeddings
2019-02-12 19:48:27,732 : Generated sentence embeddings
2019-02-12 19:48:27,733 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:48:58,843 : Best param found at split 1: l2reg = 0.001                 with score 89.32
2019-02-12 19:49:27,528 : Best param found at split 2: l2reg = 1e-05                 with score 90.18
2019-02-12 19:49:55,447 : Best param found at split 3: l2reg = 0.0001                 with score 89.92
2019-02-12 19:50:27,060 : Best param found at split 4: l2reg = 1e-05                 with score 89.52
2019-02-12 19:50:55,485 : Best param found at split 5: l2reg = 0.0001                 with score 89.15
2019-02-12 19:50:57,166 : Dev acc : 89.62 Test acc : 89.34

2019-02-12 19:50:57,167 : ***** Transfer task : SST Binary classification *****


2019-02-12 19:50:57,294 : loading BERT mode bert-base-uncased
2019-02-12 19:50:57,294 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:50:57,317 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:50:57,317 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpedtyvt24
2019-02-12 19:50:59,750 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:51:01,172 : Computing embedding for train
2019-02-12 19:52:12,735 : Computed train embeddings
2019-02-12 19:52:12,735 : Computing embedding for dev
2019-02-12 19:52:13,937 : Computed dev embeddings
2019-02-12 19:52:13,937 : Computing embedding for test
2019-02-12 19:52:16,496 : Computed test embeddings
2019-02-12 19:52:16,496 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:52:58,110 : [('reg:1e-05', 76.61), ('reg:0.0001', 76.61), ('reg:0.001', 72.02), ('reg:0.01', 68.12)]
2019-02-12 19:52:58,110 : Validation : best param found is reg = 1e-05 with score             76.61
2019-02-12 19:52:58,110 : Evaluating...
2019-02-12 19:53:15,701 : 
Dev acc : 76.61 Test acc : 77.32 for             SST Binary classification

2019-02-12 19:53:15,708 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 19:53:15,756 : loading BERT mode bert-base-uncased
2019-02-12 19:53:15,756 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:53:15,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:53:15,778 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphvjku5l0
2019-02-12 19:53:18,211 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:53:19,692 : Computing embedding for train
2019-02-12 19:53:34,791 : Computed train embeddings
2019-02-12 19:53:34,791 : Computing embedding for dev
2019-02-12 19:53:36,840 : Computed dev embeddings
2019-02-12 19:53:36,840 : Computing embedding for test
2019-02-12 19:53:40,900 : Computed test embeddings
2019-02-12 19:53:40,900 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:53:48,497 : [('reg:1e-05', 33.97), ('reg:0.0001', 33.79), ('reg:0.001', 33.7), ('reg:0.01', 30.06)]
2019-02-12 19:53:48,497 : Validation : best param found is reg = 1e-05 with score             33.97
2019-02-12 19:53:48,497 : Evaluating...
2019-02-12 19:53:50,712 : 
Dev acc : 33.97 Test acc : 35.29 for             SST Fine-Grained classification

2019-02-12 19:53:50,712 : ***** Transfer task : TREC *****


2019-02-12 19:53:50,725 : loading BERT mode bert-base-uncased
2019-02-12 19:53:50,726 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:53:50,744 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:53:50,745 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplv0vn19g
2019-02-12 19:53:53,169 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:54:02,917 : Computed train embeddings
2019-02-12 19:54:03,649 : Computed test embeddings
2019-02-12 19:54:03,649 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 19:54:24,658 : [('reg:1e-05', 55.58), ('reg:0.0001', 54.83), ('reg:0.001', 49.45), ('reg:0.01', 35.03)]
2019-02-12 19:54:24,658 : Cross-validation : best param found is reg = 1e-05             with score 55.58
2019-02-12 19:54:24,658 : Evaluating...
2019-02-12 19:54:25,872 : 
Dev acc : 55.58 Test acc : 70.6             for TREC

2019-02-12 19:54:25,873 : ***** Transfer task : MRPC *****


2019-02-12 19:54:25,925 : loading BERT mode bert-base-uncased
2019-02-12 19:54:25,925 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:54:25,945 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:54:25,945 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfble_swg
2019-02-12 19:54:28,352 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:54:29,819 : Computing embedding for train
2019-02-12 19:54:44,143 : Computed train embeddings
2019-02-12 19:54:44,143 : Computing embedding for test
2019-02-12 19:54:49,606 : Computed test embeddings
2019-02-12 19:54:49,622 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 19:54:58,441 : [('reg:1e-05', 69.92), ('reg:0.0001', 69.06), ('reg:0.001', 69.26), ('reg:0.01', 68.64)]
2019-02-12 19:54:58,441 : Cross-validation : best param found is reg = 1e-05             with score 69.92
2019-02-12 19:54:58,441 : Evaluating...
2019-02-12 19:54:59,159 : Dev acc : 69.92 Test acc 66.78; Test F1 72.78 for MRPC.

2019-02-12 19:54:59,159 : ***** Transfer task : SICK-Entailment*****


2019-02-12 19:54:59,182 : loading BERT mode bert-base-uncased
2019-02-12 19:54:59,182 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:54:59,238 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:54:59,238 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt7mpyqyf
2019-02-12 19:55:01,672 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:55:03,080 : Computing embedding for train
2019-02-12 19:55:11,665 : Computed train embeddings
2019-02-12 19:55:11,665 : Computing embedding for dev
2019-02-12 19:55:12,796 : Computed dev embeddings
2019-02-12 19:55:12,796 : Computing embedding for test
2019-02-12 19:55:22,283 : Computed test embeddings
2019-02-12 19:55:22,310 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:55:24,710 : [('reg:1e-05', 67.2), ('reg:0.0001', 67.8), ('reg:0.001', 62.6), ('reg:0.01', 56.4)]
2019-02-12 19:55:24,711 : Validation : best param found is reg = 0.0001 with score             67.8
2019-02-12 19:55:24,711 : Evaluating...
2019-02-12 19:55:25,345 : 
Dev acc : 67.8 Test acc : 66.02 for                        SICK entailment

2019-02-12 19:55:25,345 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 19:55:25,372 : loading BERT mode bert-base-uncased
2019-02-12 19:55:25,372 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:55:25,391 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:55:25,391 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjat0k8ys
2019-02-12 19:55:27,829 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:55:29,301 : Computing embedding for train
2019-02-12 19:55:37,377 : Computed train embeddings
2019-02-12 19:55:37,377 : Computing embedding for dev
2019-02-12 19:55:38,390 : Computed dev embeddings
2019-02-12 19:55:38,391 : Computing embedding for test
2019-02-12 19:55:47,160 : Computed test embeddings
2019-02-12 19:56:50,564 : Dev : Pearson 0.7366201715154551
2019-02-12 19:56:50,565 : Test : Pearson 0.7360099592786526 Spearman 0.6785485397063359 MSE 0.46972610856880587                        for SICK Relatedness

2019-02-12 19:56:50,565 : 

***** Transfer task : STSBenchmark*****


2019-02-12 19:56:50,644 : loading BERT mode bert-base-uncased
2019-02-12 19:56:50,645 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:56:50,664 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:56:50,665 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpet9met3h
2019-02-12 19:56:53,074 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:56:54,550 : Computing embedding for train
2019-02-12 19:57:10,283 : Computed train embeddings
2019-02-12 19:57:10,283 : Computing embedding for dev
2019-02-12 19:57:14,827 : Computed dev embeddings
2019-02-12 19:57:14,827 : Computing embedding for test
2019-02-12 19:57:18,815 : Computed test embeddings
2019-02-12 19:58:23,897 : Dev : Pearson 0.6165401906253217
2019-02-12 19:58:23,897 : Test : Pearson 0.5297957746046918 Spearman 0.5172197936854424 MSE 1.9830345758340644                        for SICK Relatedness

2019-02-12 19:58:23,897 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 19:58:24,142 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 19:58:24,152 : loading BERT mode bert-base-uncased
2019-02-12 19:58:24,152 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:58:24,246 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:58:24,246 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaziipaqw
2019-02-12 19:58:26,687 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:58:28,101 : Computing embeddings for train/dev/test
2019-02-12 20:00:42,331 : Computed embeddings
2019-02-12 20:00:42,331 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:02:04,320 : [('reg:1e-05', 82.1), ('reg:0.0001', 83.51), ('reg:0.001', 76.18), ('reg:0.01', 65.05)]
2019-02-12 20:02:04,321 : Validation : best param found is reg = 0.0001 with score             83.51
2019-02-12 20:02:04,321 : Evaluating...
2019-02-12 20:02:22,335 : 
Dev acc : 83.5 Test acc : 84.2 for LENGTH classification

2019-02-12 20:02:22,344 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 20:02:22,814 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 20:02:22,858 : loading BERT mode bert-base-uncased
2019-02-12 20:02:22,859 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:02:22,888 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:02:22,888 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppybxrsxi
2019-02-12 20:02:25,334 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:02:26,761 : Computing embeddings for train/dev/test
2019-02-12 20:04:44,621 : Computed embeddings
2019-02-12 20:04:44,622 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:06:21,453 : [('reg:1e-05', 2.09), ('reg:0.0001', 0.54), ('reg:0.001', 0.22), ('reg:0.01', 0.18)]
2019-02-12 20:06:21,453 : Validation : best param found is reg = 1e-05 with score             2.09
2019-02-12 20:06:21,454 : Evaluating...
2019-02-12 20:06:39,072 : 
Dev acc : 2.1 Test acc : 2.2 for WORDCONTENT classification

2019-02-12 20:06:39,081 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 20:06:39,595 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 20:06:39,660 : loading BERT mode bert-base-uncased
2019-02-12 20:06:39,660 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:06:39,686 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:06:39,686 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm0jqj2zd
2019-02-12 20:06:42,168 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:06:43,584 : Computing embeddings for train/dev/test
2019-02-12 20:08:52,428 : Computed embeddings
2019-02-12 20:08:52,428 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:10:06,808 : [('reg:1e-05', 30.53), ('reg:0.0001', 28.9), ('reg:0.001', 26.21), ('reg:0.01', 24.8)]
2019-02-12 20:10:06,809 : Validation : best param found is reg = 1e-05 with score             30.53
2019-02-12 20:10:06,809 : Evaluating...
2019-02-12 20:10:29,816 : 
Dev acc : 30.5 Test acc : 30.9 for DEPTH classification

2019-02-12 20:10:29,824 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 20:10:30,357 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 20:10:30,417 : loading BERT mode bert-base-uncased
2019-02-12 20:10:30,418 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:10:30,442 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:10:30,443 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp241efdxa
2019-02-12 20:10:32,888 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:10:34,462 : Computing embeddings for train/dev/test
2019-02-12 20:12:49,624 : Computed embeddings
2019-02-12 20:12:49,624 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:14:58,973 : [('reg:1e-05', 51.96), ('reg:0.0001', 50.08), ('reg:0.001', 36.04), ('reg:0.01', 17.24)]
2019-02-12 20:14:58,974 : Validation : best param found is reg = 1e-05 with score             51.96
2019-02-12 20:14:58,974 : Evaluating...
2019-02-12 20:15:29,306 : 
Dev acc : 52.0 Test acc : 52.6 for TOPCONSTITUENTS classification

2019-02-12 20:15:29,314 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 20:15:29,694 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 20:15:29,760 : loading BERT mode bert-base-uncased
2019-02-12 20:15:29,760 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:15:29,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:15:29,791 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1mydnr9k
2019-02-12 20:15:32,275 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:15:33,754 : Computing embeddings for train/dev/test
2019-02-12 20:17:48,029 : Computed embeddings
2019-02-12 20:17:48,029 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:19:14,680 : [('reg:1e-05', 50.03), ('reg:0.0001', 50.62), ('reg:0.001', 50.08), ('reg:0.01', 50.0)]
2019-02-12 20:19:14,680 : Validation : best param found is reg = 0.0001 with score             50.62
2019-02-12 20:19:14,680 : Evaluating...
2019-02-12 20:19:36,333 : 
Dev acc : 50.6 Test acc : 50.8 for BIGRAMSHIFT classification

2019-02-12 20:19:36,341 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 20:19:36,724 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 20:19:36,788 : loading BERT mode bert-base-uncased
2019-02-12 20:19:36,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:19:36,902 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:19:36,902 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpddumpkf6
2019-02-12 20:19:39,375 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:19:40,834 : Computing embeddings for train/dev/test
2019-02-12 20:21:54,183 : Computed embeddings
2019-02-12 20:21:54,183 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:23:45,618 : [('reg:1e-05', 86.3), ('reg:0.0001', 86.35), ('reg:0.001', 85.32), ('reg:0.01', 79.47)]
2019-02-12 20:23:45,618 : Validation : best param found is reg = 0.0001 with score             86.35
2019-02-12 20:23:45,618 : Evaluating...
2019-02-12 20:24:04,870 : 
Dev acc : 86.3 Test acc : 83.7 for TENSE classification

2019-02-12 20:24:04,878 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 20:24:05,461 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 20:24:05,524 : loading BERT mode bert-base-uncased
2019-02-12 20:24:05,525 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:24:05,552 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:24:05,552 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvn05_1ua
2019-02-12 20:24:07,976 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:24:09,434 : Computing embeddings for train/dev/test
2019-02-12 20:26:29,361 : Computed embeddings
2019-02-12 20:26:29,362 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:28:16,694 : [('reg:1e-05', 79.58), ('reg:0.0001', 80.38), ('reg:0.001', 78.38), ('reg:0.01', 69.26)]
2019-02-12 20:28:16,694 : Validation : best param found is reg = 0.0001 with score             80.38
2019-02-12 20:28:16,694 : Evaluating...
2019-02-12 20:28:41,625 : 
Dev acc : 80.4 Test acc : 79.3 for SUBJNUMBER classification

2019-02-12 20:28:41,633 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 20:28:42,024 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 20:28:42,090 : loading BERT mode bert-base-uncased
2019-02-12 20:28:42,090 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:28:42,205 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:28:42,205 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpguhf2hbj
2019-02-12 20:28:44,632 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:28:46,116 : Computing embeddings for train/dev/test
2019-02-12 20:31:01,056 : Computed embeddings
2019-02-12 20:31:01,057 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:32:11,486 : [('reg:1e-05', 76.57), ('reg:0.0001', 73.58), ('reg:0.001', 72.42), ('reg:0.01', 57.15)]
2019-02-12 20:32:11,486 : Validation : best param found is reg = 1e-05 with score             76.57
2019-02-12 20:32:11,486 : Evaluating...
2019-02-12 20:32:38,760 : 
Dev acc : 76.6 Test acc : 77.7 for OBJNUMBER classification

2019-02-12 20:32:38,769 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 20:32:39,330 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 20:32:39,399 : loading BERT mode bert-base-uncased
2019-02-12 20:32:39,399 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:32:39,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:32:39,427 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3zm1z9fv
2019-02-12 20:32:41,858 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:32:43,326 : Computing embeddings for train/dev/test
2019-02-12 20:35:15,581 : Computed embeddings
2019-02-12 20:35:15,582 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:36:06,364 : [('reg:1e-05', 50.71), ('reg:0.0001', 50.64), ('reg:0.001', 50.41), ('reg:0.01', 51.93)]
2019-02-12 20:36:06,365 : Validation : best param found is reg = 0.01 with score             51.93
2019-02-12 20:36:06,365 : Evaluating...
2019-02-12 20:36:23,087 : 
Dev acc : 51.9 Test acc : 51.2 for ODDMANOUT classification

2019-02-12 20:36:23,095 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 20:36:23,683 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 20:36:23,759 : loading BERT mode bert-base-uncased
2019-02-12 20:36:23,759 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:36:23,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:36:23,790 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl1la69g4
2019-02-12 20:36:26,222 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:36:27,669 : Computing embeddings for train/dev/test
2019-02-12 20:38:59,712 : Computed embeddings
2019-02-12 20:38:59,713 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:40:14,399 : [('reg:1e-05', 50.2), ('reg:0.0001', 51.28), ('reg:0.001', 50.21), ('reg:0.01', 50.0)]
2019-02-12 20:40:14,399 : Validation : best param found is reg = 0.0001 with score             51.28
2019-02-12 20:40:14,399 : Evaluating...
2019-02-12 20:40:48,592 : 
Dev acc : 51.3 Test acc : 51.1 for COORDINATIONINVERSION classification

2019-02-12 20:40:48,601 : {'STS12': {'MSRpar': {'pearson': (0.20957768183758127, 6.855901076166273e-09), 'spearman': SpearmanrResult(correlation=0.24741431001937889, pvalue=6.351686450790725e-12), 'nsamples': 750}, 'MSRvid': {'pearson': (0.18471185471575277, 3.505861737099239e-07), 'spearman': SpearmanrResult(correlation=0.20121675418462498, pvalue=2.7240956444257893e-08), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.41281892468462666, 2.5743881745885668e-20), 'spearman': SpearmanrResult(correlation=0.5718226882234826, pvalue=3.2867977380275963e-41), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.17988673811149283, 7.097050513707926e-07), 'spearman': SpearmanrResult(correlation=0.20657415984555558, pvalue=1.1329284525694145e-08), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4148563456234101, 4.980731049406594e-18), 'spearman': SpearmanrResult(correlation=0.3423739796529586, pvalue=2.0495124066658055e-12), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.2803703089945727, 'wmean': 0.25278113717265266}, 'spearman': {'mean': 0.3138803783852001, 'wmean': 0.2865115025139249}}}, 'STS13': {'FNWN': {'pearson': (0.24586318113138844, 0.0006493101505026304), 'spearman': SpearmanrResult(correlation=0.25793242628177665, pvalue=0.0003392994535591399), 'nsamples': 189}, 'headlines': {'pearson': (0.07875680340889082, 0.03103708016774191), 'spearman': SpearmanrResult(correlation=0.22852501310264486, pvalue=2.4208034834259683e-10), 'nsamples': 750}, 'OnWN': {'pearson': (0.12811032494546332, 0.0023648612978812667), 'spearman': SpearmanrResult(correlation=0.12313483392889595, pvalue=0.003487742307300251), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.1509101031619142, 'wmean': 0.11827042405660362}, 'spearman': {'mean': 0.20319742443777247, 'wmean': 0.19281442015223338}}}, 'STS14': {'deft-forum': {'pearson': (0.11538986797424855, 0.014318720628070786), 'spearman': SpearmanrResult(correlation=0.15759037257978045, pvalue=0.0007945580345750862), 'nsamples': 450}, 'deft-news': {'pearson': (0.3808147918343299, 8.650564732105277e-12), 'spearman': SpearmanrResult(correlation=0.48903093589926244, pvalue=1.9234982512927177e-19), 'nsamples': 300}, 'headlines': {'pearson': (0.1046278100233268, 0.004124914763125461), 'spearman': SpearmanrResult(correlation=0.1956135268678275, pvalue=6.648598509658943e-08), 'nsamples': 750}, 'images': {'pearson': (0.16331443292658118, 6.947057618015882e-06), 'spearman': SpearmanrResult(correlation=0.280788783809463, pvalue=4.686484727978056e-15), 'nsamples': 750}, 'OnWN': {'pearson': (0.1604798190674563, 1.0043078046707245e-05), 'spearman': SpearmanrResult(correlation=0.19565235632519398, pvalue=6.608200588558437e-08), 'nsamples': 750}, 'tweet-news': {'pearson': (0.21016073181674608, 6.213560278373888e-09), 'spearman': SpearmanrResult(correlation=0.2721255406021118, pvalue=3.360828208949676e-14), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.18913124227378145, 'wmean': 0.1720285262704783}, 'spearman': {'mean': 0.2651335860139399, 'wmean': 0.24686936110243388}}}, 'STS15': {'answers-forums': {'pearson': (0.20323692246214264, 7.371232774080578e-05), 'spearman': SpearmanrResult(correlation=0.248123586044143, pvalue=1.1437554771474135e-06), 'nsamples': 375}, 'answers-students': {'pearson': (0.2899532222833136, 5.400155565378655e-16), 'spearman': SpearmanrResult(correlation=0.32144379626940583, pvalue=1.730463249037383e-19), 'nsamples': 750}, 'belief': {'pearson': (0.24307959719984862, 1.9061256308860306e-06), 'spearman': SpearmanrResult(correlation=0.33870375932452595, pvalue=1.6142628611099424e-11), 'nsamples': 375}, 'headlines': {'pearson': (0.19519008016870132, 7.10494099375917e-08), 'spearman': SpearmanrResult(correlation=0.3342449351273671, pvalue=4.943182389278818e-21), 'nsamples': 750}, 'images': {'pearson': (0.06142596476738297, 0.0927627072967018), 'spearman': SpearmanrResult(correlation=0.3328533172725625, pvalue=7.335689625056953e-21), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.19857715737627782, 'wmean': 0.1924318817625984}, 'spearman': {'mean': 0.3150738788076009, 'wmean': 0.3204889303384175}}}, 'STS16': {'answer-answer': {'pearson': (0.19935456198975998, 0.0014049382306962708), 'spearman': SpearmanrResult(correlation=0.2953564743200639, pvalue=1.6569713189231106e-06), 'nsamples': 254}, 'headlines': {'pearson': (0.22091862588123087, 0.00044476783842506515), 'spearman': SpearmanrResult(correlation=0.41609470221111844, pvalue=7.606418081870445e-12), 'nsamples': 249}, 'plagiarism': {'pearson': (0.31548401382228575, 1.0407160095857589e-06), 'spearman': SpearmanrResult(correlation=0.4267501919902511, pvalue=1.3523522996894843e-11), 'nsamples': 230}, 'postediting': {'pearson': (0.4109399739539846, 2.329614183774486e-11), 'spearman': SpearmanrResult(correlation=0.7150904669175994, pvalue=1.6665324094692384e-39), 'nsamples': 244}, 'question-question': {'pearson': (-0.02034711293960753, 0.7699648165834302), 'spearman': SpearmanrResult(correlation=-0.0058821611953608485, pvalue=0.9326365254824067), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.22527001254153073, 'wmean': 0.2312166330601564}, 'spearman': {'mean': 0.3694819348487344, 'wmean': 0.3794547822290778}}}, 'MR': {'devacc': 63.78, 'acc': 63.21, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 67.58, 'acc': 65.75, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.32, 'acc': 82.04, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 89.62, 'acc': 89.34, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.61, 'acc': 77.32, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 33.97, 'acc': 35.29, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 55.58, 'acc': 70.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.92, 'acc': 66.78, 'f1': 72.78, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 67.8, 'acc': 66.02, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7366201715154551, 'pearson': 0.7360099592786526, 'spearman': 0.6785485397063359, 'mse': 0.46972610856880587, 'yhat': array([2.0335987 , 3.76765732, 2.07680388, ..., 3.32845714, 4.45162953,
       4.55134006]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6165401906253217, 'pearson': 0.5297957746046918, 'spearman': 0.5172197936854424, 'mse': 1.9830345758340644, 'yhat': array([2.68352167, 2.41053469, 2.11342493, ..., 3.5773854 , 3.62337375,
       3.32789398]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 83.51, 'acc': 84.18, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 2.09, 'acc': 2.17, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.53, 'acc': 30.85, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 51.96, 'acc': 52.56, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.62, 'acc': 50.79, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.35, 'acc': 83.71, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.38, 'acc': 79.3, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.57, 'acc': 77.65, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 51.93, 'acc': 51.17, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 51.28, 'acc': 51.13, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 20:40:48,601 : ********************************************************************************
2019-02-12 20:40:48,601 : ********************************************************************************
2019-02-12 20:40:48,601 : ********************************************************************************
2019-02-12 20:40:48,601 : layer 3
2019-02-12 20:40:48,601 : ********************************************************************************
2019-02-12 20:40:48,601 : ********************************************************************************
2019-02-12 20:40:48,601 : ********************************************************************************
2019-02-12 20:40:48,685 : ***** Transfer task : STS12 *****


2019-02-12 20:40:48,697 : loading BERT mode bert-base-uncased
2019-02-12 20:40:48,697 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:40:48,714 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:40:48,715 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp069ygh9f
2019-02-12 20:40:51,131 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:40:56,132 : MSRpar : pearson = 0.2095, spearman = 0.2580
2019-02-12 20:40:59,009 : MSRvid : pearson = 0.1817, spearman = 0.2457
2019-02-12 20:41:00,986 : SMTeuroparl : pearson = 0.4043, spearman = 0.5703
2019-02-12 20:41:04,394 : surprise.OnWN : pearson = 0.1086, spearman = 0.1731
2019-02-12 20:41:05,898 : surprise.SMTnews : pearson = 0.4768, spearman = 0.3707
2019-02-12 20:41:05,898 : ALL (weighted average) : Pearson = 0.2415,             Spearman = 0.2951
2019-02-12 20:41:05,898 : ALL (average) : Pearson = 0.2762,             Spearman = 0.3236

2019-02-12 20:41:05,898 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 20:41:05,908 : loading BERT mode bert-base-uncased
2019-02-12 20:41:05,908 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:41:05,925 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:41:05,925 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmazqlk9e
2019-02-12 20:41:08,317 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:41:11,042 : FNWN : pearson = 0.2314, spearman = 0.2417
2019-02-12 20:41:14,198 : headlines : pearson = 0.0666, spearman = 0.2287
2019-02-12 20:41:16,502 : OnWN : pearson = 0.1808, spearman = 0.1936
2019-02-12 20:41:16,502 : ALL (weighted average) : Pearson = 0.1301,             Spearman = 0.2172
2019-02-12 20:41:16,502 : ALL (average) : Pearson = 0.1596,             Spearman = 0.2213

2019-02-12 20:41:16,502 : ***** Transfer task : STS14 *****


2019-02-12 20:41:16,518 : loading BERT mode bert-base-uncased
2019-02-12 20:41:16,518 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:41:16,536 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:41:16,536 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphj5h8rj4
2019-02-12 20:41:18,921 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:41:22,140 : deft-forum : pearson = 0.1024, spearman = 0.1631
2019-02-12 20:41:23,453 : deft-news : pearson = 0.3203, spearman = 0.5082
2019-02-12 20:41:25,986 : headlines : pearson = 0.0884, spearman = 0.2009
2019-02-12 20:41:27,352 : images : pearson = 0.1570, spearman = 0.2858
2019-02-12 20:41:28,750 : OnWN : pearson = 0.1698, spearman = 0.2143
2019-02-12 20:41:30,453 : tweet-news : pearson = 0.1509, spearman = 0.2409
2019-02-12 20:41:30,453 : ALL (weighted average) : Pearson = 0.1511,             Spearman = 0.2486
2019-02-12 20:41:30,453 : ALL (average) : Pearson = 0.1648,             Spearman = 0.2689

2019-02-12 20:41:30,454 : ***** Transfer task : STS15 *****


2019-02-12 20:41:30,486 : loading BERT mode bert-base-uncased
2019-02-12 20:41:30,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:41:30,504 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:41:30,505 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqf5_alme
2019-02-12 20:41:32,969 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:41:35,454 : answers-forums : pearson = 0.1666, spearman = 0.2199
2019-02-12 20:41:36,835 : answers-students : pearson = 0.2645, spearman = 0.3200
2019-02-12 20:41:37,930 : belief : pearson = 0.1637, spearman = 0.3282
2019-02-12 20:41:39,371 : headlines : pearson = 0.1780, spearman = 0.3409
2019-02-12 20:41:40,771 : images : pearson = 0.0057, spearman = 0.3311
2019-02-12 20:41:40,772 : ALL (weighted average) : Pearson = 0.1533,             Spearman = 0.3165
2019-02-12 20:41:40,772 : ALL (average) : Pearson = 0.1557,             Spearman = 0.3080

2019-02-12 20:41:40,772 : ***** Transfer task : STS16 *****


2019-02-12 20:41:40,842 : loading BERT mode bert-base-uncased
2019-02-12 20:41:40,842 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:41:40,860 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:41:40,861 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_scu66hh
2019-02-12 20:41:43,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:41:45,205 : answer-answer : pearson = 0.1853, spearman = 0.3211
2019-02-12 20:41:45,652 : headlines : pearson = 0.1928, spearman = 0.4197
2019-02-12 20:41:46,178 : plagiarism : pearson = 0.2437, spearman = 0.4166
2019-02-12 20:41:46,950 : postediting : pearson = 0.3143, spearman = 0.7303
2019-02-12 20:41:47,352 : question-question : pearson = 0.0586, spearman = 0.1374
2019-02-12 20:41:47,352 : ALL (weighted average) : Pearson = 0.2024,             Spearman = 0.4121
2019-02-12 20:41:47,352 : ALL (average) : Pearson = 0.1989,             Spearman = 0.4050

2019-02-12 20:41:47,352 : ***** Transfer task : MR *****


2019-02-12 20:41:47,369 : loading BERT mode bert-base-uncased
2019-02-12 20:41:47,369 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:41:47,389 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:41:47,389 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpub5x5s7c
2019-02-12 20:41:49,821 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:41:51,259 : Generating sentence embeddings
2019-02-12 20:42:07,826 : Generated sentence embeddings
2019-02-12 20:42:07,827 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:42:29,291 : Best param found at split 1: l2reg = 0.0001                 with score 64.07
2019-02-12 20:42:51,912 : Best param found at split 2: l2reg = 1e-05                 with score 65.03
2019-02-12 20:43:26,970 : Best param found at split 3: l2reg = 1e-05                 with score 61.84
2019-02-12 20:44:00,592 : Best param found at split 4: l2reg = 0.0001                 with score 63.01
2019-02-12 20:44:41,118 : Best param found at split 5: l2reg = 0.0001                 with score 64.29
2019-02-12 20:44:44,048 : Dev acc : 63.65 Test acc : 66.26

2019-02-12 20:44:44,049 : ***** Transfer task : CR *****


2019-02-12 20:44:44,057 : loading BERT mode bert-base-uncased
2019-02-12 20:44:44,057 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:44:44,076 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:44:44,077 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1l8ffuef
2019-02-12 20:44:46,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:44:48,015 : Generating sentence embeddings
2019-02-12 20:44:54,223 : Generated sentence embeddings
2019-02-12 20:44:54,223 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:45:03,136 : Best param found at split 1: l2reg = 0.01                 with score 68.33
2019-02-12 20:45:11,937 : Best param found at split 2: l2reg = 0.0001                 with score 65.85
2019-02-12 20:45:18,094 : Best param found at split 3: l2reg = 0.0001                 with score 69.74
2019-02-12 20:45:22,272 : Best param found at split 4: l2reg = 0.0001                 with score 69.61
2019-02-12 20:45:26,410 : Best param found at split 5: l2reg = 0.0001                 with score 68.49
2019-02-12 20:45:26,666 : Dev acc : 68.4 Test acc : 65.75

2019-02-12 20:45:26,666 : ***** Transfer task : MPQA *****


2019-02-12 20:45:26,671 : loading BERT mode bert-base-uncased
2019-02-12 20:45:26,672 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:45:26,691 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:45:26,691 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwyuzbc22
2019-02-12 20:45:29,122 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:45:30,617 : Generating sentence embeddings
2019-02-12 20:45:36,265 : Generated sentence embeddings
2019-02-12 20:45:36,265 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:45:58,690 : Best param found at split 1: l2reg = 0.0001                 with score 78.21
2019-02-12 20:46:14,992 : Best param found at split 2: l2reg = 0.01                 with score 78.55
2019-02-12 20:46:41,535 : Best param found at split 3: l2reg = 0.0001                 with score 80.9
2019-02-12 20:47:18,434 : Best param found at split 4: l2reg = 1e-05                 with score 80.02
2019-02-12 20:48:12,305 : Best param found at split 5: l2reg = 0.0001                 with score 82.09
2019-02-12 20:48:15,556 : Dev acc : 79.95 Test acc : 82.32

2019-02-12 20:48:15,557 : ***** Transfer task : SUBJ *****


2019-02-12 20:48:15,571 : loading BERT mode bert-base-uncased
2019-02-12 20:48:15,571 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:48:15,591 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:48:15,591 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps1kxe_vc
2019-02-12 20:48:17,998 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:48:19,557 : Generating sentence embeddings
2019-02-12 20:48:45,173 : Generated sentence embeddings
2019-02-12 20:48:45,173 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:49:19,021 : Best param found at split 1: l2reg = 1e-05                 with score 89.84
2019-02-12 20:49:45,452 : Best param found at split 2: l2reg = 0.001                 with score 90.6
2019-02-12 20:50:01,518 : Best param found at split 3: l2reg = 1e-05                 with score 89.6
2019-02-12 20:50:21,117 : Best param found at split 4: l2reg = 1e-05                 with score 90.19
2019-02-12 20:50:39,520 : Best param found at split 5: l2reg = 0.001                 with score 88.98
2019-02-12 20:50:39,945 : Dev acc : 89.84 Test acc : 87.94

2019-02-12 20:50:39,946 : ***** Transfer task : SST Binary classification *****


2019-02-12 20:50:40,071 : loading BERT mode bert-base-uncased
2019-02-12 20:50:40,071 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:50:40,093 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:50:40,093 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx62y0_4x
2019-02-12 20:50:42,530 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:50:44,016 : Computing embedding for train
2019-02-12 20:52:13,058 : Computed train embeddings
2019-02-12 20:52:13,058 : Computing embedding for dev
2019-02-12 20:52:14,533 : Computed dev embeddings
2019-02-12 20:52:14,534 : Computing embedding for test
2019-02-12 20:52:17,738 : Computed test embeddings
2019-02-12 20:52:17,738 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:52:55,922 : [('reg:1e-05', 74.2), ('reg:0.0001', 74.43), ('reg:0.001', 75.69), ('reg:0.01', 61.58)]
2019-02-12 20:52:55,922 : Validation : best param found is reg = 0.001 with score             75.69
2019-02-12 20:52:55,922 : Evaluating...
2019-02-12 20:53:05,118 : 
Dev acc : 75.69 Test acc : 73.48 for             SST Binary classification

2019-02-12 20:53:05,124 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 20:53:05,173 : loading BERT mode bert-base-uncased
2019-02-12 20:53:05,173 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:53:05,194 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:53:05,195 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpikjiw_q6
2019-02-12 20:53:07,616 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:53:09,079 : Computing embedding for train
2019-02-12 20:53:23,283 : Computed train embeddings
2019-02-12 20:53:23,283 : Computing embedding for dev
2019-02-12 20:53:25,309 : Computed dev embeddings
2019-02-12 20:53:25,309 : Computing embedding for test
2019-02-12 20:53:29,601 : Computed test embeddings
2019-02-12 20:53:29,601 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:53:34,056 : [('reg:1e-05', 35.6), ('reg:0.0001', 31.15), ('reg:0.001', 33.88), ('reg:0.01', 33.7)]
2019-02-12 20:53:34,056 : Validation : best param found is reg = 1e-05 with score             35.6
2019-02-12 20:53:34,056 : Evaluating...
2019-02-12 20:53:35,075 : 
Dev acc : 35.6 Test acc : 36.02 for             SST Fine-Grained classification

2019-02-12 20:53:35,076 : ***** Transfer task : TREC *****


2019-02-12 20:53:35,094 : loading BERT mode bert-base-uncased
2019-02-12 20:53:35,094 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:53:35,120 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:53:35,120 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplejix2mg
2019-02-12 20:53:37,588 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:53:44,325 : Computed train embeddings
2019-02-12 20:53:44,764 : Computed test embeddings
2019-02-12 20:53:44,765 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 20:53:58,585 : [('reg:1e-05', 52.0), ('reg:0.0001', 51.78), ('reg:0.001', 46.0), ('reg:0.01', 38.06)]
2019-02-12 20:53:58,586 : Cross-validation : best param found is reg = 1e-05             with score 52.0
2019-02-12 20:53:58,586 : Evaluating...
2019-02-12 20:53:59,291 : 
Dev acc : 52.0 Test acc : 75.2             for TREC

2019-02-12 20:53:59,292 : ***** Transfer task : MRPC *****


2019-02-12 20:53:59,313 : loading BERT mode bert-base-uncased
2019-02-12 20:53:59,313 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:53:59,333 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:53:59,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2tphpyfn
2019-02-12 20:54:01,766 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:54:03,213 : Computing embedding for train
2019-02-12 20:54:17,333 : Computed train embeddings
2019-02-12 20:54:17,333 : Computing embedding for test
2019-02-12 20:54:22,793 : Computed test embeddings
2019-02-12 20:54:22,809 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 20:54:31,998 : [('reg:1e-05', 69.18), ('reg:0.0001', 68.64), ('reg:0.001', 69.5), ('reg:0.01', 67.86)]
2019-02-12 20:54:31,998 : Cross-validation : best param found is reg = 0.001             with score 69.5
2019-02-12 20:54:31,998 : Evaluating...
2019-02-12 20:54:32,481 : Dev acc : 69.5 Test acc 71.83; Test F1 80.94 for MRPC.

2019-02-12 20:54:32,481 : ***** Transfer task : SICK-Entailment*****


2019-02-12 20:54:32,542 : loading BERT mode bert-base-uncased
2019-02-12 20:54:32,542 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:54:32,562 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:54:32,562 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpafswla5_
2019-02-12 20:54:35,003 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:54:36,428 : Computing embedding for train
2019-02-12 20:54:44,520 : Computed train embeddings
2019-02-12 20:54:44,520 : Computing embedding for dev
2019-02-12 20:54:45,532 : Computed dev embeddings
2019-02-12 20:54:45,532 : Computing embedding for test
2019-02-12 20:54:52,920 : Computed test embeddings
2019-02-12 20:54:52,948 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:54:54,968 : [('reg:1e-05', 68.6), ('reg:0.0001', 69.6), ('reg:0.001', 66.6), ('reg:0.01', 61.2)]
2019-02-12 20:54:54,969 : Validation : best param found is reg = 0.0001 with score             69.6
2019-02-12 20:54:54,969 : Evaluating...
2019-02-12 20:54:55,523 : 
Dev acc : 69.6 Test acc : 67.77 for                        SICK entailment

2019-02-12 20:54:55,523 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 20:54:55,550 : loading BERT mode bert-base-uncased
2019-02-12 20:54:55,550 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:54:55,607 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:54:55,607 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj8bkw0h3
2019-02-12 20:54:58,001 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:54:59,496 : Computing embedding for train
2019-02-12 20:55:10,719 : Computed train embeddings
2019-02-12 20:55:10,719 : Computing embedding for dev
2019-02-12 20:55:11,925 : Computed dev embeddings
2019-02-12 20:55:11,925 : Computing embedding for test
2019-02-12 20:55:23,217 : Computed test embeddings
2019-02-12 20:56:54,745 : Dev : Pearson 0.7572475777026654
2019-02-12 20:56:54,747 : Test : Pearson 0.7527080905415069 Spearman 0.6883958069583715 MSE 0.4448644346280351                        for SICK Relatedness

2019-02-12 20:56:54,748 : 

***** Transfer task : STSBenchmark*****


2019-02-12 20:56:54,814 : loading BERT mode bert-base-uncased
2019-02-12 20:56:54,814 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:56:54,833 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:56:54,833 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw50rqk68
2019-02-12 20:56:57,270 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:56:58,722 : Computing embedding for train
2019-02-12 20:57:10,140 : Computed train embeddings
2019-02-12 20:57:10,140 : Computing embedding for dev
2019-02-12 20:57:13,426 : Computed dev embeddings
2019-02-12 20:57:13,427 : Computing embedding for test
2019-02-12 20:57:16,182 : Computed test embeddings
2019-02-12 20:57:58,958 : Dev : Pearson 0.6342857221656193
2019-02-12 20:57:58,958 : Test : Pearson 0.5413199282897899 Spearman 0.5347957204141336 MSE 1.9346301160062207                        for SICK Relatedness

2019-02-12 20:57:58,958 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 20:57:59,270 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 20:57:59,281 : loading BERT mode bert-base-uncased
2019-02-12 20:57:59,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:57:59,305 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:57:59,305 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaw0kxjqp
2019-02-12 20:58:01,743 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:58:03,240 : Computing embeddings for train/dev/test
2019-02-12 21:00:44,611 : Computed embeddings
2019-02-12 21:00:44,611 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:01:33,742 : [('reg:1e-05', 87.25), ('reg:0.0001', 86.75), ('reg:0.001', 80.58), ('reg:0.01', 49.72)]
2019-02-12 21:01:33,742 : Validation : best param found is reg = 1e-05 with score             87.25
2019-02-12 21:01:33,742 : Evaluating...
2019-02-12 21:02:03,786 : 
Dev acc : 87.2 Test acc : 87.2 for LENGTH classification

2019-02-12 21:02:03,794 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 21:02:04,140 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 21:02:04,184 : loading BERT mode bert-base-uncased
2019-02-12 21:02:04,184 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:02:04,278 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:02:04,278 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps3klb2za
2019-02-12 21:02:06,668 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:02:08,235 : Computing embeddings for train/dev/test
2019-02-12 21:04:58,605 : Computed embeddings
2019-02-12 21:04:58,605 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:05:55,278 : [('reg:1e-05', 1.99), ('reg:0.0001', 0.69), ('reg:0.001', 0.16), ('reg:0.01', 0.14)]
2019-02-12 21:05:55,278 : Validation : best param found is reg = 1e-05 with score             1.99
2019-02-12 21:05:55,278 : Evaluating...
2019-02-12 21:06:12,921 : 
Dev acc : 2.0 Test acc : 1.9 for WORDCONTENT classification

2019-02-12 21:06:12,929 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 21:06:13,445 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 21:06:13,508 : loading BERT mode bert-base-uncased
2019-02-12 21:06:13,509 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:06:13,533 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:06:13,533 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgo9ssydy
2019-02-12 21:06:15,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:06:17,456 : Computing embeddings for train/dev/test
2019-02-12 21:08:58,312 : Computed embeddings
2019-02-12 21:08:58,312 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:09:44,249 : [('reg:1e-05', 30.11), ('reg:0.0001', 28.76), ('reg:0.001', 25.71), ('reg:0.01', 22.12)]
2019-02-12 21:09:44,249 : Validation : best param found is reg = 1e-05 with score             30.11
2019-02-12 21:09:44,249 : Evaluating...
2019-02-12 21:10:03,968 : 
Dev acc : 30.1 Test acc : 30.2 for DEPTH classification

2019-02-12 21:10:03,976 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 21:10:04,333 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 21:10:04,393 : loading BERT mode bert-base-uncased
2019-02-12 21:10:04,393 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:10:04,497 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:10:04,497 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkh2e318o
2019-02-12 21:10:06,885 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:10:08,433 : Computing embeddings for train/dev/test
2019-02-12 21:13:16,312 : Computed embeddings
2019-02-12 21:13:16,312 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:14:35,915 : [('reg:1e-05', 47.49), ('reg:0.0001', 46.83), ('reg:0.001', 34.72), ('reg:0.01', 15.29)]
2019-02-12 21:14:35,915 : Validation : best param found is reg = 1e-05 with score             47.49
2019-02-12 21:14:35,915 : Evaluating...
2019-02-12 21:15:07,787 : 
Dev acc : 47.5 Test acc : 48.0 for TOPCONSTITUENTS classification

2019-02-12 21:15:07,795 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 21:15:08,305 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 21:15:08,369 : loading BERT mode bert-base-uncased
2019-02-12 21:15:08,369 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:15:08,398 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:15:08,399 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvah4giqo
2019-02-12 21:15:10,791 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:15:12,334 : Computing embeddings for train/dev/test
2019-02-12 21:17:59,411 : Computed embeddings
2019-02-12 21:17:59,411 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:19:21,306 : [('reg:1e-05', 51.76), ('reg:0.0001', 50.99), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-12 21:19:21,307 : Validation : best param found is reg = 1e-05 with score             51.76
2019-02-12 21:19:21,307 : Evaluating...
2019-02-12 21:19:58,051 : 
Dev acc : 51.8 Test acc : 51.9 for BIGRAMSHIFT classification

2019-02-12 21:19:58,059 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 21:19:58,609 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 21:19:58,674 : loading BERT mode bert-base-uncased
2019-02-12 21:19:58,674 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:19:58,704 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:19:58,704 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptl6gq944
2019-02-12 21:20:01,109 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:20:02,645 : Computing embeddings for train/dev/test
2019-02-12 21:22:31,836 : Computed embeddings
2019-02-12 21:22:31,836 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:24:23,458 : [('reg:1e-05', 87.51), ('reg:0.0001', 87.49), ('reg:0.001', 86.42), ('reg:0.01', 76.85)]
2019-02-12 21:24:23,458 : Validation : best param found is reg = 1e-05 with score             87.51
2019-02-12 21:24:23,458 : Evaluating...
2019-02-12 21:24:59,064 : 
Dev acc : 87.5 Test acc : 85.5 for TENSE classification

2019-02-12 21:24:59,072 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 21:24:59,484 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 21:24:59,546 : loading BERT mode bert-base-uncased
2019-02-12 21:24:59,546 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:24:59,572 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:24:59,572 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbvrso8w0
2019-02-12 21:25:01,995 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:25:03,452 : Computing embeddings for train/dev/test
2019-02-12 21:27:17,771 : Computed embeddings
2019-02-12 21:27:17,772 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:28:50,466 : [('reg:1e-05', 79.88), ('reg:0.0001', 80.1), ('reg:0.001', 74.87), ('reg:0.01', 63.58)]
2019-02-12 21:28:50,466 : Validation : best param found is reg = 0.0001 with score             80.1
2019-02-12 21:28:50,466 : Evaluating...
2019-02-12 21:29:12,552 : 
Dev acc : 80.1 Test acc : 79.0 for SUBJNUMBER classification

2019-02-12 21:29:12,560 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 21:29:12,967 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 21:29:13,033 : loading BERT mode bert-base-uncased
2019-02-12 21:29:13,034 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:29:13,149 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:29:13,149 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdc8ig1ta
2019-02-12 21:29:15,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:29:17,028 : Computing embeddings for train/dev/test
2019-02-12 21:31:20,721 : Computed embeddings
2019-02-12 21:31:20,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:32:44,298 : [('reg:1e-05', 77.7), ('reg:0.0001', 75.06), ('reg:0.001', 70.92), ('reg:0.01', 66.23)]
2019-02-12 21:32:44,298 : Validation : best param found is reg = 1e-05 with score             77.7
2019-02-12 21:32:44,298 : Evaluating...
2019-02-12 21:32:59,217 : 
Dev acc : 77.7 Test acc : 77.4 for OBJNUMBER classification

2019-02-12 21:32:59,226 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 21:32:59,601 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 21:32:59,669 : loading BERT mode bert-base-uncased
2019-02-12 21:32:59,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:32:59,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:32:59,791 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp41919q9w
2019-02-12 21:33:02,231 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:33:03,653 : Computing embeddings for train/dev/test
2019-02-12 21:35:29,976 : Computed embeddings
2019-02-12 21:35:29,976 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:36:26,307 : [('reg:1e-05', 54.72), ('reg:0.0001', 52.59), ('reg:0.001', 51.52), ('reg:0.01', 50.24)]
2019-02-12 21:36:26,307 : Validation : best param found is reg = 1e-05 with score             54.72
2019-02-12 21:36:26,307 : Evaluating...
2019-02-12 21:36:40,140 : 
Dev acc : 54.7 Test acc : 53.9 for ODDMANOUT classification

2019-02-12 21:36:40,150 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 21:36:40,735 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 21:36:40,810 : loading BERT mode bert-base-uncased
2019-02-12 21:36:40,810 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:36:40,841 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:36:40,841 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa6bla_ng
2019-02-12 21:36:43,270 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:36:44,693 : Computing embeddings for train/dev/test
2019-02-12 21:39:17,049 : Computed embeddings
2019-02-12 21:39:17,050 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:40:11,215 : [('reg:1e-05', 55.02), ('reg:0.0001', 54.39), ('reg:0.001', 51.45), ('reg:0.01', 50.02)]
2019-02-12 21:40:11,215 : Validation : best param found is reg = 1e-05 with score             55.02
2019-02-12 21:40:11,215 : Evaluating...
2019-02-12 21:40:31,625 : 
Dev acc : 55.0 Test acc : 53.8 for COORDINATIONINVERSION classification

2019-02-12 21:40:31,635 : {'STS12': {'MSRpar': {'pearson': (0.20945693944427735, 6.9967563097552725e-09), 'spearman': SpearmanrResult(correlation=0.25802253776381934, pvalue=7.161629692866363e-13), 'nsamples': 750}, 'MSRvid': {'pearson': (0.1817445500358232, 5.421383083994221e-07), 'spearman': SpearmanrResult(correlation=0.24568339719121218, pvalue=8.982990477854401e-12), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4043030526978324, 1.771832363667932e-19), 'spearman': SpearmanrResult(correlation=0.5703156417894253, pvalue=5.907155056396839e-41), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.10863243859512511, 0.002893044815774046), 'spearman': SpearmanrResult(correlation=0.1730915633548561, pvalue=1.8565908224712926e-06), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.47679325179491316, 4.8633395443373256e-24), 'spearman': SpearmanrResult(correlation=0.370660312407875, pvalue=1.9318755781911735e-14), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.27618604651359424, 'wmean': 0.24153508838831877}, 'spearman': {'mean': 0.3235546905014376, 'wmean': 0.2951307811983604}}}, 'STS13': {'FNWN': {'pearson': (0.23136781735976955, 0.0013588246413546916), 'spearman': SpearmanrResult(correlation=0.24168285537444215, pvalue=0.0008070811660923214), 'nsamples': 189}, 'headlines': {'pearson': (0.0665754262361011, 0.06842099503832993), 'spearman': SpearmanrResult(correlation=0.2286868536442046, pvalue=2.3495495962033673e-10), 'nsamples': 750}, 'OnWN': {'pearson': (0.18083580077715605, 1.638472052659954e-05), 'spearman': SpearmanrResult(correlation=0.19363283738021173, pvalue=3.840309874373167e-06), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.1595930147910089, 'wmean': 0.13007264759603787}, 'spearman': {'mean': 0.22133418213295283, 'wmean': 0.2172141477794812}}}, 'STS14': {'deft-forum': {'pearson': (0.1024290570255165, 0.029816314110708477), 'spearman': SpearmanrResult(correlation=0.16306281364278286, pvalue=0.0005151851768563842), 'nsamples': 450}, 'deft-news': {'pearson': (0.3203380401976993, 1.382747451331689e-08), 'spearman': SpearmanrResult(correlation=0.5082487788114827, pvalue=4.138806076821094e-21), 'nsamples': 300}, 'headlines': {'pearson': (0.08837392296666471, 0.015481244790591158), 'spearman': SpearmanrResult(correlation=0.20091407128055813, pvalue=2.860498450911207e-08), 'nsamples': 750}, 'images': {'pearson': (0.1569568102555124, 1.574061611183727e-05), 'spearman': SpearmanrResult(correlation=0.28583209074109345, pvalue=1.4410823016756517e-15), 'nsamples': 750}, 'OnWN': {'pearson': (0.16984829366685344, 2.9002955030927377e-06), 'spearman': SpearmanrResult(correlation=0.21427743310163158, pvalue=3.077267320677454e-09), 'nsamples': 750}, 'tweet-news': {'pearson': (0.1508698303227869, 3.3448891727989087e-05), 'spearman': SpearmanrResult(correlation=0.24089438948105124, pvalue=2.3116618189585133e-11), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.16480265907250555, 'wmean': 0.15112830150124143}, 'spearman': {'mean': 0.26887159617643336, 'wmean': 0.2486110368629194}}}, 'STS15': {'answers-forums': {'pearson': (0.16664849892153513, 0.0011993314806627536), 'spearman': SpearmanrResult(correlation=0.21994388540840118, pvalue=1.725666959297952e-05), 'nsamples': 375}, 'answers-students': {'pearson': (0.26445322500893437, 1.815491749647492e-13), 'spearman': SpearmanrResult(correlation=0.31999221058815996, pvalue=2.5624437485852926e-19), 'nsamples': 750}, 'belief': {'pearson': (0.1637190157556346, 0.0014660931610345801), 'spearman': SpearmanrResult(correlation=0.32823832330643044, pvalue=7.196240437881015e-11), 'nsamples': 375}, 'headlines': {'pearson': (0.1780433694389158, 9.245953436045306e-07), 'spearman': SpearmanrResult(correlation=0.3409005417892076, pvalue=7.275982847614968e-22), 'nsamples': 750}, 'images': {'pearson': (0.005683500399955518, 0.8765133079693878), 'spearman': SpearmanrResult(correlation=0.33110162542256905, pvalue=1.202228490061825e-20), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.15570952190499507, 'wmean': 0.15334096304659764}, 'spearman': {'mean': 0.30803531730295364, 'wmean': 0.3165213705393381}}}, 'STS16': {'answer-answer': {'pearson': (0.18525103206964225, 0.003041161319017776), 'spearman': SpearmanrResult(correlation=0.3210643703516975, pvalue=1.6875477355189946e-07), 'nsamples': 254}, 'headlines': {'pearson': (0.1928456086696344, 0.002239509097837317), 'spearman': SpearmanrResult(correlation=0.41973150383902785, pvalue=4.786540731906739e-12), 'nsamples': 249}, 'plagiarism': {'pearson': (0.24368686170748624, 0.0001899845723577504), 'spearman': SpearmanrResult(correlation=0.4165842485982131, pvalue=4.543234119013539e-11), 'nsamples': 230}, 'postediting': {'pearson': (0.3142979312473921, 5.390535305317704e-07), 'spearman': SpearmanrResult(correlation=0.730258746391663, pvalue=6.318764990858222e-42), 'nsamples': 244}, 'question-question': {'pearson': (0.058583904074218776, 0.3994621369620561), 'spearman': SpearmanrResult(correlation=0.13743518679580682, pvalue=0.047212625302725374), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.19893306755367474, 'wmean': 0.20240558859445643}, 'spearman': {'mean': 0.4050148111952817, 'wmean': 0.412128971216465}}}, 'MR': {'devacc': 63.65, 'acc': 66.26, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 68.4, 'acc': 65.75, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 79.95, 'acc': 82.32, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 89.84, 'acc': 87.94, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 75.69, 'acc': 73.48, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.6, 'acc': 36.02, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 52.0, 'acc': 75.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.5, 'acc': 71.83, 'f1': 80.94, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 69.6, 'acc': 67.77, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7572475777026654, 'pearson': 0.7527080905415069, 'spearman': 0.6883958069583715, 'mse': 0.4448644346280351, 'yhat': array([2.056121  , 4.04908499, 1.90648232, ..., 3.55824354, 3.98396706,
       4.38456622]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6342857221656193, 'pearson': 0.5413199282897899, 'spearman': 0.5347957204141336, 'mse': 1.9346301160062207, 'yhat': array([2.86092747, 2.2328785 , 2.69322221, ..., 3.73555332, 3.66448071,
       3.33935188]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 87.25, 'acc': 87.15, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 1.99, 'acc': 1.91, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.11, 'acc': 30.22, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 47.49, 'acc': 48.05, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 51.76, 'acc': 51.89, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.51, 'acc': 85.46, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.1, 'acc': 79.05, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.7, 'acc': 77.42, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.72, 'acc': 53.89, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.02, 'acc': 53.75, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 21:40:31,635 : ********************************************************************************
2019-02-12 21:40:31,635 : ********************************************************************************
2019-02-12 21:40:31,635 : ********************************************************************************
2019-02-12 21:40:31,635 : layer 4
2019-02-12 21:40:31,635 : ********************************************************************************
2019-02-12 21:40:31,635 : ********************************************************************************
2019-02-12 21:40:31,636 : ********************************************************************************
2019-02-12 21:40:31,718 : ***** Transfer task : STS12 *****


2019-02-12 21:40:31,731 : loading BERT mode bert-base-uncased
2019-02-12 21:40:31,731 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:40:31,747 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:40:31,748 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj9gau32_
2019-02-12 21:40:34,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:40:38,999 : MSRpar : pearson = 0.3329, spearman = 0.3789
2019-02-12 21:40:41,170 : MSRvid : pearson = 0.4663, spearman = 0.5942
2019-02-12 21:40:42,758 : SMTeuroparl : pearson = 0.4427, spearman = 0.5680
2019-02-12 21:40:45,265 : surprise.OnWN : pearson = 0.1711, spearman = 0.2073
2019-02-12 21:40:46,680 : surprise.SMTnews : pearson = 0.6665, spearman = 0.5084
2019-02-12 21:40:46,680 : ALL (weighted average) : Pearson = 0.3851,             Spearman = 0.4340
2019-02-12 21:40:46,680 : ALL (average) : Pearson = 0.4159,             Spearman = 0.4514

2019-02-12 21:40:46,680 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 21:40:46,688 : loading BERT mode bert-base-uncased
2019-02-12 21:40:46,688 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:40:46,705 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:40:46,706 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnf3udlv6
2019-02-12 21:40:49,132 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:40:51,327 : FNWN : pearson = 0.3230, spearman = 0.3143
2019-02-12 21:40:53,226 : headlines : pearson = 0.2994, spearman = 0.4266
2019-02-12 21:40:54,939 : OnWN : pearson = 0.2803, spearman = 0.3220
2019-02-12 21:40:54,939 : ALL (weighted average) : Pearson = 0.2953,             Spearman = 0.3733
2019-02-12 21:40:54,939 : ALL (average) : Pearson = 0.3009,             Spearman = 0.3543

2019-02-12 21:40:54,939 : ***** Transfer task : STS14 *****


2019-02-12 21:40:54,957 : loading BERT mode bert-base-uncased
2019-02-12 21:40:54,957 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:40:54,974 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:40:54,975 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbldvn4ke
2019-02-12 21:40:57,368 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:41:00,434 : deft-forum : pearson = 0.2759, spearman = 0.3438
2019-02-12 21:41:01,715 : deft-news : pearson = 0.5811, spearman = 0.6768
2019-02-12 21:41:03,411 : headlines : pearson = 0.2888, spearman = 0.3552
2019-02-12 21:41:05,170 : images : pearson = 0.5990, spearman = 0.5940
2019-02-12 21:41:07,041 : OnWN : pearson = 0.2934, spearman = 0.3269
2019-02-12 21:41:09,278 : tweet-news : pearson = 0.2815, spearman = 0.3298
2019-02-12 21:41:09,278 : ALL (weighted average) : Pearson = 0.3721,             Spearman = 0.4165
2019-02-12 21:41:09,278 : ALL (average) : Pearson = 0.3866,             Spearman = 0.4377

2019-02-12 21:41:09,278 : ***** Transfer task : STS15 *****


2019-02-12 21:41:09,313 : loading BERT mode bert-base-uncased
2019-02-12 21:41:09,314 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:41:09,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:41:09,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp95nl7e8x
2019-02-12 21:41:11,755 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:41:14,233 : answers-forums : pearson = 0.3999, spearman = 0.4372
2019-02-12 21:41:15,598 : answers-students : pearson = 0.3854, spearman = 0.4209
2019-02-12 21:41:16,682 : belief : pearson = 0.3637, spearman = 0.4716
2019-02-12 21:41:18,113 : headlines : pearson = 0.3944, spearman = 0.4949
2019-02-12 21:41:19,492 : images : pearson = 0.1379, spearman = 0.5091
2019-02-12 21:41:19,493 : ALL (weighted average) : Pearson = 0.3249,             Spearman = 0.4698
2019-02-12 21:41:19,493 : ALL (average) : Pearson = 0.3363,             Spearman = 0.4667

2019-02-12 21:41:19,493 : ***** Transfer task : STS16 *****


2019-02-12 21:41:19,562 : loading BERT mode bert-base-uncased
2019-02-12 21:41:19,562 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:41:19,580 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:41:19,580 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmycgblxu
2019-02-12 21:41:22,022 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:41:23,899 : answer-answer : pearson = 0.3228, spearman = 0.4578
2019-02-12 21:41:24,277 : headlines : pearson = 0.3946, spearman = 0.5296
2019-02-12 21:41:24,731 : plagiarism : pearson = 0.4793, spearman = 0.6467
2019-02-12 21:41:25,430 : postediting : pearson = 0.4859, spearman = 0.7718
2019-02-12 21:41:26,102 : question-question : pearson = 0.5365, spearman = 0.5480
2019-02-12 21:41:26,102 : ALL (weighted average) : Pearson = 0.4394,             Spearman = 0.5900
2019-02-12 21:41:26,102 : ALL (average) : Pearson = 0.4438,             Spearman = 0.5908

2019-02-12 21:41:26,102 : ***** Transfer task : MR *****


2019-02-12 21:41:26,123 : loading BERT mode bert-base-uncased
2019-02-12 21:41:26,123 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:41:26,143 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:41:26,143 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm0b67zeo
2019-02-12 21:41:28,567 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:41:30,102 : Generating sentence embeddings
2019-02-12 21:41:52,479 : Generated sentence embeddings
2019-02-12 21:41:52,480 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:42:09,869 : Best param found at split 1: l2reg = 0.01                 with score 64.0
2019-02-12 21:42:38,036 : Best param found at split 2: l2reg = 0.0001                 with score 67.85
2019-02-12 21:42:56,565 : Best param found at split 3: l2reg = 1e-05                 with score 66.94
2019-02-12 21:43:23,571 : Best param found at split 4: l2reg = 1e-05                 with score 67.44
2019-02-12 21:43:59,851 : Best param found at split 5: l2reg = 1e-05                 with score 66.37
2019-02-12 21:44:01,342 : Dev acc : 66.52 Test acc : 65.33

2019-02-12 21:44:01,343 : ***** Transfer task : CR *****


2019-02-12 21:44:01,351 : loading BERT mode bert-base-uncased
2019-02-12 21:44:01,351 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:44:01,371 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:44:01,371 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfz5jjl7z
2019-02-12 21:44:03,807 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:44:05,301 : Generating sentence embeddings
2019-02-12 21:44:11,935 : Generated sentence embeddings
2019-02-12 21:44:11,935 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:44:21,288 : Best param found at split 1: l2reg = 0.001                 with score 68.57
2019-02-12 21:44:30,833 : Best param found at split 2: l2reg = 0.001                 with score 68.73
2019-02-12 21:44:39,962 : Best param found at split 3: l2reg = 0.001                 with score 70.89
2019-02-12 21:44:48,806 : Best param found at split 4: l2reg = 0.001                 with score 70.8
2019-02-12 21:44:57,502 : Best param found at split 5: l2reg = 1e-05                 with score 70.54
2019-02-12 21:44:57,838 : Dev acc : 69.91 Test acc : 64.03

2019-02-12 21:44:57,838 : ***** Transfer task : MPQA *****


2019-02-12 21:44:57,843 : loading BERT mode bert-base-uncased
2019-02-12 21:44:57,843 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:44:57,862 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:44:57,862 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6cnmgw71
2019-02-12 21:45:00,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:45:01,801 : Generating sentence embeddings
2019-02-12 21:45:11,400 : Generated sentence embeddings
2019-02-12 21:45:11,401 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:45:33,987 : Best param found at split 1: l2reg = 0.0001                 with score 81.42
2019-02-12 21:46:04,774 : Best param found at split 2: l2reg = 1e-05                 with score 83.75
2019-02-12 21:46:33,183 : Best param found at split 3: l2reg = 0.0001                 with score 83.88
2019-02-12 21:47:14,325 : Best param found at split 4: l2reg = 0.001                 with score 82.84
2019-02-12 21:47:43,802 : Best param found at split 5: l2reg = 0.0001                 with score 83.05
2019-02-12 21:47:45,541 : Dev acc : 82.99 Test acc : 81.84

2019-02-12 21:47:45,542 : ***** Transfer task : SUBJ *****


2019-02-12 21:47:45,555 : loading BERT mode bert-base-uncased
2019-02-12 21:47:45,555 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:47:45,577 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:47:45,577 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpafabnf3l
2019-02-12 21:47:48,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:47:49,486 : Generating sentence embeddings
2019-02-12 21:48:09,671 : Generated sentence embeddings
2019-02-12 21:48:09,671 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:48:35,351 : Best param found at split 1: l2reg = 0.0001                 with score 90.86
2019-02-12 21:49:01,339 : Best param found at split 2: l2reg = 1e-05                 with score 91.35
2019-02-12 21:49:25,093 : Best param found at split 3: l2reg = 0.001                 with score 90.94
2019-02-12 21:49:54,274 : Best param found at split 4: l2reg = 0.0001                 with score 91.7
2019-02-12 21:50:23,286 : Best param found at split 5: l2reg = 1e-05                 with score 91.14
2019-02-12 21:50:24,795 : Dev acc : 91.2 Test acc : 91.02

2019-02-12 21:50:24,796 : ***** Transfer task : SST Binary classification *****


2019-02-12 21:50:24,920 : loading BERT mode bert-base-uncased
2019-02-12 21:50:24,920 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:50:24,942 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:50:24,942 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjeyj43yf
2019-02-12 21:50:27,361 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:50:28,839 : Computing embedding for train
2019-02-12 21:51:55,488 : Computed train embeddings
2019-02-12 21:51:55,489 : Computing embedding for dev
2019-02-12 21:51:57,148 : Computed dev embeddings
2019-02-12 21:51:57,148 : Computing embedding for test
2019-02-12 21:52:00,965 : Computed test embeddings
2019-02-12 21:52:00,965 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:52:35,976 : [('reg:1e-05', 77.52), ('reg:0.0001', 77.64), ('reg:0.001', 77.64), ('reg:0.01', 73.62)]
2019-02-12 21:52:35,977 : Validation : best param found is reg = 0.0001 with score             77.64
2019-02-12 21:52:35,977 : Evaluating...
2019-02-12 21:52:45,614 : 
Dev acc : 77.64 Test acc : 76.83 for             SST Binary classification

2019-02-12 21:52:45,621 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 21:52:45,670 : loading BERT mode bert-base-uncased
2019-02-12 21:52:45,671 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:52:45,691 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:52:45,692 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9o5h5u3c
2019-02-12 21:52:48,163 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:52:49,591 : Computing embedding for train
2019-02-12 21:53:02,699 : Computed train embeddings
2019-02-12 21:53:02,699 : Computing embedding for dev
2019-02-12 21:53:04,451 : Computed dev embeddings
2019-02-12 21:53:04,451 : Computing embedding for test
2019-02-12 21:53:07,947 : Computed test embeddings
2019-02-12 21:53:07,947 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:53:13,471 : [('reg:1e-05', 36.15), ('reg:0.0001', 37.6), ('reg:0.001', 36.06), ('reg:0.01', 27.88)]
2019-02-12 21:53:13,471 : Validation : best param found is reg = 0.0001 with score             37.6
2019-02-12 21:53:13,471 : Evaluating...
2019-02-12 21:53:14,879 : 
Dev acc : 37.6 Test acc : 37.38 for             SST Fine-Grained classification

2019-02-12 21:53:14,880 : ***** Transfer task : TREC *****


2019-02-12 21:53:14,893 : loading BERT mode bert-base-uncased
2019-02-12 21:53:14,893 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:53:14,911 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:53:14,911 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9v3s7smj
2019-02-12 21:53:17,333 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:53:25,403 : Computed train embeddings
2019-02-12 21:53:25,974 : Computed test embeddings
2019-02-12 21:53:25,975 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 21:53:42,490 : [('reg:1e-05', 55.36), ('reg:0.0001', 54.75), ('reg:0.001', 49.97), ('reg:0.01', 44.28)]
2019-02-12 21:53:42,490 : Cross-validation : best param found is reg = 1e-05             with score 55.36
2019-02-12 21:53:42,490 : Evaluating...
2019-02-12 21:53:43,594 : 
Dev acc : 55.36 Test acc : 67.8             for TREC

2019-02-12 21:53:43,594 : ***** Transfer task : MRPC *****


2019-02-12 21:53:43,616 : loading BERT mode bert-base-uncased
2019-02-12 21:53:43,616 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:53:43,636 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:53:43,636 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprowmnc_9
2019-02-12 21:53:46,060 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:53:47,547 : Computing embedding for train
2019-02-12 21:54:01,713 : Computed train embeddings
2019-02-12 21:54:01,713 : Computing embedding for test
2019-02-12 21:54:07,933 : Computed test embeddings
2019-02-12 21:54:07,949 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 21:54:21,242 : [('reg:1e-05', 69.36), ('reg:0.0001', 69.55), ('reg:0.001', 69.06), ('reg:0.01', 68.52)]
2019-02-12 21:54:21,242 : Cross-validation : best param found is reg = 0.0001             with score 69.55
2019-02-12 21:54:21,242 : Evaluating...
2019-02-12 21:54:21,982 : Dev acc : 69.55 Test acc 67.54; Test F1 80.35 for MRPC.

2019-02-12 21:54:21,983 : ***** Transfer task : SICK-Entailment*****


2019-02-12 21:54:22,045 : loading BERT mode bert-base-uncased
2019-02-12 21:54:22,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:54:22,064 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:54:22,064 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx8hec25f
2019-02-12 21:54:24,497 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:54:25,962 : Computing embedding for train
2019-02-12 21:54:37,673 : Computed train embeddings
2019-02-12 21:54:37,674 : Computing embedding for dev
2019-02-12 21:54:39,582 : Computed dev embeddings
2019-02-12 21:54:39,582 : Computing embedding for test
2019-02-12 21:54:53,533 : Computed test embeddings
2019-02-12 21:54:53,560 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:54:56,642 : [('reg:1e-05', 72.4), ('reg:0.0001', 74.6), ('reg:0.001', 66.4), ('reg:0.01', 69.4)]
2019-02-12 21:54:56,643 : Validation : best param found is reg = 0.0001 with score             74.6
2019-02-12 21:54:56,643 : Evaluating...
2019-02-12 21:54:57,497 : 
Dev acc : 74.6 Test acc : 74.08 for                        SICK entailment

2019-02-12 21:54:57,497 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 21:54:57,524 : loading BERT mode bert-base-uncased
2019-02-12 21:54:57,524 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:54:57,579 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:54:57,579 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6qp95pyi
2019-02-12 21:55:00,001 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:55:01,457 : Computing embedding for train
2019-02-12 21:55:11,519 : Computed train embeddings
2019-02-12 21:55:11,519 : Computing embedding for dev
2019-02-12 21:55:12,480 : Computed dev embeddings
2019-02-12 21:55:12,480 : Computing embedding for test
2019-02-12 21:55:20,725 : Computed test embeddings
2019-02-12 21:56:03,275 : Dev : Pearson 0.7835423335857185
2019-02-12 21:56:03,276 : Test : Pearson 0.7722223091331447 Spearman 0.6980729292493973 MSE 0.4108660467326688                        for SICK Relatedness

2019-02-12 21:56:03,276 : 

***** Transfer task : STSBenchmark*****


2019-02-12 21:56:03,318 : loading BERT mode bert-base-uncased
2019-02-12 21:56:03,318 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:56:03,346 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:56:03,347 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpluzb5vho
2019-02-12 21:56:05,796 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:56:07,278 : Computing embedding for train
2019-02-12 21:56:20,179 : Computed train embeddings
2019-02-12 21:56:20,179 : Computing embedding for dev
2019-02-12 21:56:23,980 : Computed dev embeddings
2019-02-12 21:56:23,980 : Computing embedding for test
2019-02-12 21:56:27,136 : Computed test embeddings
2019-02-12 21:57:26,928 : Dev : Pearson 0.6845589020484055
2019-02-12 21:57:26,928 : Test : Pearson 0.6424955470744644 Spearman 0.6384598771531589 MSE 1.6651889503720172                        for SICK Relatedness

2019-02-12 21:57:26,928 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 21:57:27,248 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 21:57:27,258 : loading BERT mode bert-base-uncased
2019-02-12 21:57:27,258 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:57:27,282 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:57:27,282 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpige3muc8
2019-02-12 21:57:29,708 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:57:31,185 : Computing embeddings for train/dev/test
2019-02-12 22:00:00,726 : Computed embeddings
2019-02-12 22:00:00,726 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:01:27,302 : [('reg:1e-05', 85.61), ('reg:0.0001', 75.81), ('reg:0.001', 72.43), ('reg:0.01', 59.9)]
2019-02-12 22:01:27,303 : Validation : best param found is reg = 1e-05 with score             85.61
2019-02-12 22:01:27,303 : Evaluating...
2019-02-12 22:01:55,536 : 
Dev acc : 85.6 Test acc : 85.7 for LENGTH classification

2019-02-12 22:01:55,544 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 22:01:55,883 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 22:01:55,929 : loading BERT mode bert-base-uncased
2019-02-12 22:01:55,930 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:01:55,957 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:01:55,958 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp65si7e4
2019-02-12 22:01:58,387 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:01:59,868 : Computing embeddings for train/dev/test
2019-02-12 22:04:17,882 : Computed embeddings
2019-02-12 22:04:17,882 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:06:12,337 : [('reg:1e-05', 11.15), ('reg:0.0001', 1.99), ('reg:0.001', 0.39), ('reg:0.01', 0.16)]
2019-02-12 22:06:12,337 : Validation : best param found is reg = 1e-05 with score             11.15
2019-02-12 22:06:12,337 : Evaluating...
2019-02-12 22:06:39,273 : 
Dev acc : 11.2 Test acc : 11.4 for WORDCONTENT classification

2019-02-12 22:06:39,283 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 22:06:39,629 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 22:06:39,693 : loading BERT mode bert-base-uncased
2019-02-12 22:06:39,693 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:06:39,718 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:06:39,718 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnr6m8lj3
2019-02-12 22:06:42,151 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:06:43,631 : Computing embeddings for train/dev/test
2019-02-12 22:08:55,193 : Computed embeddings
2019-02-12 22:08:55,193 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:09:57,014 : [('reg:1e-05', 29.02), ('reg:0.0001', 26.53), ('reg:0.001', 25.27), ('reg:0.01', 22.46)]
2019-02-12 22:09:57,014 : Validation : best param found is reg = 1e-05 with score             29.02
2019-02-12 22:09:57,014 : Evaluating...
2019-02-12 22:10:15,966 : 
Dev acc : 29.0 Test acc : 29.4 for DEPTH classification

2019-02-12 22:10:15,975 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 22:10:16,347 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 22:10:16,409 : loading BERT mode bert-base-uncased
2019-02-12 22:10:16,409 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:10:16,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:10:16,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp05t06s33
2019-02-12 22:10:18,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:10:20,432 : Computing embeddings for train/dev/test
2019-02-12 22:12:59,374 : Computed embeddings
2019-02-12 22:12:59,374 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:14:32,877 : [('reg:1e-05', 49.74), ('reg:0.0001', 48.02), ('reg:0.001', 33.77), ('reg:0.01', 18.0)]
2019-02-12 22:14:32,877 : Validation : best param found is reg = 1e-05 with score             49.74
2019-02-12 22:14:32,877 : Evaluating...
2019-02-12 22:14:56,649 : 
Dev acc : 49.7 Test acc : 49.7 for TOPCONSTITUENTS classification

2019-02-12 22:14:56,656 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 22:14:56,992 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 22:14:57,057 : loading BERT mode bert-base-uncased
2019-02-12 22:14:57,057 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:14:57,176 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:14:57,177 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcfzilapa
2019-02-12 22:14:59,606 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:15:01,076 : Computing embeddings for train/dev/test
2019-02-12 22:17:56,364 : Computed embeddings
2019-02-12 22:17:56,364 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:18:59,117 : [('reg:1e-05', 63.24), ('reg:0.0001', 62.76), ('reg:0.001', 61.6), ('reg:0.01', 51.2)]
2019-02-12 22:18:59,117 : Validation : best param found is reg = 1e-05 with score             63.24
2019-02-12 22:18:59,117 : Evaluating...
2019-02-12 22:19:09,413 : 
Dev acc : 63.2 Test acc : 63.9 for BIGRAMSHIFT classification

2019-02-12 22:19:09,421 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 22:19:09,977 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 22:19:10,042 : loading BERT mode bert-base-uncased
2019-02-12 22:19:10,043 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:19:10,073 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:19:10,073 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptixxygwn
2019-02-12 22:19:12,509 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:19:13,997 : Computing embeddings for train/dev/test
2019-02-12 22:22:09,680 : Computed embeddings
2019-02-12 22:22:09,681 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:23:22,853 : [('reg:1e-05', 88.02), ('reg:0.0001', 88.01), ('reg:0.001', 87.65), ('reg:0.01', 84.85)]
2019-02-12 22:23:22,853 : Validation : best param found is reg = 1e-05 with score             88.02
2019-02-12 22:23:22,854 : Evaluating...
2019-02-12 22:23:32,680 : 
Dev acc : 88.0 Test acc : 86.5 for TENSE classification

2019-02-12 22:23:32,688 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 22:23:33,286 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 22:23:33,351 : loading BERT mode bert-base-uncased
2019-02-12 22:23:33,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:23:33,379 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:23:33,380 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzo1a4uuk
2019-02-12 22:23:35,813 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:23:37,267 : Computing embeddings for train/dev/test
2019-02-12 22:26:26,084 : Computed embeddings
2019-02-12 22:26:26,084 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:27:02,031 : [('reg:1e-05', 76.65), ('reg:0.0001', 76.74), ('reg:0.001', 76.35), ('reg:0.01', 65.59)]
2019-02-12 22:27:02,031 : Validation : best param found is reg = 0.0001 with score             76.74
2019-02-12 22:27:02,032 : Evaluating...
2019-02-12 22:27:09,490 : 
Dev acc : 76.7 Test acc : 76.0 for SUBJNUMBER classification

2019-02-12 22:27:09,498 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 22:27:09,946 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 22:27:10,019 : loading BERT mode bert-base-uncased
2019-02-12 22:27:10,019 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:27:10,049 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:27:10,049 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz_7hhffi
2019-02-12 22:27:12,486 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:27:13,942 : Computing embeddings for train/dev/test
2019-02-12 22:29:53,668 : Computed embeddings
2019-02-12 22:29:53,668 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:30:45,039 : [('reg:1e-05', 76.79), ('reg:0.0001', 76.55), ('reg:0.001', 75.48), ('reg:0.01', 70.65)]
2019-02-12 22:30:45,039 : Validation : best param found is reg = 1e-05 with score             76.79
2019-02-12 22:30:45,039 : Evaluating...
2019-02-12 22:30:57,562 : 
Dev acc : 76.8 Test acc : 78.2 for OBJNUMBER classification

2019-02-12 22:30:57,570 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 22:30:57,961 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 22:30:58,031 : loading BERT mode bert-base-uncased
2019-02-12 22:30:58,031 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:30:58,156 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:30:58,156 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc7zredqe
2019-02-12 22:31:00,603 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:31:02,039 : Computing embeddings for train/dev/test
2019-02-12 22:33:52,075 : Computed embeddings
2019-02-12 22:33:52,075 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:34:33,238 : [('reg:1e-05', 56.21), ('reg:0.0001', 56.23), ('reg:0.001', 55.82), ('reg:0.01', 53.38)]
2019-02-12 22:34:33,238 : Validation : best param found is reg = 0.0001 with score             56.23
2019-02-12 22:34:33,238 : Evaluating...
2019-02-12 22:34:45,703 : 
Dev acc : 56.2 Test acc : 55.1 for ODDMANOUT classification

2019-02-12 22:34:45,714 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 22:34:46,335 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 22:34:46,412 : loading BERT mode bert-base-uncased
2019-02-12 22:34:46,412 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:34:46,443 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:34:46,444 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5p3etlsj
2019-02-12 22:34:48,923 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:34:50,434 : Computing embeddings for train/dev/test
2019-02-12 22:38:00,010 : Computed embeddings
2019-02-12 22:38:00,011 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:38:54,949 : [('reg:1e-05', 50.51), ('reg:0.0001', 50.41), ('reg:0.001', 50.11), ('reg:0.01', 50.0)]
2019-02-12 22:38:54,949 : Validation : best param found is reg = 1e-05 with score             50.51
2019-02-12 22:38:54,949 : Evaluating...
2019-02-12 22:39:03,763 : 
Dev acc : 50.5 Test acc : 50.4 for COORDINATIONINVERSION classification

2019-02-12 22:39:03,774 : {'STS12': {'MSRpar': {'pearson': (0.33292502347645775, 7.188344123227065e-21), 'spearman': SpearmanrResult(correlation=0.37886442982537843, pvalue=5.1969848823478316e-27), 'nsamples': 750}, 'MSRvid': {'pearson': (0.46629641787386994, 9.36072814445691e-42), 'spearman': SpearmanrResult(correlation=0.5941606759247019, pvalue=9.148099646369967e-73), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4426609980696041, 1.9019753251301734e-23), 'spearman': SpearmanrResult(correlation=0.5680340120737659, pvalue=1.426775373375149e-40), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.17112287023827047, 2.436352374364928e-06), 'spearman': SpearmanrResult(correlation=0.20727000211778826, pvalue=1.009149738385023e-08), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6665441706814754, 1.351565849589579e-52), 'spearman': SpearmanrResult(correlation=0.5084412056152384, pvalue=1.284039112912398e-27), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4159098960679355, 'wmean': 0.38509998581316135}, 'spearman': {'mean': 0.4513540651113746, 'wmean': 0.43398229841803093}}}, 'STS13': {'FNWN': {'pearson': (0.3230289067687263, 5.798967318734957e-06), 'spearman': SpearmanrResult(correlation=0.3143457619828799, pvalue=1.0575759439484824e-05), 'nsamples': 189}, 'headlines': {'pearson': (0.2994400380572252, 5.299365499203079e-17), 'spearman': SpearmanrResult(correlation=0.42655429912751003, pvalue=1.6302399291856864e-34), 'nsamples': 750}, 'OnWN': {'pearson': (0.28029559643629076, 1.3790423321402124e-11), 'spearman': SpearmanrResult(correlation=0.32200501172566365, pvalue=5.3232705230227676e-15), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.30092151375408077, 'wmean': 0.29525221434864485}, 'spearman': {'mean': 0.3543016909453512, 'wmean': 0.3733145899589961}}}, 'STS14': {'deft-forum': {'pearson': (0.27592225785898816, 2.6349144891394305e-09), 'spearman': SpearmanrResult(correlation=0.343791101719268, pvalue=6.260185504106288e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.5810863824189059, 1.731263948305825e-28), 'spearman': SpearmanrResult(correlation=0.6767708143016062, pvalue=1.5703130579198019e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.2887631631434192, 7.181668267809188e-16), 'spearman': SpearmanrResult(correlation=0.3551768369710113, pvalue=1.0180402429064104e-23), 'nsamples': 750}, 'images': {'pearson': (0.598977393291072, 3.225416986538311e-74), 'spearman': SpearmanrResult(correlation=0.5939580428705691, pvalue=1.0517410227212211e-72), 'nsamples': 750}, 'OnWN': {'pearson': (0.293438161332521, 2.3250466987259993e-16), 'spearman': SpearmanrResult(correlation=0.32685303923385467, pvalue=3.9319924902383705e-20), 'nsamples': 750}, 'tweet-news': {'pearson': (0.2815084186947909, 3.966466450783058e-15), 'spearman': SpearmanrResult(correlation=0.3297626523716799, pvalue=1.750048522060265e-20), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.38661596278994953, 'wmean': 0.3721350088289517}, 'spearman': {'mean': 0.43771874791133153, 'wmean': 0.4165467116398637}}}, 'STS15': {'answers-forums': {'pearson': (0.39993017222831573, 7.787513770813092e-16), 'spearman': SpearmanrResult(correlation=0.43722069904865174, pvalue=6.116577442181571e-19), 'nsamples': 375}, 'answers-students': {'pearson': (0.3854143443074299, 5.705363112896917e-28), 'spearman': SpearmanrResult(correlation=0.4208734725834483, pvalue=1.482648903059762e-33), 'nsamples': 750}, 'belief': {'pearson': (0.3637259341034387, 3.5808061869116454e-13), 'spearman': SpearmanrResult(correlation=0.4716294188116715, pvalue=3.6366561252445195e-22), 'nsamples': 375}, 'headlines': {'pearson': (0.3944149920363467, 2.523012999055229e-29), 'spearman': SpearmanrResult(correlation=0.4948542536949515, pvalue=1.4009270910846216e-47), 'nsamples': 750}, 'images': {'pearson': (0.13785510354352376, 0.00015240604801924936), 'spearman': SpearmanrResult(correlation=0.5090947908414748, pvalue=1.0702907711810444e-50), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.33626810924381095, 'wmean': 0.3248781232632944}, 'spearman': {'mean': 0.46673452699603957, 'wmean': 0.46981189401250906}}}, 'STS16': {'answer-answer': {'pearson': (0.3227857799723158, 1.4368946136760226e-07), 'spearman': SpearmanrResult(correlation=0.4577944141353731, pvalue=1.4598402819800768e-14), 'nsamples': 254}, 'headlines': {'pearson': (0.3946195903560399, 1.0489855906865772e-10), 'spearman': SpearmanrResult(correlation=0.5295754201085991, pvalue=2.1080428555495031e-19), 'nsamples': 249}, 'plagiarism': {'pearson': (0.4792632681535299, 1.304559449358429e-14), 'spearman': SpearmanrResult(correlation=0.6466916698126703, pvalue=1.237523526088338e-28), 'nsamples': 230}, 'postediting': {'pearson': (0.4859340249532845, 7.288720182146055e-16), 'spearman': SpearmanrResult(correlation=0.7717754747804415, pvalue=1.738534894969914e-49), 'nsamples': 244}, 'question-question': {'pearson': (0.5364525615813561, 5.6804109315632e-17), 'spearman': SpearmanrResult(correlation=0.5480250331575404, pvalue=8.836394484882632e-18), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.44381104500330526, 'wmean': 0.43943078013999903}, 'spearman': {'mean': 0.5907724023989249, 'wmean': 0.5899945974963692}}}, 'MR': {'devacc': 66.52, 'acc': 65.33, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 69.91, 'acc': 64.03, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 82.99, 'acc': 81.84, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.2, 'acc': 91.02, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.64, 'acc': 76.83, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.6, 'acc': 37.38, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 55.36, 'acc': 67.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.55, 'acc': 67.54, 'f1': 80.35, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 74.6, 'acc': 74.08, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7835423335857185, 'pearson': 0.7722223091331447, 'spearman': 0.6980729292493973, 'mse': 0.4108660467326688, 'yhat': array([3.05129275, 4.29797008, 2.18803492, ..., 2.9057259 , 4.33315012,
       4.27719228]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6845589020484055, 'pearson': 0.6424955470744644, 'spearman': 0.6384598771531589, 'mse': 1.6651889503720172, 'yhat': array([2.41764016, 1.33976334, 2.81588475, ..., 3.87299817, 3.7774294 ,
       3.46973953]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 85.61, 'acc': 85.69, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 11.15, 'acc': 11.44, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.02, 'acc': 29.42, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 49.74, 'acc': 49.73, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 63.24, 'acc': 63.9, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.02, 'acc': 86.49, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 76.74, 'acc': 76.04, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.79, 'acc': 78.23, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 56.23, 'acc': 55.07, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.51, 'acc': 50.41, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 22:39:03,774 : ********************************************************************************
2019-02-12 22:39:03,774 : ********************************************************************************
2019-02-12 22:39:03,774 : ********************************************************************************
2019-02-12 22:39:03,774 : layer 5
2019-02-12 22:39:03,774 : ********************************************************************************
2019-02-12 22:39:03,775 : ********************************************************************************
2019-02-12 22:39:03,775 : ********************************************************************************
2019-02-12 22:39:03,863 : ***** Transfer task : STS12 *****


2019-02-12 22:39:03,876 : loading BERT mode bert-base-uncased
2019-02-12 22:39:03,876 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:39:03,893 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:39:03,893 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4zrfnb5_
2019-02-12 22:39:06,321 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:39:10,465 : MSRpar : pearson = 0.2971, spearman = 0.3514
2019-02-12 22:39:12,046 : MSRvid : pearson = 0.3494, spearman = 0.4170
2019-02-12 22:39:13,118 : SMTeuroparl : pearson = 0.4627, spearman = 0.5757
2019-02-12 22:39:15,105 : surprise.OnWN : pearson = 0.2639, spearman = 0.2607
2019-02-12 22:39:16,250 : surprise.SMTnews : pearson = 0.6512, spearman = 0.5008
2019-02-12 22:39:16,250 : ALL (weighted average) : Pearson = 0.3716,             Spearman = 0.3977
2019-02-12 22:39:16,250 : ALL (average) : Pearson = 0.4049,             Spearman = 0.4211

2019-02-12 22:39:16,250 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 22:39:16,258 : loading BERT mode bert-base-uncased
2019-02-12 22:39:16,258 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:39:16,275 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:39:16,276 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2umtjwk6
2019-02-12 22:39:18,708 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:39:20,924 : FNWN : pearson = 0.2605, spearman = 0.2693
2019-02-12 22:39:22,241 : headlines : pearson = 0.4268, spearman = 0.4616
2019-02-12 22:39:23,233 : OnWN : pearson = 0.2192, spearman = 0.2325
2019-02-12 22:39:23,233 : ALL (weighted average) : Pearson = 0.3282,             Spearman = 0.3517
2019-02-12 22:39:23,233 : ALL (average) : Pearson = 0.3022,             Spearman = 0.3211

2019-02-12 22:39:23,233 : ***** Transfer task : STS14 *****


2019-02-12 22:39:23,249 : loading BERT mode bert-base-uncased
2019-02-12 22:39:23,249 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:39:23,297 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:39:23,297 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyuhhjios
2019-02-12 22:39:25,738 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:39:27,909 : deft-forum : pearson = 0.1723, spearman = 0.2090
2019-02-12 22:39:28,828 : deft-news : pearson = 0.6384, spearman = 0.6529
2019-02-12 22:39:30,494 : headlines : pearson = 0.3766, spearman = 0.3803
2019-02-12 22:39:32,285 : images : pearson = 0.4072, spearman = 0.3987
2019-02-12 22:39:34,164 : OnWN : pearson = 0.3094, spearman = 0.3178
2019-02-12 22:39:36,203 : tweet-news : pearson = 0.3350, spearman = 0.3366
2019-02-12 22:39:36,204 : ALL (weighted average) : Pearson = 0.3574,             Spearman = 0.3640
2019-02-12 22:39:36,204 : ALL (average) : Pearson = 0.3731,             Spearman = 0.3825

2019-02-12 22:39:36,204 : ***** Transfer task : STS15 *****


2019-02-12 22:39:36,251 : loading BERT mode bert-base-uncased
2019-02-12 22:39:36,251 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:39:36,269 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:39:36,269 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpge0ftru6
2019-02-12 22:39:38,694 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:39:41,307 : answers-forums : pearson = 0.3616, spearman = 0.3888
2019-02-12 22:39:43,123 : answers-students : pearson = 0.4689, spearman = 0.4909
2019-02-12 22:39:44,481 : belief : pearson = 0.3829, spearman = 0.4623
2019-02-12 22:39:46,256 : headlines : pearson = 0.4983, spearman = 0.5240
2019-02-12 22:39:48,101 : images : pearson = 0.1685, spearman = 0.3588
2019-02-12 22:39:48,101 : ALL (weighted average) : Pearson = 0.3770,             Spearman = 0.4498
2019-02-12 22:39:48,102 : ALL (average) : Pearson = 0.3761,             Spearman = 0.4450

2019-02-12 22:39:48,102 : ***** Transfer task : STS16 *****


2019-02-12 22:39:48,170 : loading BERT mode bert-base-uncased
2019-02-12 22:39:48,170 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:39:48,189 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:39:48,189 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp140v2i79
2019-02-12 22:39:50,619 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:39:52,541 : answer-answer : pearson = 0.3889, spearman = 0.4610
2019-02-12 22:39:52,984 : headlines : pearson = 0.4948, spearman = 0.5431
2019-02-12 22:39:53,500 : plagiarism : pearson = 0.6495, spearman = 0.7059
2019-02-12 22:39:54,261 : postediting : pearson = 0.5843, spearman = 0.7856
2019-02-12 22:39:54,658 : question-question : pearson = 0.4453, spearman = 0.4464
2019-02-12 22:39:54,658 : ALL (weighted average) : Pearson = 0.5118,             Spearman = 0.5899
2019-02-12 22:39:54,658 : ALL (average) : Pearson = 0.5126,             Spearman = 0.5884

2019-02-12 22:39:54,658 : ***** Transfer task : MR *****


2019-02-12 22:39:54,674 : loading BERT mode bert-base-uncased
2019-02-12 22:39:54,675 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:39:54,695 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:39:54,695 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjmsscnnl
2019-02-12 22:39:57,141 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:39:58,602 : Generating sentence embeddings
2019-02-12 22:40:16,948 : Generated sentence embeddings
2019-02-12 22:40:16,949 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:40:42,423 : Best param found at split 1: l2reg = 0.001                 with score 70.34
2019-02-12 22:41:17,325 : Best param found at split 2: l2reg = 0.0001                 with score 69.14
2019-02-12 22:41:50,700 : Best param found at split 3: l2reg = 0.0001                 with score 70.22
2019-02-12 22:42:14,922 : Best param found at split 4: l2reg = 1e-05                 with score 68.83
2019-02-12 22:42:41,509 : Best param found at split 5: l2reg = 0.001                 with score 69.11
2019-02-12 22:42:43,220 : Dev acc : 69.53 Test acc : 68.13

2019-02-12 22:42:43,221 : ***** Transfer task : CR *****


2019-02-12 22:42:43,228 : loading BERT mode bert-base-uncased
2019-02-12 22:42:43,228 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:42:43,248 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:42:43,248 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz03ey2sg
2019-02-12 22:42:45,672 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:42:47,160 : Generating sentence embeddings
2019-02-12 22:42:53,130 : Generated sentence embeddings
2019-02-12 22:42:53,130 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:43:02,241 : Best param found at split 1: l2reg = 0.001                 with score 72.04
2019-02-12 22:43:12,335 : Best param found at split 2: l2reg = 0.001                 with score 74.0
2019-02-12 22:43:21,369 : Best param found at split 3: l2reg = 1e-05                 with score 72.71
2019-02-12 22:43:31,752 : Best param found at split 4: l2reg = 0.001                 with score 74.64
2019-02-12 22:43:41,861 : Best param found at split 5: l2reg = 0.001                 with score 73.72
2019-02-12 22:43:42,334 : Dev acc : 73.42 Test acc : 69.38

2019-02-12 22:43:42,334 : ***** Transfer task : MPQA *****


2019-02-12 22:43:42,340 : loading BERT mode bert-base-uncased
2019-02-12 22:43:42,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:43:42,359 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:43:42,359 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8co73207
2019-02-12 22:43:44,758 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:43:46,261 : Generating sentence embeddings
2019-02-12 22:43:57,183 : Generated sentence embeddings
2019-02-12 22:43:57,184 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:44:23,891 : Best param found at split 1: l2reg = 0.001                 with score 84.04
2019-02-12 22:44:42,793 : Best param found at split 2: l2reg = 1e-05                 with score 83.42
2019-02-12 22:45:09,476 : Best param found at split 3: l2reg = 1e-05                 with score 83.7
2019-02-12 22:45:38,703 : Best param found at split 4: l2reg = 0.001                 with score 83.62
2019-02-12 22:46:25,986 : Best param found at split 5: l2reg = 0.001                 with score 85.56
2019-02-12 22:46:28,043 : Dev acc : 84.07 Test acc : 84.5

2019-02-12 22:46:28,044 : ***** Transfer task : SUBJ *****


2019-02-12 22:46:28,058 : loading BERT mode bert-base-uncased
2019-02-12 22:46:28,058 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:46:28,079 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:46:28,079 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppzml1xxt
2019-02-12 22:46:30,471 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:46:32,022 : Generating sentence embeddings
2019-02-12 22:46:55,820 : Generated sentence embeddings
2019-02-12 22:46:55,820 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:47:25,711 : Best param found at split 1: l2reg = 0.0001                 with score 92.99
2019-02-12 22:47:55,766 : Best param found at split 2: l2reg = 0.0001                 with score 93.14
2019-02-12 22:48:20,283 : Best param found at split 3: l2reg = 1e-05                 with score 92.36
2019-02-12 22:48:45,124 : Best param found at split 4: l2reg = 1e-05                 with score 93.19
2019-02-12 22:49:13,828 : Best param found at split 5: l2reg = 0.001                 with score 92.62
2019-02-12 22:49:15,465 : Dev acc : 92.86 Test acc : 92.68

2019-02-12 22:49:15,466 : ***** Transfer task : SST Binary classification *****


2019-02-12 22:49:15,593 : loading BERT mode bert-base-uncased
2019-02-12 22:49:15,593 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:49:15,617 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:49:15,617 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpllf00nn8
2019-02-12 22:49:18,034 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:49:19,506 : Computing embedding for train
2019-02-12 22:51:06,015 : Computed train embeddings
2019-02-12 22:51:06,015 : Computing embedding for dev
2019-02-12 22:51:07,869 : Computed dev embeddings
2019-02-12 22:51:07,869 : Computing embedding for test
2019-02-12 22:51:11,744 : Computed test embeddings
2019-02-12 22:51:11,744 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:51:47,356 : [('reg:1e-05', 78.33), ('reg:0.0001', 78.1), ('reg:0.001', 77.75), ('reg:0.01', 67.78)]
2019-02-12 22:51:47,357 : Validation : best param found is reg = 1e-05 with score             78.33
2019-02-12 22:51:47,357 : Evaluating...
2019-02-12 22:51:56,437 : 
Dev acc : 78.33 Test acc : 77.81 for             SST Binary classification

2019-02-12 22:51:56,442 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 22:51:56,492 : loading BERT mode bert-base-uncased
2019-02-12 22:51:56,492 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:51:56,512 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:51:56,513 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv1cgxlat
2019-02-12 22:51:58,925 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:52:00,387 : Computing embedding for train
2019-02-12 22:52:16,565 : Computed train embeddings
2019-02-12 22:52:16,565 : Computing embedding for dev
2019-02-12 22:52:18,302 : Computed dev embeddings
2019-02-12 22:52:18,302 : Computing embedding for test
2019-02-12 22:52:21,818 : Computed test embeddings
2019-02-12 22:52:21,818 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:52:27,636 : [('reg:1e-05', 34.33), ('reg:0.0001', 35.6), ('reg:0.001', 36.33), ('reg:0.01', 35.6)]
2019-02-12 22:52:27,636 : Validation : best param found is reg = 0.001 with score             36.33
2019-02-12 22:52:27,636 : Evaluating...
2019-02-12 22:52:29,051 : 
Dev acc : 36.33 Test acc : 38.73 for             SST Fine-Grained classification

2019-02-12 22:52:29,051 : ***** Transfer task : TREC *****


2019-02-12 22:52:29,064 : loading BERT mode bert-base-uncased
2019-02-12 22:52:29,064 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:52:29,083 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:52:29,083 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpio_favmd
2019-02-12 22:52:31,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:52:39,964 : Computed train embeddings
2019-02-12 22:52:40,374 : Computed test embeddings
2019-02-12 22:52:40,374 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 22:52:51,637 : [('reg:1e-05', 67.77), ('reg:0.0001', 66.97), ('reg:0.001', 65.33), ('reg:0.01', 61.07)]
2019-02-12 22:52:51,637 : Cross-validation : best param found is reg = 1e-05             with score 67.77
2019-02-12 22:52:51,637 : Evaluating...
2019-02-12 22:52:52,691 : 
Dev acc : 67.77 Test acc : 79.2             for TREC

2019-02-12 22:52:52,692 : ***** Transfer task : MRPC *****


2019-02-12 22:52:52,713 : loading BERT mode bert-base-uncased
2019-02-12 22:52:52,713 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:52:52,733 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:52:52,733 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy31fex41
2019-02-12 22:52:55,146 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:52:56,632 : Computing embedding for train
2019-02-12 22:53:08,537 : Computed train embeddings
2019-02-12 22:53:08,537 : Computing embedding for test
2019-02-12 22:53:14,328 : Computed test embeddings
2019-02-12 22:53:14,344 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 22:53:24,129 : [('reg:1e-05', 69.63), ('reg:0.0001', 70.07), ('reg:0.001', 68.06), ('reg:0.01', 69.55)]
2019-02-12 22:53:24,129 : Cross-validation : best param found is reg = 0.0001             with score 70.07
2019-02-12 22:53:24,129 : Evaluating...
2019-02-12 22:53:24,911 : Dev acc : 70.07 Test acc 67.42; Test F1 75.46 for MRPC.

2019-02-12 22:53:24,911 : ***** Transfer task : SICK-Entailment*****


2019-02-12 22:53:24,975 : loading BERT mode bert-base-uncased
2019-02-12 22:53:24,975 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:53:24,994 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:53:24,994 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvnwts_3z
2019-02-12 22:53:27,450 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:53:28,946 : Computing embedding for train
2019-02-12 22:53:36,789 : Computed train embeddings
2019-02-12 22:53:36,789 : Computing embedding for dev
2019-02-12 22:53:37,599 : Computed dev embeddings
2019-02-12 22:53:37,599 : Computing embedding for test
2019-02-12 22:53:48,245 : Computed test embeddings
2019-02-12 22:53:48,273 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:53:52,562 : [('reg:1e-05', 72.4), ('reg:0.0001', 73.2), ('reg:0.001', 67.8), ('reg:0.01', 62.2)]
2019-02-12 22:53:52,562 : Validation : best param found is reg = 0.0001 with score             73.2
2019-02-12 22:53:52,562 : Evaluating...
2019-02-12 22:53:53,719 : 
Dev acc : 73.2 Test acc : 70.98 for                        SICK entailment

2019-02-12 22:53:53,719 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 22:53:53,747 : loading BERT mode bert-base-uncased
2019-02-12 22:53:53,747 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:53:53,806 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:53:53,806 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpit2sthmw
2019-02-12 22:53:56,200 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:53:57,723 : Computing embedding for train
2019-02-12 22:54:06,619 : Computed train embeddings
2019-02-12 22:54:06,620 : Computing embedding for dev
2019-02-12 22:54:07,741 : Computed dev embeddings
2019-02-12 22:54:07,741 : Computing embedding for test
2019-02-12 22:54:17,766 : Computed test embeddings
2019-02-12 22:55:14,928 : Dev : Pearson 0.7498275509314094
2019-02-12 22:55:14,930 : Test : Pearson 0.7556118409882442 Spearman 0.6920325135739948 MSE 0.44186181803867103                        for SICK Relatedness

2019-02-12 22:55:14,931 : 

***** Transfer task : STSBenchmark*****


2019-02-12 22:55:15,000 : loading BERT mode bert-base-uncased
2019-02-12 22:55:15,000 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:55:15,020 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:55:15,020 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp_m9fpto
2019-02-12 22:55:17,454 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:55:19,000 : Computing embedding for train
2019-02-12 22:55:37,084 : Computed train embeddings
2019-02-12 22:55:37,084 : Computing embedding for dev
2019-02-12 22:55:41,082 : Computed dev embeddings
2019-02-12 22:55:41,082 : Computing embedding for test
2019-02-12 22:55:44,899 : Computed test embeddings
2019-02-12 22:56:25,637 : Dev : Pearson 0.6365482500673998
2019-02-12 22:56:25,637 : Test : Pearson 0.6203475545781522 Spearman 0.6151130448745691 MSE 1.6714750152740945                        for SICK Relatedness

2019-02-12 22:56:25,637 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 22:56:25,952 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 22:56:25,962 : loading BERT mode bert-base-uncased
2019-02-12 22:56:25,962 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:56:25,985 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:56:25,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphh8aiapm
2019-02-12 22:56:28,425 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:56:29,910 : Computing embeddings for train/dev/test
2019-02-12 22:59:06,499 : Computed embeddings
2019-02-12 22:59:06,499 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:59:59,612 : [('reg:1e-05', 71.81), ('reg:0.0001', 73.46), ('reg:0.001', 63.1), ('reg:0.01', 54.8)]
2019-02-12 22:59:59,612 : Validation : best param found is reg = 0.0001 with score             73.46
2019-02-12 22:59:59,612 : Evaluating...
2019-02-12 23:00:12,289 : 
Dev acc : 73.5 Test acc : 75.0 for LENGTH classification

2019-02-12 23:00:12,298 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 23:00:12,641 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 23:00:12,686 : loading BERT mode bert-base-uncased
2019-02-12 23:00:12,686 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:00:12,714 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:00:12,714 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxktpizae
2019-02-12 23:00:15,151 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:00:16,684 : Computing embeddings for train/dev/test
2019-02-12 23:02:35,973 : Computed embeddings
2019-02-12 23:02:35,974 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:03:49,811 : [('reg:1e-05', 10.63), ('reg:0.0001', 2.44), ('reg:0.001', 0.47), ('reg:0.01', 0.2)]
2019-02-12 23:03:49,811 : Validation : best param found is reg = 1e-05 with score             10.63
2019-02-12 23:03:49,811 : Evaluating...
2019-02-12 23:04:12,350 : 
Dev acc : 10.6 Test acc : 10.5 for WORDCONTENT classification

2019-02-12 23:04:12,359 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 23:04:12,700 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 23:04:12,764 : loading BERT mode bert-base-uncased
2019-02-12 23:04:12,764 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:04:12,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:04:12,858 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf0izwstm
2019-02-12 23:04:15,274 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:04:16,771 : Computing embeddings for train/dev/test
2019-02-12 23:06:54,030 : Computed embeddings
2019-02-12 23:06:54,031 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:08:14,630 : [('reg:1e-05', 29.86), ('reg:0.0001', 27.77), ('reg:0.001', 26.97), ('reg:0.01', 25.02)]
2019-02-12 23:08:14,630 : Validation : best param found is reg = 1e-05 with score             29.86
2019-02-12 23:08:14,630 : Evaluating...
2019-02-12 23:08:25,261 : 
Dev acc : 29.9 Test acc : 29.8 for DEPTH classification

2019-02-12 23:08:25,270 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 23:08:25,829 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 23:08:25,891 : loading BERT mode bert-base-uncased
2019-02-12 23:08:25,891 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:08:25,918 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:08:25,918 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdywfk0z7
2019-02-12 23:08:28,362 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:08:29,850 : Computing embeddings for train/dev/test
2019-02-12 23:10:49,149 : Computed embeddings
2019-02-12 23:10:49,149 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:12:05,546 : [('reg:1e-05', 68.04), ('reg:0.0001', 66.68), ('reg:0.001', 60.84), ('reg:0.01', 39.0)]
2019-02-12 23:12:05,546 : Validation : best param found is reg = 1e-05 with score             68.04
2019-02-12 23:12:05,546 : Evaluating...
2019-02-12 23:12:24,447 : 
Dev acc : 68.0 Test acc : 68.3 for TOPCONSTITUENTS classification

2019-02-12 23:12:24,456 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 23:12:24,793 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 23:12:24,858 : loading BERT mode bert-base-uncased
2019-02-12 23:12:24,859 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:12:24,977 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:12:24,977 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2qpejfxl
2019-02-12 23:12:27,395 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:12:28,873 : Computing embeddings for train/dev/test
2019-02-12 23:15:22,262 : Computed embeddings
2019-02-12 23:15:22,262 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:16:51,259 : [('reg:1e-05', 81.16), ('reg:0.0001', 81.92), ('reg:0.001', 80.83), ('reg:0.01', 72.26)]
2019-02-12 23:16:51,260 : Validation : best param found is reg = 0.0001 with score             81.92
2019-02-12 23:16:51,260 : Evaluating...
2019-02-12 23:17:18,038 : 
Dev acc : 81.9 Test acc : 81.1 for BIGRAMSHIFT classification

2019-02-12 23:17:18,046 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 23:17:18,602 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 23:17:18,667 : loading BERT mode bert-base-uncased
2019-02-12 23:17:18,667 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:17:18,697 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:17:18,697 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplsxfo1p2
2019-02-12 23:17:21,127 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:17:22,608 : Computing embeddings for train/dev/test
2019-02-12 23:20:12,509 : Computed embeddings
2019-02-12 23:20:12,509 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:21:19,450 : [('reg:1e-05', 89.66), ('reg:0.0001', 89.7), ('reg:0.001', 89.95), ('reg:0.01', 89.9)]
2019-02-12 23:21:19,450 : Validation : best param found is reg = 0.001 with score             89.95
2019-02-12 23:21:19,450 : Evaluating...
2019-02-12 23:21:29,843 : 
Dev acc : 90.0 Test acc : 88.7 for TENSE classification

2019-02-12 23:21:29,851 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 23:21:30,414 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 23:21:30,477 : loading BERT mode bert-base-uncased
2019-02-12 23:21:30,477 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:21:30,506 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:21:30,506 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8asyjthe
2019-02-12 23:21:32,978 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:21:34,463 : Computing embeddings for train/dev/test
2019-02-12 23:24:15,279 : Computed embeddings
2019-02-12 23:24:15,279 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:25:09,744 : [('reg:1e-05', 84.63), ('reg:0.0001', 85.04), ('reg:0.001', 84.33), ('reg:0.01', 72.69)]
2019-02-12 23:25:09,744 : Validation : best param found is reg = 0.0001 with score             85.04
2019-02-12 23:25:09,744 : Evaluating...
2019-02-12 23:25:27,348 : 
Dev acc : 85.0 Test acc : 84.3 for SUBJNUMBER classification

2019-02-12 23:25:27,357 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 23:25:27,781 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 23:25:27,847 : loading BERT mode bert-base-uncased
2019-02-12 23:25:27,847 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:25:27,874 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:25:27,874 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxoz7l6hs
2019-02-12 23:25:30,350 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:25:31,836 : Computing embeddings for train/dev/test
2019-02-12 23:28:03,033 : Computed embeddings
2019-02-12 23:28:03,033 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:28:58,471 : [('reg:1e-05', 77.59), ('reg:0.0001', 77.43), ('reg:0.001', 76.93), ('reg:0.01', 72.88)]
2019-02-12 23:28:58,471 : Validation : best param found is reg = 1e-05 with score             77.59
2019-02-12 23:28:58,472 : Evaluating...
2019-02-12 23:29:15,746 : 
Dev acc : 77.6 Test acc : 79.2 for OBJNUMBER classification

2019-02-12 23:29:15,756 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 23:29:16,152 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 23:29:16,221 : loading BERT mode bert-base-uncased
2019-02-12 23:29:16,221 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:29:16,348 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:29:16,348 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpds533280
2019-02-12 23:29:18,788 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:29:20,276 : Computing embeddings for train/dev/test
2019-02-12 23:32:07,857 : Computed embeddings
2019-02-12 23:32:07,857 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:33:12,768 : [('reg:1e-05', 56.73), ('reg:0.0001', 57.22), ('reg:0.001', 56.41), ('reg:0.01', 54.6)]
2019-02-12 23:33:12,768 : Validation : best param found is reg = 0.0001 with score             57.22
2019-02-12 23:33:12,768 : Evaluating...
2019-02-12 23:33:39,827 : 
Dev acc : 57.2 Test acc : 57.9 for ODDMANOUT classification

2019-02-12 23:33:39,834 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 23:33:40,234 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 23:33:40,310 : loading BERT mode bert-base-uncased
2019-02-12 23:33:40,310 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:33:40,438 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:33:40,438 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnii0h4pf
2019-02-12 23:33:42,910 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:33:44,437 : Computing embeddings for train/dev/test
2019-02-12 23:36:39,189 : Computed embeddings
2019-02-12 23:36:39,189 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:37:41,989 : [('reg:1e-05', 54.88), ('reg:0.0001', 54.62), ('reg:0.001', 52.86), ('reg:0.01', 50.4)]
2019-02-12 23:37:41,989 : Validation : best param found is reg = 1e-05 with score             54.88
2019-02-12 23:37:41,989 : Evaluating...
2019-02-12 23:37:58,553 : 
Dev acc : 54.9 Test acc : 54.6 for COORDINATIONINVERSION classification

2019-02-12 23:37:58,561 : {'STS12': {'MSRpar': {'pearson': (0.2971227234301257, 9.418333968579935e-17), 'spearman': SpearmanrResult(correlation=0.35142632036170107, pvalue=3.192464704960887e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.34937659623191775, 5.923635071681274e-23), 'spearman': SpearmanrResult(correlation=0.41698694697985667, pvalue=6.554150548387194e-33), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4626515966033861, 1.0029133258902804e-25), 'spearman': SpearmanrResult(correlation=0.5756765837572106, pvalue=7.238850842890226e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.26394954746824617, 2.0242578577525526e-13), 'spearman': SpearmanrResult(correlation=0.26073821390731844, pvalue=4.029961159439806e-13), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6512370926667364, 1.6387094141862897e-49), 'spearman': SpearmanrResult(correlation=0.5008252729490448, pvalue=1.0093993922350583e-26), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4048675112800824, 'wmean': 0.3716336335787321}, 'spearman': {'mean': 0.42113066759102635, 'wmean': 0.39766037541437765}}}, 'STS13': {'FNWN': {'pearson': (0.2605235436491155, 0.00029395363582579274), 'spearman': SpearmanrResult(correlation=0.26930418586625043, pvalue=0.00017879974831908193), 'nsamples': 189}, 'headlines': {'pearson': (0.4268185667756522, 1.4696126319516573e-34), 'spearman': SpearmanrResult(correlation=0.4616479615342043, pvalue=7.386552637514035e-41), 'nsamples': 750}, 'OnWN': {'pearson': (0.2192320225526428, 1.5631373431128984e-07), 'spearman': SpearmanrResult(correlation=0.23248703927576894, pvalue=2.5359466571712093e-08), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.3021913776591368, 'wmean': 0.3282280263223031}, 'spearman': {'mean': 0.32114639555874125, 'wmean': 0.3517064608753873}}}, 'STS14': {'deft-forum': {'pearson': (0.17230108294297514, 0.00024028965360522862), 'spearman': SpearmanrResult(correlation=0.20902438845557683, pvalue=7.777132575835715e-06), 'nsamples': 450}, 'deft-news': {'pearson': (0.6384187304669929, 9.534020638482025e-36), 'spearman': SpearmanrResult(correlation=0.6528549005542249, pvalue=7.958712793391714e-38), 'nsamples': 300}, 'headlines': {'pearson': (0.37657664401592794, 1.1111439688567723e-26), 'spearman': SpearmanrResult(correlation=0.3803337333069784, pvalue=3.1798873606343923e-27), 'nsamples': 750}, 'images': {'pearson': (0.4071561798414186, 2.581716206052868e-31), 'spearman': SpearmanrResult(correlation=0.39865970418224156, pvalue=5.6044854569499715e-30), 'nsamples': 750}, 'OnWN': {'pearson': (0.3094026058937166, 4.212259948470871e-18), 'spearman': SpearmanrResult(correlation=0.31781712849714416, pvalue=4.596061873607641e-19), 'nsamples': 750}, 'tweet-news': {'pearson': (0.3350291422258736, 3.953779379131649e-21), 'spearman': SpearmanrResult(correlation=0.3365704209581113, pvalue=2.5442787190877192e-21), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3731473975644841, 'wmean': 0.3573825427859038}, 'spearman': {'mean': 0.38254337932571286, 'wmean': 0.3639875160479023}}}, 'STS15': {'answers-forums': {'pearson': (0.36158190409207785, 5.029002078944632e-13), 'spearman': SpearmanrResult(correlation=0.3888180721765381, pvalue=5.545363270832513e-15), 'nsamples': 375}, 'answers-students': {'pearson': (0.4688868182346517, 2.919777695546361e-42), 'spearman': SpearmanrResult(correlation=0.4909319229852269, pvalue=9.537035784989864e-47), 'nsamples': 750}, 'belief': {'pearson': (0.3829437745161806, 1.5197412304166492e-14), 'spearman': SpearmanrResult(correlation=0.46233858763569785, pvalue=2.93777533904565e-21), 'nsamples': 375}, 'headlines': {'pearson': (0.49831736729198006, 2.5228985993672837e-48), 'spearman': SpearmanrResult(correlation=0.5239540166334941, pvalue=4.13238877345749e-54), 'nsamples': 750}, 'images': {'pearson': (0.1685303765123521, 3.4684356124511663e-06), 'spearman': SpearmanrResult(correlation=0.35881483418116455, pvalue=3.3105522623918325e-24), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3760520481294484, 'wmean': 0.3769993503357783}, 'spearman': {'mean': 0.44497148672242426, 'wmean': 0.4498197759265009}}}, 'STS16': {'answer-answer': {'pearson': (0.3889011107540634, 1.3442679928987114e-10), 'spearman': SpearmanrResult(correlation=0.46104432511878457, pvalue=8.99948857747052e-15), 'nsamples': 254}, 'headlines': {'pearson': (0.494777084216068, 8.828006452188184e-17), 'spearman': SpearmanrResult(correlation=0.5430694695380944, pvalue=1.672342206772441e-20), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6495016721254091, 6.022522611604663e-29), 'spearman': SpearmanrResult(correlation=0.705883715572025, pvalue=5.312396291554973e-36), 'nsamples': 230}, 'postediting': {'pearson': (0.5843303277420319, 9.759781574414356e-24), 'spearman': SpearmanrResult(correlation=0.7856024923373829, pvalue=2.274207572005856e-52), 'nsamples': 244}, 'question-question': {'pearson': (0.4452786566959501, 1.4239798826252169e-11), 'spearman': SpearmanrResult(correlation=0.446386214605813, pvalue=1.2504590024650582e-11), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5125577703067045, 'wmean': 0.5118091061624676}, 'spearman': {'mean': 0.58839724343442, 'wmean': 0.5899363727315842}}}, 'MR': {'devacc': 69.53, 'acc': 68.13, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 73.42, 'acc': 69.38, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.07, 'acc': 84.5, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.86, 'acc': 92.68, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.33, 'acc': 77.81, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 36.33, 'acc': 38.73, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.77, 'acc': 79.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.07, 'acc': 67.42, 'f1': 75.46, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.2, 'acc': 70.98, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7498275509314094, 'pearson': 0.7556118409882442, 'spearman': 0.6920325135739948, 'mse': 0.44186181803867103, 'yhat': array([3.34850412, 3.91901105, 2.24816336, ..., 2.95443197, 4.44030722,
       4.58507018]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6365482500673998, 'pearson': 0.6203475545781522, 'spearman': 0.6151130448745691, 'mse': 1.6714750152740945, 'yhat': array([2.41759952, 1.6228788 , 2.95518608, ..., 3.84841727, 3.63706733,
       3.25899408]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 73.46, 'acc': 74.95, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 10.63, 'acc': 10.49, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.86, 'acc': 29.81, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 68.04, 'acc': 68.32, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 81.92, 'acc': 81.09, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.95, 'acc': 88.68, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.04, 'acc': 84.35, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.59, 'acc': 79.21, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.22, 'acc': 57.88, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 54.88, 'acc': 54.56, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 23:37:58,561 : ********************************************************************************
2019-02-12 23:37:58,561 : ********************************************************************************
2019-02-12 23:37:58,561 : ********************************************************************************
2019-02-12 23:37:58,561 : layer 6
2019-02-12 23:37:58,561 : ********************************************************************************
2019-02-12 23:37:58,561 : ********************************************************************************
2019-02-12 23:37:58,561 : ********************************************************************************
2019-02-12 23:37:58,650 : ***** Transfer task : STS12 *****


2019-02-12 23:37:58,687 : loading BERT mode bert-base-uncased
2019-02-12 23:37:58,688 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:37:58,704 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:37:58,704 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0w36q09w
2019-02-12 23:38:01,137 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:38:05,319 : MSRpar : pearson = 0.2264, spearman = 0.2758
2019-02-12 23:38:07,211 : MSRvid : pearson = 0.1589, spearman = 0.1729
2019-02-12 23:38:08,571 : SMTeuroparl : pearson = 0.4183, spearman = 0.5189
2019-02-12 23:38:10,915 : surprise.OnWN : pearson = 0.3270, spearman = 0.3222
2019-02-12 23:38:12,158 : surprise.SMTnews : pearson = 0.5912, spearman = 0.4420
2019-02-12 23:38:12,158 : ALL (weighted average) : Pearson = 0.3096,             Spearman = 0.3194
2019-02-12 23:38:12,159 : ALL (average) : Pearson = 0.3444,             Spearman = 0.3464

2019-02-12 23:38:12,159 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 23:38:12,167 : loading BERT mode bert-base-uncased
2019-02-12 23:38:12,167 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:38:12,184 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:38:12,185 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb9xfxs93
2019-02-12 23:38:14,623 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:38:17,094 : FNWN : pearson = 0.0960, spearman = 0.1243
2019-02-12 23:38:19,117 : headlines : pearson = 0.4524, spearman = 0.4446
2019-02-12 23:38:20,696 : OnWN : pearson = 0.1898, spearman = 0.1940
2019-02-12 23:38:20,697 : ALL (weighted average) : Pearson = 0.3093,             Spearman = 0.3105
2019-02-12 23:38:20,697 : ALL (average) : Pearson = 0.2461,             Spearman = 0.2543

2019-02-12 23:38:20,697 : ***** Transfer task : STS14 *****


2019-02-12 23:38:20,714 : loading BERT mode bert-base-uncased
2019-02-12 23:38:20,715 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:38:20,764 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:38:20,764 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp68r63qul
2019-02-12 23:38:23,209 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:38:25,995 : deft-forum : pearson = -0.0012, spearman = 0.0070
2019-02-12 23:38:27,266 : deft-news : pearson = 0.4521, spearman = 0.4936
2019-02-12 23:38:29,468 : headlines : pearson = 0.4028, spearman = 0.3796
2019-02-12 23:38:31,752 : images : pearson = 0.2372, spearman = 0.2497
2019-02-12 23:38:34,063 : OnWN : pearson = 0.3358, spearman = 0.3405
2019-02-12 23:38:36,688 : tweet-news : pearson = 0.3987, spearman = 0.3799
2019-02-12 23:38:36,688 : ALL (weighted average) : Pearson = 0.3109,             Spearman = 0.3103
2019-02-12 23:38:36,688 : ALL (average) : Pearson = 0.3042,             Spearman = 0.3084

2019-02-12 23:38:36,688 : ***** Transfer task : STS15 *****


2019-02-12 23:38:36,719 : loading BERT mode bert-base-uncased
2019-02-12 23:38:36,719 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:38:36,736 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:38:36,736 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn0x0l6c6
2019-02-12 23:38:39,190 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:38:42,040 : answers-forums : pearson = 0.2137, spearman = 0.2275
2019-02-12 23:38:44,840 : answers-students : pearson = 0.4765, spearman = 0.4838
2019-02-12 23:38:46,654 : belief : pearson = 0.3384, spearman = 0.4115
2019-02-12 23:38:49,695 : headlines : pearson = 0.4804, spearman = 0.4834
2019-02-12 23:38:53,203 : images : pearson = 0.1113, spearman = 0.1542
2019-02-12 23:38:53,203 : ALL (weighted average) : Pearson = 0.3361,             Spearman = 0.3602
2019-02-12 23:38:53,203 : ALL (average) : Pearson = 0.3241,             Spearman = 0.3521

2019-02-12 23:38:53,203 : ***** Transfer task : STS16 *****


2019-02-12 23:38:53,276 : loading BERT mode bert-base-uncased
2019-02-12 23:38:53,276 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:38:53,294 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:38:53,294 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzafelvmg
2019-02-12 23:38:55,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:38:58,075 : answer-answer : pearson = 0.3775, spearman = 0.4270
2019-02-12 23:38:58,899 : headlines : pearson = 0.5445, spearman = 0.5538
2019-02-12 23:38:59,896 : plagiarism : pearson = 0.6029, spearman = 0.6481
2019-02-12 23:39:01,226 : postediting : pearson = 0.5883, spearman = 0.7422
2019-02-12 23:39:02,131 : question-question : pearson = 0.1718, spearman = 0.2521
2019-02-12 23:39:02,131 : ALL (weighted average) : Pearson = 0.4634,             Spearman = 0.5306
2019-02-12 23:39:02,131 : ALL (average) : Pearson = 0.4570,             Spearman = 0.5247

2019-02-12 23:39:02,131 : ***** Transfer task : MR *****


2019-02-12 23:39:02,147 : loading BERT mode bert-base-uncased
2019-02-12 23:39:02,148 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:39:02,168 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:39:02,168 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpguz3x9qk
2019-02-12 23:39:04,608 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:39:06,196 : Generating sentence embeddings
2019-02-12 23:39:27,497 : Generated sentence embeddings
2019-02-12 23:39:27,497 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:39:45,565 : Best param found at split 1: l2reg = 1e-05                 with score 73.21
2019-02-12 23:39:58,836 : Best param found at split 2: l2reg = 1e-05                 with score 70.83
2019-02-12 23:40:19,614 : Best param found at split 3: l2reg = 1e-05                 with score 70.73
2019-02-12 23:40:43,881 : Best param found at split 4: l2reg = 0.001                 with score 71.27
2019-02-12 23:41:20,687 : Best param found at split 5: l2reg = 0.01                 with score 70.79
2019-02-12 23:41:23,505 : Dev acc : 71.37 Test acc : 70.79

2019-02-12 23:41:23,506 : ***** Transfer task : CR *****


2019-02-12 23:41:23,514 : loading BERT mode bert-base-uncased
2019-02-12 23:41:23,514 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:41:23,534 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:41:23,534 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqd1hoyop
2019-02-12 23:41:25,978 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:41:27,566 : Generating sentence embeddings
2019-02-12 23:41:35,755 : Generated sentence embeddings
2019-02-12 23:41:35,755 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:41:53,808 : Best param found at split 1: l2reg = 0.001                 with score 76.42
2019-02-12 23:42:07,712 : Best param found at split 2: l2reg = 1e-05                 with score 75.92
2019-02-12 23:42:22,784 : Best param found at split 3: l2reg = 0.001                 with score 74.87
2019-02-12 23:42:34,130 : Best param found at split 4: l2reg = 0.0001                 with score 74.81
2019-02-12 23:42:44,617 : Best param found at split 5: l2reg = 0.001                 with score 74.71
2019-02-12 23:42:45,109 : Dev acc : 75.35 Test acc : 70.57

2019-02-12 23:42:45,110 : ***** Transfer task : MPQA *****


2019-02-12 23:42:45,115 : loading BERT mode bert-base-uncased
2019-02-12 23:42:45,115 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:42:45,133 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:42:45,134 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv7cojill
2019-02-12 23:42:47,572 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:42:49,141 : Generating sentence embeddings
2019-02-12 23:43:00,348 : Generated sentence embeddings
2019-02-12 23:43:00,349 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:43:22,502 : Best param found at split 1: l2reg = 1e-05                 with score 83.71
2019-02-12 23:43:42,890 : Best param found at split 2: l2reg = 0.001                 with score 85.93
2019-02-12 23:44:04,400 : Best param found at split 3: l2reg = 1e-05                 with score 84.97
2019-02-12 23:44:30,195 : Best param found at split 4: l2reg = 0.0001                 with score 85.55
2019-02-12 23:45:02,972 : Best param found at split 5: l2reg = 0.001                 with score 85.0
2019-02-12 23:45:05,229 : Dev acc : 85.03 Test acc : 86.19

2019-02-12 23:45:05,230 : ***** Transfer task : SUBJ *****


2019-02-12 23:45:05,246 : loading BERT mode bert-base-uncased
2019-02-12 23:45:05,246 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:45:05,265 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:45:05,265 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu1w84_yy
2019-02-12 23:45:07,700 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:45:09,255 : Generating sentence embeddings
2019-02-12 23:45:34,505 : Generated sentence embeddings
2019-02-12 23:45:34,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:46:17,100 : Best param found at split 1: l2reg = 0.0001                 with score 94.0
2019-02-12 23:46:47,183 : Best param found at split 2: l2reg = 1e-05                 with score 93.84
2019-02-12 23:47:18,382 : Best param found at split 3: l2reg = 1e-05                 with score 93.62
2019-02-12 23:47:45,111 : Best param found at split 4: l2reg = 0.001                 with score 94.05
2019-02-12 23:48:04,345 : Best param found at split 5: l2reg = 1e-05                 with score 93.68
2019-02-12 23:48:05,811 : Dev acc : 93.84 Test acc : 93.85

2019-02-12 23:48:05,812 : ***** Transfer task : SST Binary classification *****


2019-02-12 23:48:05,943 : loading BERT mode bert-base-uncased
2019-02-12 23:48:05,943 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:48:05,966 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:48:05,966 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptw4vmugq
2019-02-12 23:48:08,401 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:48:09,861 : Computing embedding for train
2019-02-12 23:49:52,503 : Computed train embeddings
2019-02-12 23:49:52,503 : Computing embedding for dev
2019-02-12 23:49:54,492 : Computed dev embeddings
2019-02-12 23:49:54,492 : Computing embedding for test
2019-02-12 23:49:58,817 : Computed test embeddings
2019-02-12 23:49:58,817 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:50:47,953 : [('reg:1e-05', 79.13), ('reg:0.0001', 79.13), ('reg:0.001', 79.36), ('reg:0.01', 77.18)]
2019-02-12 23:50:47,953 : Validation : best param found is reg = 0.001 with score             79.36
2019-02-12 23:50:47,953 : Evaluating...
2019-02-12 23:50:58,198 : 
Dev acc : 79.36 Test acc : 77.81 for             SST Binary classification

2019-02-12 23:50:58,203 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 23:50:58,251 : loading BERT mode bert-base-uncased
2019-02-12 23:50:58,251 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:50:58,273 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:50:58,273 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmfgg17ct
2019-02-12 23:51:00,710 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:51:02,216 : Computing embedding for train
2019-02-12 23:51:19,436 : Computed train embeddings
2019-02-12 23:51:19,436 : Computing embedding for dev
2019-02-12 23:51:21,755 : Computed dev embeddings
2019-02-12 23:51:21,755 : Computing embedding for test
2019-02-12 23:51:26,390 : Computed test embeddings
2019-02-12 23:51:26,390 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:51:32,818 : [('reg:1e-05', 38.33), ('reg:0.0001', 40.42), ('reg:0.001', 39.15), ('reg:0.01', 34.97)]
2019-02-12 23:51:32,818 : Validation : best param found is reg = 0.0001 with score             40.42
2019-02-12 23:51:32,818 : Evaluating...
2019-02-12 23:51:35,171 : 
Dev acc : 40.42 Test acc : 41.49 for             SST Fine-Grained classification

2019-02-12 23:51:35,171 : ***** Transfer task : TREC *****


2019-02-12 23:51:35,191 : loading BERT mode bert-base-uncased
2019-02-12 23:51:35,191 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:51:35,220 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:51:35,221 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps_pj0tbq
2019-02-12 23:51:37,664 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:51:48,360 : Computed train embeddings
2019-02-12 23:51:49,072 : Computed test embeddings
2019-02-12 23:51:49,073 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 23:52:07,829 : [('reg:1e-05', 75.73), ('reg:0.0001', 75.97), ('reg:0.001', 74.61), ('reg:0.01', 66.25)]
2019-02-12 23:52:07,829 : Cross-validation : best param found is reg = 0.0001             with score 75.97
2019-02-12 23:52:07,829 : Evaluating...
2019-02-12 23:52:09,315 : 
Dev acc : 75.97 Test acc : 86.0             for TREC

2019-02-12 23:52:09,316 : ***** Transfer task : MRPC *****


2019-02-12 23:52:09,338 : loading BERT mode bert-base-uncased
2019-02-12 23:52:09,338 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:52:09,359 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:52:09,359 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqdas1iz0
2019-02-12 23:52:11,789 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:52:13,298 : Computing embedding for train
2019-02-12 23:52:29,883 : Computed train embeddings
2019-02-12 23:52:29,883 : Computing embedding for test
2019-02-12 23:52:37,639 : Computed test embeddings
2019-02-12 23:52:37,655 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 23:52:48,391 : [('reg:1e-05', 69.26), ('reg:0.0001', 70.39), ('reg:0.001', 70.05), ('reg:0.01', 69.16)]
2019-02-12 23:52:48,391 : Cross-validation : best param found is reg = 0.0001             with score 70.39
2019-02-12 23:52:48,391 : Evaluating...
2019-02-12 23:52:49,196 : Dev acc : 70.39 Test acc 71.88; Test F1 81.17 for MRPC.

2019-02-12 23:52:49,197 : ***** Transfer task : SICK-Entailment*****


2019-02-12 23:52:49,220 : loading BERT mode bert-base-uncased
2019-02-12 23:52:49,221 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:52:49,278 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:52:49,278 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr40t2e0k
2019-02-12 23:52:51,715 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:52:53,176 : Computing embedding for train
2019-02-12 23:53:04,498 : Computed train embeddings
2019-02-12 23:53:04,498 : Computing embedding for dev
2019-02-12 23:53:05,821 : Computed dev embeddings
2019-02-12 23:53:05,822 : Computing embedding for test
2019-02-12 23:53:18,538 : Computed test embeddings
2019-02-12 23:53:18,566 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:53:21,753 : [('reg:1e-05', 71.4), ('reg:0.0001', 68.2), ('reg:0.001', 73.2), ('reg:0.01', 67.0)]
2019-02-12 23:53:21,753 : Validation : best param found is reg = 0.001 with score             73.2
2019-02-12 23:53:21,753 : Evaluating...
2019-02-12 23:53:22,555 : 
Dev acc : 73.2 Test acc : 72.17 for                        SICK entailment

2019-02-12 23:53:22,555 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 23:53:22,582 : loading BERT mode bert-base-uncased
2019-02-12 23:53:22,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:53:22,602 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:53:22,602 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpev4t3xvs
2019-02-12 23:53:25,039 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:53:26,598 : Computing embedding for train
2019-02-12 23:53:38,882 : Computed train embeddings
2019-02-12 23:53:38,882 : Computing embedding for dev
2019-02-12 23:53:40,415 : Computed dev embeddings
2019-02-12 23:53:40,415 : Computing embedding for test
2019-02-12 23:53:54,415 : Computed test embeddings
2019-02-12 23:55:01,275 : Dev : Pearson 0.7423340884408229
2019-02-12 23:55:01,278 : Test : Pearson 0.7209892196358876 Spearman 0.6576542740719158 MSE 0.4989185742570675                        for SICK Relatedness

2019-02-12 23:55:01,278 : 

***** Transfer task : STSBenchmark*****


2019-02-12 23:55:01,347 : loading BERT mode bert-base-uncased
2019-02-12 23:55:01,348 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:55:01,368 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:55:01,368 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpylhw507x
2019-02-12 23:55:03,822 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:55:05,245 : Computing embedding for train
2019-02-12 23:55:20,310 : Computed train embeddings
2019-02-12 23:55:20,310 : Computing embedding for dev
2019-02-12 23:55:24,270 : Computed dev embeddings
2019-02-12 23:55:24,270 : Computing embedding for test
2019-02-12 23:55:27,860 : Computed test embeddings
2019-02-12 23:56:32,621 : Dev : Pearson 0.5317967869206648
2019-02-12 23:56:32,621 : Test : Pearson 0.5121652098439453 Spearman 0.5074582556323265 MSE 1.817709283474579                        for SICK Relatedness

2019-02-12 23:56:32,622 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 23:56:32,870 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 23:56:32,880 : loading BERT mode bert-base-uncased
2019-02-12 23:56:32,880 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:56:32,973 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:56:32,973 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjojbmzrq
2019-02-12 23:56:35,409 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:56:36,909 : Computing embeddings for train/dev/test
2019-02-12 23:59:22,297 : Computed embeddings
2019-02-12 23:59:22,297 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:00:17,790 : [('reg:1e-05', 74.33), ('reg:0.0001', 74.29), ('reg:0.001', 65.36), ('reg:0.01', 49.42)]
2019-02-13 00:00:17,790 : Validation : best param found is reg = 1e-05 with score             74.33
2019-02-13 00:00:17,790 : Evaluating...
2019-02-13 00:00:35,828 : 
Dev acc : 74.3 Test acc : 75.3 for LENGTH classification

2019-02-13 00:00:35,835 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 00:00:36,208 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 00:00:36,253 : loading BERT mode bert-base-uncased
2019-02-13 00:00:36,253 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:00:36,282 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:00:36,282 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2lvq3r79
2019-02-13 00:00:38,748 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:00:40,234 : Computing embeddings for train/dev/test
2019-02-13 00:03:12,846 : Computed embeddings
2019-02-13 00:03:12,846 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:04:33,137 : [('reg:1e-05', 15.08), ('reg:0.0001', 2.25), ('reg:0.001', 0.62), ('reg:0.01', 0.23)]
2019-02-13 00:04:33,137 : Validation : best param found is reg = 1e-05 with score             15.08
2019-02-13 00:04:33,137 : Evaluating...
2019-02-13 00:05:05,311 : 
Dev acc : 15.1 Test acc : 15.1 for WORDCONTENT classification

2019-02-13 00:05:05,318 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 00:05:05,674 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 00:05:05,739 : loading BERT mode bert-base-uncased
2019-02-13 00:05:05,740 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:05:05,837 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:05:05,837 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpddl_ti0t
2019-02-13 00:05:08,277 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:05:09,738 : Computing embeddings for train/dev/test
2019-02-13 00:07:35,654 : Computed embeddings
2019-02-13 00:07:35,654 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:08:54,552 : [('reg:1e-05', 29.87), ('reg:0.0001', 24.71), ('reg:0.001', 27.54), ('reg:0.01', 23.65)]
2019-02-13 00:08:54,553 : Validation : best param found is reg = 1e-05 with score             29.87
2019-02-13 00:08:54,553 : Evaluating...
2019-02-13 00:09:15,376 : 
Dev acc : 29.9 Test acc : 29.3 for DEPTH classification

2019-02-13 00:09:15,383 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 00:09:15,758 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 00:09:15,820 : loading BERT mode bert-base-uncased
2019-02-13 00:09:15,820 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:09:15,930 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:09:15,931 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpczb0gaq0
2019-02-13 00:09:18,372 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:09:19,947 : Computing embeddings for train/dev/test
2019-02-13 00:11:42,576 : Computed embeddings
2019-02-13 00:11:42,576 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:12:44,827 : [('reg:1e-05', 72.41), ('reg:0.0001', 71.15), ('reg:0.001', 63.99), ('reg:0.01', 53.5)]
2019-02-13 00:12:44,827 : Validation : best param found is reg = 1e-05 with score             72.41
2019-02-13 00:12:44,827 : Evaluating...
2019-02-13 00:13:01,563 : 
Dev acc : 72.4 Test acc : 73.0 for TOPCONSTITUENTS classification

2019-02-13 00:13:01,571 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 00:13:02,092 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 00:13:02,158 : loading BERT mode bert-base-uncased
2019-02-13 00:13:02,158 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:13:02,190 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:13:02,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2doac0w_
2019-02-13 00:13:04,629 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:13:06,115 : Computing embeddings for train/dev/test
2019-02-13 00:16:01,485 : Computed embeddings
2019-02-13 00:16:01,485 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:17:27,898 : [('reg:1e-05', 81.45), ('reg:0.0001', 81.3), ('reg:0.001', 80.92), ('reg:0.01', 77.32)]
2019-02-13 00:17:27,898 : Validation : best param found is reg = 1e-05 with score             81.45
2019-02-13 00:17:27,898 : Evaluating...
2019-02-13 00:17:51,952 : 
Dev acc : 81.5 Test acc : 80.6 for BIGRAMSHIFT classification

2019-02-13 00:17:51,960 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 00:17:52,368 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 00:17:52,434 : loading BERT mode bert-base-uncased
2019-02-13 00:17:52,434 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:17:52,466 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:17:52,466 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp53s9as4l
2019-02-13 00:17:54,940 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:17:56,429 : Computing embeddings for train/dev/test
2019-02-13 00:20:41,595 : Computed embeddings
2019-02-13 00:20:41,595 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:21:52,728 : [('reg:1e-05', 90.27), ('reg:0.0001', 90.4), ('reg:0.001', 90.35), ('reg:0.01', 89.97)]
2019-02-13 00:21:52,728 : Validation : best param found is reg = 0.0001 with score             90.4
2019-02-13 00:21:52,729 : Evaluating...
2019-02-13 00:22:06,212 : 
Dev acc : 90.4 Test acc : 89.0 for TENSE classification

2019-02-13 00:22:06,220 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 00:22:06,637 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 00:22:06,702 : loading BERT mode bert-base-uncased
2019-02-13 00:22:06,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:22:06,733 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:22:06,733 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphh9lafyh
2019-02-13 00:22:09,187 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:22:10,647 : Computing embeddings for train/dev/test
2019-02-13 00:24:35,932 : Computed embeddings
2019-02-13 00:24:35,932 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:25:43,746 : [('reg:1e-05', 87.45), ('reg:0.0001', 87.39), ('reg:0.001', 87.34), ('reg:0.01', 84.03)]
2019-02-13 00:25:43,746 : Validation : best param found is reg = 1e-05 with score             87.45
2019-02-13 00:25:43,746 : Evaluating...
2019-02-13 00:25:56,208 : 
Dev acc : 87.5 Test acc : 88.4 for SUBJNUMBER classification

2019-02-13 00:25:56,215 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 00:25:56,630 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 00:25:56,697 : loading BERT mode bert-base-uncased
2019-02-13 00:25:56,697 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:25:56,818 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:25:56,818 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp07m1ddwz
2019-02-13 00:25:59,271 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:26:00,670 : Computing embeddings for train/dev/test
2019-02-13 00:28:08,367 : Computed embeddings
2019-02-13 00:28:08,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:29:28,291 : [('reg:1e-05', 76.95), ('reg:0.0001', 76.94), ('reg:0.001', 76.43), ('reg:0.01', 73.91)]
2019-02-13 00:29:28,292 : Validation : best param found is reg = 1e-05 with score             76.95
2019-02-13 00:29:28,292 : Evaluating...
2019-02-13 00:29:39,347 : 
Dev acc : 77.0 Test acc : 78.4 for OBJNUMBER classification

2019-02-13 00:29:39,354 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 00:29:39,736 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 00:29:39,805 : loading BERT mode bert-base-uncased
2019-02-13 00:29:39,805 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:29:39,928 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:29:39,929 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplaw9o3k7
2019-02-13 00:29:42,363 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:29:43,799 : Computing embeddings for train/dev/test
2019-02-13 00:32:16,913 : Computed embeddings
2019-02-13 00:32:16,913 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:33:32,027 : [('reg:1e-05', 56.8), ('reg:0.0001', 56.7), ('reg:0.001', 56.39), ('reg:0.01', 55.95)]
2019-02-13 00:33:32,027 : Validation : best param found is reg = 1e-05 with score             56.8
2019-02-13 00:33:32,028 : Evaluating...
2019-02-13 00:33:45,637 : 
Dev acc : 56.8 Test acc : 56.7 for ODDMANOUT classification

2019-02-13 00:33:45,644 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 00:33:46,244 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 00:33:46,320 : loading BERT mode bert-base-uncased
2019-02-13 00:33:46,320 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:33:46,352 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:33:46,352 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwpq6tx_0
2019-02-13 00:33:48,848 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:33:50,755 : Computing embeddings for train/dev/test
2019-02-13 00:36:24,482 : Computed embeddings
2019-02-13 00:36:24,482 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:37:47,469 : [('reg:1e-05', 66.27), ('reg:0.0001', 66.19), ('reg:0.001', 62.86), ('reg:0.01', 57.53)]
2019-02-13 00:37:47,469 : Validation : best param found is reg = 1e-05 with score             66.27
2019-02-13 00:37:47,469 : Evaluating...
2019-02-13 00:38:15,922 : 
Dev acc : 66.3 Test acc : 64.8 for COORDINATIONINVERSION classification

2019-02-13 00:38:15,931 : {'STS12': {'MSRpar': {'pearson': (0.22644315917116492, 3.5480762836370453e-10), 'spearman': SpearmanrResult(correlation=0.27581296444987563, pvalue=1.4654842594850125e-14), 'nsamples': 750}, 'MSRvid': {'pearson': (0.1589473440802136, 1.2225639644218391e-05), 'spearman': SpearmanrResult(correlation=0.1728871942556177, pvalue=1.909985864879762e-06), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4183465670228753, 7.143612428553414e-21), 'spearman': SpearmanrResult(correlation=0.5189394895707813, pvalue=5.2327305455121144e-33), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3270301022648735, 3.743922696161727e-20), 'spearman': SpearmanrResult(correlation=0.32219457352015296, pvalue=1.4112917025617514e-19), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5911874617434961, 5.7251373853490225e-39), 'spearman': SpearmanrResult(correlation=0.4420374383198061, pvalue=1.6232538320249476e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3443909268565247, 'wmean': 0.30959469936819295}, 'spearman': {'mean': 0.34637433202324674, 'wmean': 0.3194135176228526}}}, 'STS13': {'FNWN': {'pearson': (0.09604012901354853, 0.1886392033239116), 'spearman': SpearmanrResult(correlation=0.12430979654199671, pvalue=0.08833315252453024), 'nsamples': 189}, 'headlines': {'pearson': (0.4523842573878113, 4.126706886004628e-39), 'spearman': SpearmanrResult(correlation=0.4445833042451607, pvalue=1.111169606462235e-37), 'nsamples': 750}, 'OnWN': {'pearson': (0.18981638732686118, 5.980497933467232e-06), 'spearman': SpearmanrResult(correlation=0.19396663787131116, pvalue=3.692828496610767e-06), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.24608025790940702, 'wmean': 0.30928451380985883}, 'spearman': {'mean': 0.25428657955282286, 'wmean': 0.3104982090507423}}}, 'STS14': {'deft-forum': {'pearson': (-0.0012185609080442483, 0.979434653816075), 'spearman': SpearmanrResult(correlation=0.006950917783066193, pvalue=0.883098160492497), 'nsamples': 450}, 'deft-news': {'pearson': (0.45206906923250445, 1.6219908196902618e-16), 'spearman': SpearmanrResult(correlation=0.49362929403146005, pvalue=7.846594189226638e-20), 'nsamples': 300}, 'headlines': {'pearson': (0.4028429891878525, 1.2452935043936456e-30), 'spearman': SpearmanrResult(correlation=0.37957967156308564, pvalue=4.092945164089396e-27), 'nsamples': 750}, 'images': {'pearson': (0.23719583125622956, 4.7312610827836906e-11), 'spearman': SpearmanrResult(correlation=0.24974986548316558, pvalue=3.962279770221814e-12), 'nsamples': 750}, 'OnWN': {'pearson': (0.33576753059717507, 3.2020611293876683e-21), 'spearman': SpearmanrResult(correlation=0.34049106342909974, pvalue=8.197335631888469e-22), 'nsamples': 750}, 'tweet-news': {'pearson': (0.39874764747316394, 5.431230105697154e-30), 'spearman': SpearmanrResult(correlation=0.3799073004941535, pvalue=3.668099186916723e-27), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.30423408447314687, 'wmean': 0.31093009793251924}, 'spearman': {'mean': 0.30838468546400516, 'wmean': 0.31027003385038565}}}, 'STS15': {'answers-forums': {'pearson': (0.21371064904459963, 3.0063921097385346e-05), 'spearman': SpearmanrResult(correlation=0.22751096076365193, pvalue=8.606733234190372e-06), 'nsamples': 375}, 'answers-students': {'pearson': (0.47647007399030844, 9.099191628638345e-44), 'spearman': SpearmanrResult(correlation=0.48381587845964097, pvalue=2.90636711688961e-45), 'nsamples': 750}, 'belief': {'pearson': (0.3384199633606691, 1.6822963500143915e-11), 'spearman': SpearmanrResult(correlation=0.4114679522827258, pvalue=9.369631103133765e-17), 'nsamples': 375}, 'headlines': {'pearson': (0.4803568992561015, 1.486442265486683e-44), 'spearman': SpearmanrResult(correlation=0.48338073045470814, pvalue=3.5724439238729087e-45), 'nsamples': 750}, 'images': {'pearson': (0.11131475857455615, 0.0022665048250826306), 'spearman': SpearmanrResult(correlation=0.15424074058129297, pvalue=2.2111906936073685e-05), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.324054468845247, 'wmean': 0.3360517595059001}, 'spearman': {'mean': 0.352083252508404, 'wmean': 0.3602317015047078}}}, 'STS16': {'answer-answer': {'pearson': (0.37748665111646185, 5.033922181274851e-10), 'spearman': SpearmanrResult(correlation=0.42703845188957035, pvalue=1.110939834219107e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.5445329104006258, 1.2617493103773368e-20), 'spearman': SpearmanrResult(correlation=0.5538113945152924, pvalue=2.0471728473334675e-21), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6028783551877503, 3.758114259132284e-24), 'spearman': SpearmanrResult(correlation=0.6481181277110284, pvalue=8.593878525428387e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.5882946343439598, 4.114500728710288e-24), 'spearman': SpearmanrResult(correlation=0.7422463101674335, pvalue=5.856585065457803e-44), 'nsamples': 244}, 'question-question': {'pearson': (0.1718040766589299, 0.012869167475419817), 'spearman': SpearmanrResult(correlation=0.2521416813114171, pvalue=0.0002305110624044935), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.45699932554154554, 'wmean': 0.4633923006476916}, 'spearman': {'mean': 0.5246711931189483, 'wmean': 0.5305562263598104}}}, 'MR': {'devacc': 71.37, 'acc': 70.79, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.35, 'acc': 70.57, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.03, 'acc': 86.19, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.84, 'acc': 93.85, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.36, 'acc': 77.81, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.42, 'acc': 41.49, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 75.97, 'acc': 86.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.39, 'acc': 71.88, 'f1': 81.17, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.2, 'acc': 72.17, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7423340884408229, 'pearson': 0.7209892196358876, 'spearman': 0.6576542740719158, 'mse': 0.4989185742570675, 'yhat': array([2.5462301 , 3.8804357 , 2.45749664, ..., 2.97158585, 4.59071346,
       4.78771255]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5317967869206648, 'pearson': 0.5121652098439453, 'spearman': 0.5074582556323265, 'mse': 1.817709283474579, 'yhat': array([2.51224466, 2.04294173, 3.26640688, ..., 3.7063257 , 3.69458564,
       3.51113953]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 74.33, 'acc': 75.28, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 15.08, 'acc': 15.06, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.87, 'acc': 29.32, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.41, 'acc': 73.05, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 81.45, 'acc': 80.62, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.4, 'acc': 89.0, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.45, 'acc': 88.42, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.95, 'acc': 78.38, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 56.8, 'acc': 56.69, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 66.27, 'acc': 64.81, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 00:38:15,931 : ********************************************************************************
2019-02-13 00:38:15,931 : ********************************************************************************
2019-02-13 00:38:15,931 : ********************************************************************************
2019-02-13 00:38:15,931 : layer 7
2019-02-13 00:38:15,931 : ********************************************************************************
2019-02-13 00:38:15,931 : ********************************************************************************
2019-02-13 00:38:15,932 : ********************************************************************************
2019-02-13 00:38:16,016 : ***** Transfer task : STS12 *****


2019-02-13 00:38:16,028 : loading BERT mode bert-base-uncased
2019-02-13 00:38:16,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:38:16,046 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:38:16,046 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp_snm0st
2019-02-13 00:38:18,509 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:38:22,784 : MSRpar : pearson = 0.2264, spearman = 0.2787
2019-02-13 00:38:24,906 : MSRvid : pearson = 0.0820, spearman = 0.1063
2019-02-13 00:38:25,830 : SMTeuroparl : pearson = 0.4300, spearman = 0.5277
2019-02-13 00:38:27,441 : surprise.OnWN : pearson = 0.2695, spearman = 0.2685
2019-02-13 00:38:28,334 : surprise.SMTnews : pearson = 0.5645, spearman = 0.4436
2019-02-13 00:38:28,334 : ALL (weighted average) : Pearson = 0.2754,             Spearman = 0.2926
2019-02-13 00:38:28,334 : ALL (average) : Pearson = 0.3145,             Spearman = 0.3250

2019-02-13 00:38:28,334 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 00:38:28,342 : loading BERT mode bert-base-uncased
2019-02-13 00:38:28,342 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:38:28,360 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:38:28,360 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9loksi32
2019-02-13 00:38:30,818 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:38:33,016 : FNWN : pearson = 0.1197, spearman = 0.1409
2019-02-13 00:38:34,342 : headlines : pearson = 0.3918, spearman = 0.3828
2019-02-13 00:38:35,335 : OnWN : pearson = 0.0238, spearman = 0.0344
2019-02-13 00:38:35,335 : ALL (weighted average) : Pearson = 0.2199,             Spearman = 0.2220
2019-02-13 00:38:35,335 : ALL (average) : Pearson = 0.1784,             Spearman = 0.1860

2019-02-13 00:38:35,335 : ***** Transfer task : STS14 *****


2019-02-13 00:38:35,354 : loading BERT mode bert-base-uncased
2019-02-13 00:38:35,354 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:38:35,372 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:38:35,372 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcm61rawt
2019-02-13 00:38:37,834 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:38:40,172 : deft-forum : pearson = -0.0438, spearman = -0.0276
2019-02-13 00:38:41,067 : deft-news : pearson = 0.3997, spearman = 0.4317
2019-02-13 00:38:42,487 : headlines : pearson = 0.3830, spearman = 0.3607
2019-02-13 00:38:43,868 : images : pearson = 0.1850, spearman = 0.1941
2019-02-13 00:38:45,268 : OnWN : pearson = 0.2458, spearman = 0.2456
2019-02-13 00:38:46,963 : tweet-news : pearson = 0.4331, spearman = 0.3943
2019-02-13 00:38:46,964 : ALL (weighted average) : Pearson = 0.2761,             Spearman = 0.2702
2019-02-13 00:38:46,964 : ALL (average) : Pearson = 0.2671,             Spearman = 0.2665

2019-02-13 00:38:46,964 : ***** Transfer task : STS15 *****


2019-02-13 00:38:46,996 : loading BERT mode bert-base-uncased
2019-02-13 00:38:46,996 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:38:47,014 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:38:47,014 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps7r3q3s_
2019-02-13 00:38:49,472 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:38:52,048 : answers-forums : pearson = 0.2572, spearman = 0.2761
2019-02-13 00:38:53,426 : answers-students : pearson = 0.3487, spearman = 0.3548
2019-02-13 00:38:54,522 : belief : pearson = 0.3762, spearman = 0.4458
2019-02-13 00:38:55,762 : headlines : pearson = 0.4313, spearman = 0.4405
2019-02-13 00:38:56,943 : images : pearson = 0.0940, spearman = 0.1312
2019-02-13 00:38:56,943 : ALL (weighted average) : Pearson = 0.2977,             Spearman = 0.3218
2019-02-13 00:38:56,943 : ALL (average) : Pearson = 0.3015,             Spearman = 0.3297

2019-02-13 00:38:56,943 : ***** Transfer task : STS16 *****


2019-02-13 00:38:57,015 : loading BERT mode bert-base-uncased
2019-02-13 00:38:57,015 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:38:57,033 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:38:57,033 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy0fe9ssj
2019-02-13 00:38:59,477 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:39:01,521 : answer-answer : pearson = 0.3658, spearman = 0.3851
2019-02-13 00:39:02,096 : headlines : pearson = 0.5119, spearman = 0.5180
2019-02-13 00:39:02,680 : plagiarism : pearson = 0.5702, spearman = 0.6335
2019-02-13 00:39:03,579 : postediting : pearson = 0.6437, spearman = 0.7545
2019-02-13 00:39:04,091 : question-question : pearson = 0.0857, spearman = 0.1811
2019-02-13 00:39:04,092 : ALL (weighted average) : Pearson = 0.4439,             Spearman = 0.5012
2019-02-13 00:39:04,092 : ALL (average) : Pearson = 0.4354,             Spearman = 0.4944

2019-02-13 00:39:04,092 : ***** Transfer task : MR *****


2019-02-13 00:39:04,109 : loading BERT mode bert-base-uncased
2019-02-13 00:39:04,109 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:39:04,130 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:39:04,130 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr8ku0ddz
2019-02-13 00:39:06,605 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:39:08,106 : Generating sentence embeddings
2019-02-13 00:39:27,339 : Generated sentence embeddings
2019-02-13 00:39:27,339 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:40:01,555 : Best param found at split 1: l2reg = 0.001                 with score 74.18
2019-02-13 00:40:43,001 : Best param found at split 2: l2reg = 0.01                 with score 73.54
2019-02-13 00:41:22,812 : Best param found at split 3: l2reg = 0.001                 with score 72.02
2019-02-13 00:41:58,912 : Best param found at split 4: l2reg = 1e-05                 with score 73.93
2019-02-13 00:42:22,666 : Best param found at split 5: l2reg = 1e-05                 with score 71.49
2019-02-13 00:42:24,370 : Dev acc : 73.03 Test acc : 73.03

2019-02-13 00:42:24,371 : ***** Transfer task : CR *****


2019-02-13 00:42:24,379 : loading BERT mode bert-base-uncased
2019-02-13 00:42:24,379 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:42:24,399 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:42:24,399 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmo21me59
2019-02-13 00:42:26,844 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:42:28,256 : Generating sentence embeddings
2019-02-13 00:42:33,074 : Generated sentence embeddings
2019-02-13 00:42:33,074 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:42:37,496 : Best param found at split 1: l2reg = 1e-05                 with score 75.82
2019-02-13 00:42:42,007 : Best param found at split 2: l2reg = 0.0001                 with score 76.88
2019-02-13 00:42:46,266 : Best param found at split 3: l2reg = 0.0001                 with score 77.15
2019-02-13 00:42:52,314 : Best param found at split 4: l2reg = 0.0001                 with score 76.07
2019-02-13 00:42:58,072 : Best param found at split 5: l2reg = 0.001                 with score 77.89
2019-02-13 00:42:58,446 : Dev acc : 76.76 Test acc : 75.74

2019-02-13 00:42:58,446 : ***** Transfer task : MPQA *****


2019-02-13 00:42:58,451 : loading BERT mode bert-base-uncased
2019-02-13 00:42:58,451 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:42:58,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:42:58,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp28kqkgjq
2019-02-13 00:43:00,928 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:43:02,436 : Generating sentence embeddings
2019-02-13 00:43:11,715 : Generated sentence embeddings
2019-02-13 00:43:11,716 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:43:37,715 : Best param found at split 1: l2reg = 0.001                 with score 86.02
2019-02-13 00:44:16,303 : Best param found at split 2: l2reg = 1e-05                 with score 85.53
2019-02-13 00:45:02,301 : Best param found at split 3: l2reg = 0.0001                 with score 85.41
2019-02-13 00:45:38,574 : Best param found at split 4: l2reg = 0.0001                 with score 86.0
2019-02-13 00:46:08,427 : Best param found at split 5: l2reg = 0.0001                 with score 84.6
2019-02-13 00:46:10,328 : Dev acc : 85.51 Test acc : 86.0

2019-02-13 00:46:10,329 : ***** Transfer task : SUBJ *****


2019-02-13 00:46:10,343 : loading BERT mode bert-base-uncased
2019-02-13 00:46:10,343 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:46:10,365 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:46:10,365 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpno7qtpcd
2019-02-13 00:46:12,810 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:46:14,354 : Generating sentence embeddings
2019-02-13 00:46:31,969 : Generated sentence embeddings
2019-02-13 00:46:31,969 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:46:48,762 : Best param found at split 1: l2reg = 1e-05                 with score 94.26
2019-02-13 00:47:06,692 : Best param found at split 2: l2reg = 0.0001                 with score 94.21
2019-02-13 00:47:32,994 : Best param found at split 3: l2reg = 1e-05                 with score 94.04
2019-02-13 00:47:55,638 : Best param found at split 4: l2reg = 0.0001                 with score 94.62
2019-02-13 00:48:35,799 : Best param found at split 5: l2reg = 0.0001                 with score 94.25
2019-02-13 00:48:38,295 : Dev acc : 94.28 Test acc : 94.35

2019-02-13 00:48:38,296 : ***** Transfer task : SST Binary classification *****


2019-02-13 00:48:38,424 : loading BERT mode bert-base-uncased
2019-02-13 00:48:38,424 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:48:38,449 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:48:38,449 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx4djj6zy
2019-02-13 00:48:40,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:48:42,497 : Computing embedding for train
2019-02-13 00:50:35,675 : Computed train embeddings
2019-02-13 00:50:35,675 : Computing embedding for dev
2019-02-13 00:50:37,649 : Computed dev embeddings
2019-02-13 00:50:37,649 : Computing embedding for test
2019-02-13 00:50:40,580 : Computed test embeddings
2019-02-13 00:50:40,580 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:51:09,707 : [('reg:1e-05', 77.64), ('reg:0.0001', 77.52), ('reg:0.001', 78.1), ('reg:0.01', 77.06)]
2019-02-13 00:51:09,707 : Validation : best param found is reg = 0.001 with score             78.1
2019-02-13 00:51:09,707 : Evaluating...
2019-02-13 00:51:15,726 : 
Dev acc : 78.1 Test acc : 78.14 for             SST Binary classification

2019-02-13 00:51:15,731 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 00:51:15,781 : loading BERT mode bert-base-uncased
2019-02-13 00:51:15,781 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:51:15,802 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:51:15,803 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpehi255jm
2019-02-13 00:51:18,239 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:51:19,700 : Computing embedding for train
2019-02-13 00:51:33,541 : Computed train embeddings
2019-02-13 00:51:33,541 : Computing embedding for dev
2019-02-13 00:51:35,322 : Computed dev embeddings
2019-02-13 00:51:35,323 : Computing embedding for test
2019-02-13 00:51:38,987 : Computed test embeddings
2019-02-13 00:51:38,987 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:51:44,145 : [('reg:1e-05', 40.87), ('reg:0.0001', 40.87), ('reg:0.001', 35.42), ('reg:0.01', 38.6)]
2019-02-13 00:51:44,145 : Validation : best param found is reg = 1e-05 with score             40.87
2019-02-13 00:51:44,145 : Evaluating...
2019-02-13 00:51:45,522 : 
Dev acc : 40.87 Test acc : 42.76 for             SST Fine-Grained classification

2019-02-13 00:51:45,522 : ***** Transfer task : TREC *****


2019-02-13 00:51:45,536 : loading BERT mode bert-base-uncased
2019-02-13 00:51:45,536 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:51:45,554 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:51:45,555 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpunf7qvc5
2019-02-13 00:51:48,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:51:56,581 : Computed train embeddings
2019-02-13 00:51:57,193 : Computed test embeddings
2019-02-13 00:51:57,193 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 00:52:14,677 : [('reg:1e-05', 78.85), ('reg:0.0001', 79.44), ('reg:0.001', 79.36), ('reg:0.01', 67.43)]
2019-02-13 00:52:14,677 : Cross-validation : best param found is reg = 0.0001             with score 79.44
2019-02-13 00:52:14,677 : Evaluating...
2019-02-13 00:52:15,986 : 
Dev acc : 79.44 Test acc : 86.6             for TREC

2019-02-13 00:52:15,987 : ***** Transfer task : MRPC *****


2019-02-13 00:52:16,008 : loading BERT mode bert-base-uncased
2019-02-13 00:52:16,009 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:52:16,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:52:16,029 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp51p9cncz
2019-02-13 00:52:18,461 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:52:19,970 : Computing embedding for train
2019-02-13 00:52:35,317 : Computed train embeddings
2019-02-13 00:52:35,317 : Computing embedding for test
2019-02-13 00:52:43,043 : Computed test embeddings
2019-02-13 00:52:43,059 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 00:53:00,124 : [('reg:1e-05', 70.49), ('reg:0.0001', 70.14), ('reg:0.001', 70.24), ('reg:0.01', 69.04)]
2019-02-13 00:53:00,124 : Cross-validation : best param found is reg = 1e-05             with score 70.49
2019-02-13 00:53:00,124 : Evaluating...
2019-02-13 00:53:01,252 : Dev acc : 70.49 Test acc 68.06; Test F1 80.54 for MRPC.

2019-02-13 00:53:01,253 : ***** Transfer task : SICK-Entailment*****


2019-02-13 00:53:01,317 : loading BERT mode bert-base-uncased
2019-02-13 00:53:01,317 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:53:01,337 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:53:01,337 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6nu3o7l3
2019-02-13 00:53:03,768 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:53:05,296 : Computing embedding for train
2019-02-13 00:53:19,935 : Computed train embeddings
2019-02-13 00:53:19,935 : Computing embedding for dev
2019-02-13 00:53:21,338 : Computed dev embeddings
2019-02-13 00:53:21,338 : Computing embedding for test
2019-02-13 00:53:34,172 : Computed test embeddings
2019-02-13 00:53:34,199 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:53:37,688 : [('reg:1e-05', 73.8), ('reg:0.0001', 68.8), ('reg:0.001', 73.8), ('reg:0.01', 71.2)]
2019-02-13 00:53:37,689 : Validation : best param found is reg = 1e-05 with score             73.8
2019-02-13 00:53:37,689 : Evaluating...
2019-02-13 00:53:38,510 : 
Dev acc : 73.8 Test acc : 71.48 for                        SICK entailment

2019-02-13 00:53:38,511 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 00:53:38,538 : loading BERT mode bert-base-uncased
2019-02-13 00:53:38,538 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:53:38,594 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:53:38,594 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprkvybpep
2019-02-13 00:53:41,033 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:53:42,495 : Computing embedding for train
2019-02-13 00:53:54,565 : Computed train embeddings
2019-02-13 00:53:54,565 : Computing embedding for dev
2019-02-13 00:53:56,051 : Computed dev embeddings
2019-02-13 00:53:56,051 : Computing embedding for test
2019-02-13 00:54:09,917 : Computed test embeddings
2019-02-13 00:55:03,158 : Dev : Pearson 0.6866385385093505
2019-02-13 00:55:03,158 : Test : Pearson 0.6971499229597029 Spearman 0.6422100419948958 MSE 0.5262799125307982                        for SICK Relatedness

2019-02-13 00:55:03,159 : 

***** Transfer task : STSBenchmark*****


2019-02-13 00:55:03,231 : loading BERT mode bert-base-uncased
2019-02-13 00:55:03,231 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:55:03,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:55:03,251 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjggqzwhg
2019-02-13 00:55:05,681 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:55:07,272 : Computing embedding for train
2019-02-13 00:55:24,851 : Computed train embeddings
2019-02-13 00:55:24,851 : Computing embedding for dev
2019-02-13 00:55:29,048 : Computed dev embeddings
2019-02-13 00:55:29,049 : Computing embedding for test
2019-02-13 00:55:32,739 : Computed test embeddings
2019-02-13 00:56:34,182 : Dev : Pearson 0.48173077157097627
2019-02-13 00:56:34,182 : Test : Pearson 0.46557109337882074 Spearman 0.4628866686484039 MSE 1.9249203571621725                        for SICK Relatedness

2019-02-13 00:56:34,182 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 00:56:34,501 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 00:56:34,511 : loading BERT mode bert-base-uncased
2019-02-13 00:56:34,511 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:56:34,534 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:56:34,534 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj1xw04dp
2019-02-13 00:56:36,968 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:56:38,461 : Computing embeddings for train/dev/test
2019-02-13 00:59:07,692 : Computed embeddings
2019-02-13 00:59:07,692 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:00:31,713 : [('reg:1e-05', 67.55), ('reg:0.0001', 67.85), ('reg:0.001', 57.27), ('reg:0.01', 45.01)]
2019-02-13 01:00:31,713 : Validation : best param found is reg = 0.0001 with score             67.85
2019-02-13 01:00:31,713 : Evaluating...
2019-02-13 01:00:50,371 : 
Dev acc : 67.8 Test acc : 68.9 for LENGTH classification

2019-02-13 01:00:50,378 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 01:00:50,722 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 01:00:50,767 : loading BERT mode bert-base-uncased
2019-02-13 01:00:50,767 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:00:50,796 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:00:50,796 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmploq6afnr
2019-02-13 01:00:53,233 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:00:54,688 : Computing embeddings for train/dev/test
2019-02-13 01:03:05,278 : Computed embeddings
2019-02-13 01:03:05,279 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:04:42,550 : [('reg:1e-05', 23.72), ('reg:0.0001', 5.3), ('reg:0.001', 0.94), ('reg:0.01', 0.47)]
2019-02-13 01:04:42,550 : Validation : best param found is reg = 1e-05 with score             23.72
2019-02-13 01:04:42,550 : Evaluating...
2019-02-13 01:05:01,439 : 
Dev acc : 23.7 Test acc : 23.0 for WORDCONTENT classification

2019-02-13 01:05:01,447 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 01:05:01,822 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 01:05:01,894 : loading BERT mode bert-base-uncased
2019-02-13 01:05:01,894 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:05:01,994 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:05:01,994 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiu6vf1v9
2019-02-13 01:05:04,454 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:05:05,887 : Computing embeddings for train/dev/test
2019-02-13 01:07:28,824 : Computed embeddings
2019-02-13 01:07:28,824 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:08:34,195 : [('reg:1e-05', 29.11), ('reg:0.0001', 29.13), ('reg:0.001', 27.82), ('reg:0.01', 24.04)]
2019-02-13 01:08:34,195 : Validation : best param found is reg = 0.0001 with score             29.13
2019-02-13 01:08:34,195 : Evaluating...
2019-02-13 01:08:43,905 : 
Dev acc : 29.1 Test acc : 30.0 for DEPTH classification

2019-02-13 01:08:43,912 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 01:08:44,514 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 01:08:44,581 : loading BERT mode bert-base-uncased
2019-02-13 01:08:44,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:08:44,612 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:08:44,612 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpibkfbjkd
2019-02-13 01:08:47,063 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:08:48,522 : Computing embeddings for train/dev/test
2019-02-13 01:11:25,793 : Computed embeddings
2019-02-13 01:11:25,793 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:12:55,311 : [('reg:1e-05', 76.11), ('reg:0.0001', 75.39), ('reg:0.001', 70.53), ('reg:0.01', 58.87)]
2019-02-13 01:12:55,312 : Validation : best param found is reg = 1e-05 with score             76.11
2019-02-13 01:12:55,312 : Evaluating...
2019-02-13 01:13:13,410 : 
Dev acc : 76.1 Test acc : 76.7 for TOPCONSTITUENTS classification

2019-02-13 01:13:13,418 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 01:13:13,770 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 01:13:13,838 : loading BERT mode bert-base-uncased
2019-02-13 01:13:13,838 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:13:13,964 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:13:13,964 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9t3muaie
2019-02-13 01:13:16,406 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:13:17,871 : Computing embeddings for train/dev/test
2019-02-13 01:15:37,998 : Computed embeddings
2019-02-13 01:15:37,998 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:16:57,128 : [('reg:1e-05', 81.91), ('reg:0.0001', 81.91), ('reg:0.001', 81.23), ('reg:0.01', 79.04)]
2019-02-13 01:16:57,128 : Validation : best param found is reg = 1e-05 with score             81.91
2019-02-13 01:16:57,129 : Evaluating...
2019-02-13 01:17:20,889 : 
Dev acc : 81.9 Test acc : 81.2 for BIGRAMSHIFT classification

2019-02-13 01:17:20,896 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 01:17:21,466 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 01:17:21,533 : loading BERT mode bert-base-uncased
2019-02-13 01:17:21,533 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:17:21,564 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:17:21,564 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu36hyxmz
2019-02-13 01:17:24,007 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:17:25,576 : Computing embeddings for train/dev/test
2019-02-13 01:20:13,728 : Computed embeddings
2019-02-13 01:20:13,728 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:21:11,292 : [('reg:1e-05', 89.84), ('reg:0.0001', 89.68), ('reg:0.001', 90.4), ('reg:0.01', 89.74)]
2019-02-13 01:21:11,292 : Validation : best param found is reg = 0.001 with score             90.4
2019-02-13 01:21:11,292 : Evaluating...
2019-02-13 01:21:26,800 : 
Dev acc : 90.4 Test acc : 89.3 for TENSE classification

2019-02-13 01:21:26,808 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 01:21:27,381 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 01:21:27,444 : loading BERT mode bert-base-uncased
2019-02-13 01:21:27,444 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:21:27,472 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:21:27,473 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp928lpzru
2019-02-13 01:21:29,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:21:31,367 : Computing embeddings for train/dev/test
2019-02-13 01:24:18,023 : Computed embeddings
2019-02-13 01:24:18,023 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:24:55,806 : [('reg:1e-05', 86.94), ('reg:0.0001', 87.01), ('reg:0.001', 86.69), ('reg:0.01', 85.53)]
2019-02-13 01:24:55,806 : Validation : best param found is reg = 0.0001 with score             87.01
2019-02-13 01:24:55,806 : Evaluating...
2019-02-13 01:25:03,235 : 
Dev acc : 87.0 Test acc : 87.6 for SUBJNUMBER classification

2019-02-13 01:25:03,243 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 01:25:03,671 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 01:25:03,738 : loading BERT mode bert-base-uncased
2019-02-13 01:25:03,738 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:25:03,765 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:25:03,765 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa38g626w
2019-02-13 01:25:06,206 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:25:07,674 : Computing embeddings for train/dev/test
2019-02-13 01:28:00,118 : Computed embeddings
2019-02-13 01:28:00,118 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:28:47,496 : [('reg:1e-05', 77.52), ('reg:0.0001', 77.57), ('reg:0.001', 77.93), ('reg:0.01', 73.52)]
2019-02-13 01:28:47,496 : Validation : best param found is reg = 0.001 with score             77.93
2019-02-13 01:28:47,497 : Evaluating...
2019-02-13 01:29:05,483 : 
Dev acc : 77.9 Test acc : 79.5 for OBJNUMBER classification

2019-02-13 01:29:05,491 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 01:29:05,886 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 01:29:05,954 : loading BERT mode bert-base-uncased
2019-02-13 01:29:05,955 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:29:06,079 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:29:06,079 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmhrhhsts
2019-02-13 01:29:08,518 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:29:10,039 : Computing embeddings for train/dev/test
2019-02-13 01:32:32,743 : Computed embeddings
2019-02-13 01:32:32,743 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:33:08,369 : [('reg:1e-05', 57.3), ('reg:0.0001', 57.3), ('reg:0.001', 57.12), ('reg:0.01', 56.23)]
2019-02-13 01:33:08,369 : Validation : best param found is reg = 1e-05 with score             57.3
2019-02-13 01:33:08,369 : Evaluating...
2019-02-13 01:33:18,389 : 
Dev acc : 57.3 Test acc : 57.6 for ODDMANOUT classification

2019-02-13 01:33:18,396 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 01:33:18,803 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 01:33:18,882 : loading BERT mode bert-base-uncased
2019-02-13 01:33:18,882 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:33:19,012 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:33:19,012 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp72hfg2ak
2019-02-13 01:33:21,453 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:33:22,869 : Computing embeddings for train/dev/test
2019-02-13 01:36:32,334 : Computed embeddings
2019-02-13 01:36:32,334 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:37:27,170 : [('reg:1e-05', 70.07), ('reg:0.0001', 70.05), ('reg:0.001', 69.78), ('reg:0.01', 66.69)]
2019-02-13 01:37:27,170 : Validation : best param found is reg = 1e-05 with score             70.07
2019-02-13 01:37:27,170 : Evaluating...
2019-02-13 01:37:47,681 : 
Dev acc : 70.1 Test acc : 68.6 for COORDINATIONINVERSION classification

2019-02-13 01:37:47,689 : {'STS12': {'MSRpar': {'pearson': (0.22640027039615485, 3.5759914582100204e-10), 'spearman': SpearmanrResult(correlation=0.2787156745428831, pvalue=7.557193941787097e-15), 'nsamples': 750}, 'MSRvid': {'pearson': (0.08201070756469313, 0.02470439543363753), 'spearman': SpearmanrResult(correlation=0.10630907637329233, pvalue=0.0035591467996128195), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4300404691811492, 4.381388110282282e-22), 'spearman': SpearmanrResult(correlation=0.5276565263889252, pvalue=2.915989012612447e-34), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.2694738556275138, 6.057379302429383e-14), 'spearman': SpearmanrResult(correlation=0.2685286736418725, pvalue=7.461001433682861e-14), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5644637630942212, 5.973290003631731e-35), 'spearman': SpearmanrResult(correlation=0.44363999471361804, pvalue=1.139320118169869e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3144778131727464, 'wmean': 0.27542575354569276}, 'spearman': {'mean': 0.3249699891321182, 'wmean': 0.29259066020649493}}}, 'STS13': {'FNWN': {'pearson': (0.11970576959920125, 0.10086600065952624), 'spearman': SpearmanrResult(correlation=0.14087564562642999, pvalue=0.053171513685932204), 'nsamples': 189}, 'headlines': {'pearson': (0.39178474942274394, 6.339388858942566e-29), 'spearman': SpearmanrResult(correlation=0.38279641409583726, pvalue=1.3879914130863963e-27), 'nsamples': 750}, 'OnWN': {'pearson': (0.0237942994462884, 0.5738428299795575), 'spearman': SpearmanrResult(correlation=0.034403766144772724, pvalue=0.41605092592363746), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.17842827282274454, 'wmean': 0.2198743696737832}, 'spearman': {'mean': 0.18602527528901333, 'wmean': 0.22201554693499378}}}, 'STS14': {'deft-forum': {'pearson': (-0.043812419629510845, 0.35379046640623246), 'spearman': SpearmanrResult(correlation=-0.027594521456751325, pvalue=0.559320372620908), 'nsamples': 450}, 'deft-news': {'pearson': (0.3996602267280921, 6.22311942731534e-13), 'spearman': SpearmanrResult(correlation=0.4317471009117585, pvalue=4.719770747266119e-15), 'nsamples': 300}, 'headlines': {'pearson': (0.3830414253544331, 1.277613975492409e-27), 'spearman': SpearmanrResult(correlation=0.36068517910047804, pvalue=1.847625502838219e-24), 'nsamples': 750}, 'images': {'pearson': (0.18503483116784253, 3.341974995764883e-07), 'spearman': SpearmanrResult(correlation=0.19409818927508896, pvalue=8.425744908582733e-08), 'nsamples': 750}, 'OnWN': {'pearson': (0.24583473892736435, 8.715757841315688e-12), 'spearman': SpearmanrResult(correlation=0.24560335047914267, pvalue=9.127555925816214e-12), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4330671251420096, 1.2312119500105807e-35), 'spearman': SpearmanrResult(correlation=0.3942585386682461, pvalue=2.665760983577104e-29), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2671376546150384, 'wmean': 0.276110951901036}, 'spearman': {'mean': 0.26646630616299377, 'wmean': 0.2701574770027217}}}, 'STS15': {'answers-forums': {'pearson': (0.2572124378645401, 4.4302525575503883e-07), 'spearman': SpearmanrResult(correlation=0.27612733461420474, pvalue=5.4684403927132705e-08), 'nsamples': 375}, 'answers-students': {'pearson': (0.34873437994248824, 7.182838666977683e-23), 'spearman': SpearmanrResult(correlation=0.35476428913867386, pvalue=1.1552874942537296e-23), 'nsamples': 750}, 'belief': {'pearson': (0.37615857441011763, 4.749753088003947e-14), 'spearman': SpearmanrResult(correlation=0.44578461663701086, pvalue=1.0411043328867419e-19), 'nsamples': 375}, 'headlines': {'pearson': (0.43133670498000176, 2.4592930497925e-35), 'spearman': SpearmanrResult(correlation=0.44051337945763597, pvalue=5.988678295773399e-37), 'nsamples': 750}, 'images': {'pearson': (0.09403549512372665, 0.009975301782076612), 'spearman': SpearmanrResult(correlation=0.13116563375830023, pvalue=0.0003161444792519731), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3014955184641749, 'wmean': 0.29769802154588637}, 'spearman': {'mean': 0.3296710507211651, 'wmean': 0.3218498194950544}}}, 'STS16': {'answer-answer': {'pearson': (0.36577856727399816, 1.851050060199976e-09), 'spearman': SpearmanrResult(correlation=0.38510218631668625, pvalue=2.0979316172554138e-10), 'nsamples': 254}, 'headlines': {'pearson': (0.5119087908138538, 4.925617910799115e-18), 'spearman': SpearmanrResult(correlation=0.5179913546506333, pvalue=1.6991253925464661e-18), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5701685836597956, 3.1370613451012896e-21), 'spearman': SpearmanrResult(correlation=0.63351982949809, pvalue=3.280206591987593e-27), 'nsamples': 230}, 'postediting': {'pearson': (0.6436571576573903, 6.132537588078914e-30), 'spearman': SpearmanrResult(correlation=0.7545230771407708, pvalue=3.673797839071777e-46), 'nsamples': 244}, 'question-question': {'pearson': (0.08569715166274253, 0.2172960498209676), 'spearman': SpearmanrResult(correlation=0.18107711429897685, pvalue=0.00869477491586174), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.4354420502135561, 'wmean': 0.44390798516687563}, 'spearman': {'mean': 0.49444271238103144, 'wmean': 0.5012260633455657}}}, 'MR': {'devacc': 73.03, 'acc': 73.03, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.76, 'acc': 75.74, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.51, 'acc': 86.0, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.28, 'acc': 94.35, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.1, 'acc': 78.14, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.87, 'acc': 42.76, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.44, 'acc': 86.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.49, 'acc': 68.06, 'f1': 80.54, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.8, 'acc': 71.48, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6866385385093505, 'pearson': 0.6971499229597029, 'spearman': 0.6422100419948958, 'mse': 0.5262799125307982, 'yhat': array([2.89347036, 4.02375374, 2.05523302, ..., 3.2186546 , 4.44047924,
       4.29090893]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.48173077157097627, 'pearson': 0.46557109337882074, 'spearman': 0.4628866686484039, 'mse': 1.9249203571621725, 'yhat': array([2.35286464, 2.04092713, 3.27200237, ..., 3.72951862, 3.6548544 ,
       3.75573437]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 67.85, 'acc': 68.86, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 23.72, 'acc': 22.98, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.13, 'acc': 30.03, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 76.11, 'acc': 76.73, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 81.91, 'acc': 81.17, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.4, 'acc': 89.28, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.01, 'acc': 87.57, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.93, 'acc': 79.55, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.3, 'acc': 57.59, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 70.07, 'acc': 68.64, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 01:37:47,689 : ********************************************************************************
2019-02-13 01:37:47,689 : ********************************************************************************
2019-02-13 01:37:47,689 : ********************************************************************************
2019-02-13 01:37:47,689 : layer 8
2019-02-13 01:37:47,689 : ********************************************************************************
2019-02-13 01:37:47,689 : ********************************************************************************
2019-02-13 01:37:47,690 : ********************************************************************************
2019-02-13 01:37:47,777 : ***** Transfer task : STS12 *****


2019-02-13 01:37:47,814 : loading BERT mode bert-base-uncased
2019-02-13 01:37:47,814 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:37:47,830 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:37:47,830 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptkrebvir
2019-02-13 01:37:50,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:37:54,444 : MSRpar : pearson = 0.2107, spearman = 0.2657
2019-02-13 01:37:56,378 : MSRvid : pearson = 0.0238, spearman = 0.0414
2019-02-13 01:37:57,767 : SMTeuroparl : pearson = 0.4292, spearman = 0.4966
2019-02-13 01:38:00,067 : surprise.OnWN : pearson = 0.3058, spearman = 0.2957
2019-02-13 01:38:01,339 : surprise.SMTnews : pearson = 0.5400, spearman = 0.4453
2019-02-13 01:38:01,340 : ALL (weighted average) : Pearson = 0.2631,             Spearman = 0.2760
2019-02-13 01:38:01,340 : ALL (average) : Pearson = 0.3019,             Spearman = 0.3089

2019-02-13 01:38:01,340 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 01:38:01,348 : loading BERT mode bert-base-uncased
2019-02-13 01:38:01,348 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:38:01,365 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:38:01,366 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppu2dudis
2019-02-13 01:38:03,803 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:38:06,130 : FNWN : pearson = 0.0855, spearman = 0.0996
2019-02-13 01:38:07,778 : headlines : pearson = 0.3958, spearman = 0.3857
2019-02-13 01:38:09,111 : OnWN : pearson = 0.0269, spearman = 0.0235
2019-02-13 01:38:09,111 : ALL (weighted average) : Pearson = 0.2187,             Spearman = 0.2142
2019-02-13 01:38:09,111 : ALL (average) : Pearson = 0.1694,             Spearman = 0.1696

2019-02-13 01:38:09,111 : ***** Transfer task : STS14 *****


2019-02-13 01:38:09,129 : loading BERT mode bert-base-uncased
2019-02-13 01:38:09,130 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:38:09,180 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:38:09,180 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7y87ynmt
2019-02-13 01:38:11,619 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:38:14,673 : deft-forum : pearson = -0.1157, spearman = -0.0979
2019-02-13 01:38:16,208 : deft-news : pearson = 0.3885, spearman = 0.4331
2019-02-13 01:38:18,632 : headlines : pearson = 0.3822, spearman = 0.3569
2019-02-13 01:38:21,132 : images : pearson = 0.1499, spearman = 0.1662
2019-02-13 01:38:23,706 : OnWN : pearson = 0.2438, spearman = 0.2446
2019-02-13 01:38:26,580 : tweet-news : pearson = 0.4445, spearman = 0.4074
2019-02-13 01:38:26,581 : ALL (weighted average) : Pearson = 0.2613,             Spearman = 0.2579
2019-02-13 01:38:26,581 : ALL (average) : Pearson = 0.2489,             Spearman = 0.2517

2019-02-13 01:38:26,581 : ***** Transfer task : STS15 *****


2019-02-13 01:38:26,612 : loading BERT mode bert-base-uncased
2019-02-13 01:38:26,612 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:38:26,630 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:38:26,630 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph0auxmvu
2019-02-13 01:38:29,097 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:38:32,388 : answers-forums : pearson = 0.2253, spearman = 0.2562
2019-02-13 01:38:35,042 : answers-students : pearson = 0.3696, spearman = 0.3774
2019-02-13 01:38:36,825 : belief : pearson = 0.3785, spearman = 0.4281
2019-02-13 01:38:39,586 : headlines : pearson = 0.4310, spearman = 0.4397
2019-02-13 01:38:42,489 : images : pearson = 0.1207, spearman = 0.1378
2019-02-13 01:38:42,490 : ALL (weighted average) : Pearson = 0.3058,             Spearman = 0.3242
2019-02-13 01:38:42,490 : ALL (average) : Pearson = 0.3050,             Spearman = 0.3278

2019-02-13 01:38:42,490 : ***** Transfer task : STS16 *****


2019-02-13 01:38:42,563 : loading BERT mode bert-base-uncased
2019-02-13 01:38:42,563 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:38:42,581 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:38:42,581 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_8805nes
2019-02-13 01:38:45,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:38:47,698 : answer-answer : pearson = 0.3336, spearman = 0.3398
2019-02-13 01:38:48,675 : headlines : pearson = 0.5388, spearman = 0.5526
2019-02-13 01:38:49,721 : plagiarism : pearson = 0.5215, spearman = 0.6059
2019-02-13 01:38:51,039 : postediting : pearson = 0.6820, spearman = 0.7569
2019-02-13 01:38:51,897 : question-question : pearson = 0.0258, spearman = 0.1001
2019-02-13 01:38:51,898 : ALL (weighted average) : Pearson = 0.4306,             Spearman = 0.4796
2019-02-13 01:38:51,898 : ALL (average) : Pearson = 0.4204,             Spearman = 0.4711

2019-02-13 01:38:51,898 : ***** Transfer task : MR *****


2019-02-13 01:38:51,916 : loading BERT mode bert-base-uncased
2019-02-13 01:38:51,916 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:38:51,936 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:38:51,936 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvhabl0kt
2019-02-13 01:38:54,376 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:38:55,983 : Generating sentence embeddings
2019-02-13 01:39:24,558 : Generated sentence embeddings
2019-02-13 01:39:24,558 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:40:02,966 : Best param found at split 1: l2reg = 0.0001                 with score 76.08
2019-02-13 01:40:25,485 : Best param found at split 2: l2reg = 0.01                 with score 73.63
2019-02-13 01:40:51,441 : Best param found at split 3: l2reg = 0.0001                 with score 74.67
2019-02-13 01:41:13,087 : Best param found at split 4: l2reg = 0.0001                 with score 74.89
2019-02-13 01:41:33,750 : Best param found at split 5: l2reg = 1e-05                 with score 74.95
2019-02-13 01:41:35,435 : Dev acc : 74.84 Test acc : 75.52

2019-02-13 01:41:35,436 : ***** Transfer task : CR *****


2019-02-13 01:41:35,443 : loading BERT mode bert-base-uncased
2019-02-13 01:41:35,444 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:41:35,464 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:41:35,464 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdv41dtvx
2019-02-13 01:41:37,906 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:41:39,381 : Generating sentence embeddings
2019-02-13 01:41:45,124 : Generated sentence embeddings
2019-02-13 01:41:45,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:41:54,409 : Best param found at split 1: l2reg = 0.001                 with score 79.56
2019-02-13 01:42:03,639 : Best param found at split 2: l2reg = 0.001                 with score 79.56
2019-02-13 01:42:15,179 : Best param found at split 3: l2reg = 0.0001                 with score 79.6
2019-02-13 01:42:27,802 : Best param found at split 4: l2reg = 0.0001                 with score 80.4
2019-02-13 01:42:39,691 : Best param found at split 5: l2reg = 0.0001                 with score 79.34
2019-02-13 01:42:40,458 : Dev acc : 79.69 Test acc : 75.18

2019-02-13 01:42:40,458 : ***** Transfer task : MPQA *****


2019-02-13 01:42:40,463 : loading BERT mode bert-base-uncased
2019-02-13 01:42:40,463 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:42:40,482 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:42:40,483 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqhwjhlsu
2019-02-13 01:42:42,917 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:42:44,559 : Generating sentence embeddings
2019-02-13 01:43:00,567 : Generated sentence embeddings
2019-02-13 01:43:00,568 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:43:37,122 : Best param found at split 1: l2reg = 0.001                 with score 86.83
2019-02-13 01:44:07,446 : Best param found at split 2: l2reg = 0.01                 with score 86.53
2019-02-13 01:44:40,985 : Best param found at split 3: l2reg = 1e-05                 with score 86.48
2019-02-13 01:45:00,183 : Best param found at split 4: l2reg = 0.01                 with score 87.06
2019-02-13 01:45:20,218 : Best param found at split 5: l2reg = 0.0001                 with score 86.51
2019-02-13 01:45:21,667 : Dev acc : 86.68 Test acc : 86.61

2019-02-13 01:45:21,668 : ***** Transfer task : SUBJ *****


2019-02-13 01:45:21,681 : loading BERT mode bert-base-uncased
2019-02-13 01:45:21,681 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:45:21,703 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:45:21,703 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp59jmp2df
2019-02-13 01:45:24,141 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:45:25,641 : Generating sentence embeddings
2019-02-13 01:45:44,533 : Generated sentence embeddings
2019-02-13 01:45:44,534 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:46:08,429 : Best param found at split 1: l2reg = 1e-05                 with score 94.48
2019-02-13 01:46:38,657 : Best param found at split 2: l2reg = 1e-05                 with score 94.75
2019-02-13 01:47:17,111 : Best param found at split 3: l2reg = 0.001                 with score 94.36
2019-02-13 01:47:46,285 : Best param found at split 4: l2reg = 0.001                 with score 94.96
2019-02-13 01:48:12,725 : Best param found at split 5: l2reg = 0.001                 with score 94.54
2019-02-13 01:48:14,440 : Dev acc : 94.62 Test acc : 93.8

2019-02-13 01:48:14,441 : ***** Transfer task : SST Binary classification *****


2019-02-13 01:48:14,573 : loading BERT mode bert-base-uncased
2019-02-13 01:48:14,573 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:48:14,596 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:48:14,596 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmponji7zsf
2019-02-13 01:48:17,039 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:48:18,529 : Computing embedding for train
2019-02-13 01:49:48,033 : Computed train embeddings
2019-02-13 01:49:48,033 : Computing embedding for dev
2019-02-13 01:49:49,434 : Computed dev embeddings
2019-02-13 01:49:49,434 : Computing embedding for test
2019-02-13 01:49:52,485 : Computed test embeddings
2019-02-13 01:49:52,485 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:50:31,470 : [('reg:1e-05', 80.96), ('reg:0.0001', 80.85), ('reg:0.001', 78.44), ('reg:0.01', 80.05)]
2019-02-13 01:50:31,470 : Validation : best param found is reg = 1e-05 with score             80.96
2019-02-13 01:50:31,470 : Evaluating...
2019-02-13 01:50:42,535 : 
Dev acc : 80.96 Test acc : 78.69 for             SST Binary classification

2019-02-13 01:50:42,540 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 01:50:42,589 : loading BERT mode bert-base-uncased
2019-02-13 01:50:42,589 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:50:42,611 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:50:42,611 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9_zqt38n
2019-02-13 01:50:45,045 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:50:46,528 : Computing embedding for train
2019-02-13 01:51:01,907 : Computed train embeddings
2019-02-13 01:51:01,908 : Computing embedding for dev
2019-02-13 01:51:03,819 : Computed dev embeddings
2019-02-13 01:51:03,819 : Computing embedding for test
2019-02-13 01:51:07,919 : Computed test embeddings
2019-02-13 01:51:07,919 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:51:12,414 : [('reg:1e-05', 36.06), ('reg:0.0001', 41.24), ('reg:0.001', 34.51), ('reg:0.01', 36.33)]
2019-02-13 01:51:12,414 : Validation : best param found is reg = 0.0001 with score             41.24
2019-02-13 01:51:12,414 : Evaluating...
2019-02-13 01:51:13,968 : 
Dev acc : 41.24 Test acc : 42.58 for             SST Fine-Grained classification

2019-02-13 01:51:13,968 : ***** Transfer task : TREC *****


2019-02-13 01:51:13,982 : loading BERT mode bert-base-uncased
2019-02-13 01:51:13,982 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:51:14,001 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:51:14,001 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpukfuioz9
2019-02-13 01:51:16,444 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:51:23,170 : Computed train embeddings
2019-02-13 01:51:23,696 : Computed test embeddings
2019-02-13 01:51:23,696 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 01:51:39,359 : [('reg:1e-05', 80.13), ('reg:0.0001', 80.09), ('reg:0.001', 76.72), ('reg:0.01', 72.66)]
2019-02-13 01:51:39,359 : Cross-validation : best param found is reg = 1e-05             with score 80.13
2019-02-13 01:51:39,359 : Evaluating...
2019-02-13 01:51:40,613 : 
Dev acc : 80.13 Test acc : 87.2             for TREC

2019-02-13 01:51:40,613 : ***** Transfer task : MRPC *****


2019-02-13 01:51:40,671 : loading BERT mode bert-base-uncased
2019-02-13 01:51:40,671 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:51:40,690 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:51:40,690 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1sugemxi
2019-02-13 01:51:43,129 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:51:44,601 : Computing embedding for train
2019-02-13 01:51:59,042 : Computed train embeddings
2019-02-13 01:51:59,042 : Computing embedding for test
2019-02-13 01:52:05,446 : Computed test embeddings
2019-02-13 01:52:05,461 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 01:52:17,998 : [('reg:1e-05', 70.46), ('reg:0.0001', 70.51), ('reg:0.001', 69.97), ('reg:0.01', 69.33)]
2019-02-13 01:52:17,998 : Cross-validation : best param found is reg = 0.0001             with score 70.51
2019-02-13 01:52:17,998 : Evaluating...
2019-02-13 01:52:18,489 : Dev acc : 70.51 Test acc 68.17; Test F1 76.65 for MRPC.

2019-02-13 01:52:18,489 : ***** Transfer task : SICK-Entailment*****


2019-02-13 01:52:18,512 : loading BERT mode bert-base-uncased
2019-02-13 01:52:18,513 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:52:18,532 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:52:18,533 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2gg_y19v
2019-02-13 01:52:21,029 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:52:22,679 : Computing embedding for train
2019-02-13 01:52:37,279 : Computed train embeddings
2019-02-13 01:52:37,280 : Computing embedding for dev
2019-02-13 01:52:39,107 : Computed dev embeddings
2019-02-13 01:52:39,107 : Computing embedding for test
2019-02-13 01:52:56,431 : Computed test embeddings
2019-02-13 01:52:56,460 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:53:03,674 : [('reg:1e-05', 70.2), ('reg:0.0001', 72.2), ('reg:0.001', 73.4), ('reg:0.01', 69.0)]
2019-02-13 01:53:03,675 : Validation : best param found is reg = 0.001 with score             73.4
2019-02-13 01:53:03,675 : Evaluating...
2019-02-13 01:53:05,559 : 
Dev acc : 73.4 Test acc : 71.69 for                        SICK entailment

2019-02-13 01:53:05,559 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 01:53:05,586 : loading BERT mode bert-base-uncased
2019-02-13 01:53:05,586 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:53:05,606 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:53:05,606 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpha_srzfv
2019-02-13 01:53:08,045 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:53:09,652 : Computing embedding for train
2019-02-13 01:53:26,960 : Computed train embeddings
2019-02-13 01:53:26,960 : Computing embedding for dev
2019-02-13 01:53:29,071 : Computed dev embeddings
2019-02-13 01:53:29,072 : Computing embedding for test
2019-02-13 01:53:46,239 : Computed test embeddings
2019-02-13 01:54:41,628 : Dev : Pearson 0.6691442503501241
2019-02-13 01:54:41,628 : Test : Pearson 0.687198663860213 Spearman 0.6351076561112308 MSE 0.5385652793527482                        for SICK Relatedness

2019-02-13 01:54:41,629 : 

***** Transfer task : STSBenchmark*****


2019-02-13 01:54:41,722 : loading BERT mode bert-base-uncased
2019-02-13 01:54:41,723 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:54:41,743 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:54:41,743 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8i4flrij
2019-02-13 01:54:44,178 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:54:45,662 : Computing embedding for train
2019-02-13 01:55:02,243 : Computed train embeddings
2019-02-13 01:55:02,243 : Computing embedding for dev
2019-02-13 01:55:07,354 : Computed dev embeddings
2019-02-13 01:55:07,354 : Computing embedding for test
2019-02-13 01:55:10,945 : Computed test embeddings
2019-02-13 01:55:56,553 : Dev : Pearson 0.47728406848883
2019-02-13 01:55:56,553 : Test : Pearson 0.44280712746696177 Spearman 0.44081226467020446 MSE 2.0199047320325065                        for SICK Relatedness

2019-02-13 01:55:56,553 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 01:55:56,804 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 01:55:56,814 : loading BERT mode bert-base-uncased
2019-02-13 01:55:56,814 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:55:56,908 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:55:56,908 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdg39wddu
2019-02-13 01:55:59,346 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:56:00,843 : Computing embeddings for train/dev/test
2019-02-13 01:59:18,126 : Computed embeddings
2019-02-13 01:59:18,126 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:00:21,700 : [('reg:1e-05', 63.13), ('reg:0.0001', 62.03), ('reg:0.001', 50.25), ('reg:0.01', 50.84)]
2019-02-13 02:00:21,700 : Validation : best param found is reg = 1e-05 with score             63.13
2019-02-13 02:00:21,700 : Evaluating...
2019-02-13 02:00:39,046 : 
Dev acc : 63.1 Test acc : 62.8 for LENGTH classification

2019-02-13 02:00:39,054 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 02:00:39,555 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 02:00:39,601 : loading BERT mode bert-base-uncased
2019-02-13 02:00:39,601 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:00:39,635 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:00:39,635 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe_t72txz
2019-02-13 02:00:42,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:00:43,543 : Computing embeddings for train/dev/test
2019-02-13 02:03:22,244 : Computed embeddings
2019-02-13 02:03:22,244 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:04:27,976 : [('reg:1e-05', 17.53), ('reg:0.0001', 3.45), ('reg:0.001', 0.8), ('reg:0.01', 0.31)]
2019-02-13 02:04:27,976 : Validation : best param found is reg = 1e-05 with score             17.53
2019-02-13 02:04:27,977 : Evaluating...
2019-02-13 02:04:44,997 : 
Dev acc : 17.5 Test acc : 17.5 for WORDCONTENT classification

2019-02-13 02:04:45,005 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 02:04:45,353 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 02:04:45,418 : loading BERT mode bert-base-uncased
2019-02-13 02:04:45,418 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:04:45,515 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:04:45,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqzfsrd7y
2019-02-13 02:04:47,955 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:04:49,379 : Computing embeddings for train/dev/test
2019-02-13 02:07:30,659 : Computed embeddings
2019-02-13 02:07:30,659 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:08:10,777 : [('reg:1e-05', 30.06), ('reg:0.0001', 27.23), ('reg:0.001', 25.58), ('reg:0.01', 24.98)]
2019-02-13 02:08:10,777 : Validation : best param found is reg = 1e-05 with score             30.06
2019-02-13 02:08:10,777 : Evaluating...
2019-02-13 02:08:27,226 : 
Dev acc : 30.1 Test acc : 30.2 for DEPTH classification

2019-02-13 02:08:27,233 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 02:08:27,787 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 02:08:27,849 : loading BERT mode bert-base-uncased
2019-02-13 02:08:27,849 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:08:27,875 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:08:27,876 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpen_wj9c8
2019-02-13 02:08:30,311 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:08:31,752 : Computing embeddings for train/dev/test
2019-02-13 02:11:03,156 : Computed embeddings
2019-02-13 02:11:03,156 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:12:16,680 : [('reg:1e-05', 74.45), ('reg:0.0001', 74.91), ('reg:0.001', 66.8), ('reg:0.01', 58.57)]
2019-02-13 02:12:16,680 : Validation : best param found is reg = 0.0001 with score             74.91
2019-02-13 02:12:16,680 : Evaluating...
2019-02-13 02:12:37,673 : 
Dev acc : 74.9 Test acc : 75.8 for TOPCONSTITUENTS classification

2019-02-13 02:12:37,681 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 02:12:38,208 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 02:12:38,274 : loading BERT mode bert-base-uncased
2019-02-13 02:12:38,274 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:12:38,305 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:12:38,305 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkg4_x46h
2019-02-13 02:12:40,772 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:12:42,202 : Computing embeddings for train/dev/test
2019-02-13 02:15:36,565 : Computed embeddings
2019-02-13 02:15:36,565 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:16:43,638 : [('reg:1e-05', 81.85), ('reg:0.0001', 81.79), ('reg:0.001', 81.19), ('reg:0.01', 78.8)]
2019-02-13 02:16:43,638 : Validation : best param found is reg = 1e-05 with score             81.85
2019-02-13 02:16:43,638 : Evaluating...
2019-02-13 02:16:52,944 : 
Dev acc : 81.8 Test acc : 81.4 for BIGRAMSHIFT classification

2019-02-13 02:16:52,952 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 02:16:53,355 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 02:16:53,421 : loading BERT mode bert-base-uncased
2019-02-13 02:16:53,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:16:53,451 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:16:53,451 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc138len9
2019-02-13 02:16:55,887 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:16:57,298 : Computing embeddings for train/dev/test
2019-02-13 02:19:15,651 : Computed embeddings
2019-02-13 02:19:15,652 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:20:15,973 : [('reg:1e-05', 89.89), ('reg:0.0001', 89.56), ('reg:0.001', 90.08), ('reg:0.01', 90.58)]
2019-02-13 02:20:15,973 : Validation : best param found is reg = 0.01 with score             90.58
2019-02-13 02:20:15,974 : Evaluating...
2019-02-13 02:20:32,357 : 
Dev acc : 90.6 Test acc : 89.7 for TENSE classification

2019-02-13 02:20:32,365 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 02:20:32,768 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 02:20:32,831 : loading BERT mode bert-base-uncased
2019-02-13 02:20:32,831 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:20:32,946 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:20:32,946 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb43ma72l
2019-02-13 02:20:35,383 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:20:36,873 : Computing embeddings for train/dev/test
2019-02-13 02:23:01,992 : Computed embeddings
2019-02-13 02:23:01,992 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:24:07,093 : [('reg:1e-05', 87.48), ('reg:0.0001', 87.46), ('reg:0.001', 86.83), ('reg:0.01', 85.53)]
2019-02-13 02:24:07,093 : Validation : best param found is reg = 1e-05 with score             87.48
2019-02-13 02:24:07,093 : Evaluating...
2019-02-13 02:24:39,368 : 
Dev acc : 87.5 Test acc : 87.4 for SUBJNUMBER classification

2019-02-13 02:24:39,375 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 02:24:39,791 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 02:24:39,858 : loading BERT mode bert-base-uncased
2019-02-13 02:24:39,859 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:24:39,977 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:24:39,977 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfguzi6qj
2019-02-13 02:24:42,418 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:24:43,910 : Computing embeddings for train/dev/test
2019-02-13 02:27:18,027 : Computed embeddings
2019-02-13 02:27:18,027 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:28:40,447 : [('reg:1e-05', 77.11), ('reg:0.0001', 77.04), ('reg:0.001', 77.0), ('reg:0.01', 71.89)]
2019-02-13 02:28:40,447 : Validation : best param found is reg = 1e-05 with score             77.11
2019-02-13 02:28:40,447 : Evaluating...
2019-02-13 02:28:59,041 : 
Dev acc : 77.1 Test acc : 78.2 for OBJNUMBER classification

2019-02-13 02:28:59,049 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 02:28:59,634 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 02:28:59,704 : loading BERT mode bert-base-uncased
2019-02-13 02:28:59,705 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:28:59,734 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:28:59,734 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp6ro8u9x
2019-02-13 02:29:02,178 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:29:03,666 : Computing embeddings for train/dev/test
2019-02-13 02:32:04,819 : Computed embeddings
2019-02-13 02:32:04,819 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:32:56,491 : [('reg:1e-05', 56.93), ('reg:0.0001', 56.94), ('reg:0.001', 57.47), ('reg:0.01', 57.48)]
2019-02-13 02:32:56,491 : Validation : best param found is reg = 0.01 with score             57.48
2019-02-13 02:32:56,491 : Evaluating...
2019-02-13 02:33:10,372 : 
Dev acc : 57.5 Test acc : 57.1 for ODDMANOUT classification

2019-02-13 02:33:10,379 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 02:33:10,974 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 02:33:11,050 : loading BERT mode bert-base-uncased
2019-02-13 02:33:11,051 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:33:11,080 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:33:11,080 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvvsw122e
2019-02-13 02:33:13,528 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:33:15,071 : Computing embeddings for train/dev/test
2019-02-13 02:36:05,144 : Computed embeddings
2019-02-13 02:36:05,144 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:37:44,565 : [('reg:1e-05', 70.88), ('reg:0.0001', 70.93), ('reg:0.001', 61.18), ('reg:0.01', 58.98)]
2019-02-13 02:37:44,565 : Validation : best param found is reg = 0.0001 with score             70.93
2019-02-13 02:37:44,565 : Evaluating...
2019-02-13 02:38:04,487 : 
Dev acc : 70.9 Test acc : 69.4 for COORDINATIONINVERSION classification

2019-02-13 02:38:04,495 : {'STS12': {'MSRpar': {'pearson': (0.21066139401165082, 5.708935805789869e-09), 'spearman': SpearmanrResult(correlation=0.2657399200763268, pvalue=1.3733389725315842e-13), 'nsamples': 750}, 'MSRvid': {'pearson': (0.023787409071418766, 0.5154012727779222), 'spearman': SpearmanrResult(correlation=0.04141954400821207, pvalue=0.2572498908787381), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.42915150589769885, 5.438010172525836e-22), 'spearman': SpearmanrResult(correlation=0.49658582602304324, pvalue=5.937320498287193e-30), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.30582979635881036, 1.0562661475247592e-17), 'spearman': SpearmanrResult(correlation=0.29567976221913966, pvalue=1.3438413685229922e-16), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5399910819948284, 1.4084582346580594e-31), 'spearman': SpearmanrResult(correlation=0.4452592396119006, pvalue=7.951976079773241e-21), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3018842374668814, 'wmean': 0.2630778418611294}, 'spearman': {'mean': 0.30893685838772444, 'wmean': 0.27597192743805793}}}, 'STS13': {'FNWN': {'pearson': (0.0854695565445628, 0.24225934430483406), 'spearman': SpearmanrResult(correlation=0.09956169737735293, pvalue=0.17286487605289325), 'nsamples': 189}, 'headlines': {'pearson': (0.3958044884340722, 1.5455389632954395e-29), 'spearman': SpearmanrResult(correlation=0.3857471988898105, pvalue=5.092623683399922e-28), 'nsamples': 750}, 'OnWN': {'pearson': (0.026945046532353897, 0.5241916748899758), 'spearman': SpearmanrResult(correlation=0.023469662318130143, pvalue=0.5790822443608172), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.16940636383699628, 'wmean': 0.2187488557447514}, 'spearman': {'mean': 0.16959285286176454, 'wmean': 0.21419602702143242}}}, 'STS14': {'deft-forum': {'pearson': (-0.11571490848384293, 0.01404492706163805), 'spearman': SpearmanrResult(correlation=-0.09794449526413412, pvalue=0.037807680086308484), 'nsamples': 450}, 'deft-news': {'pearson': (0.38848768968750397, 3.022961312645294e-12), 'spearman': SpearmanrResult(correlation=0.4330976738617186, pvalue=3.79876367580961e-15), 'nsamples': 300}, 'headlines': {'pearson': (0.3821974502630231, 1.699158067732195e-27), 'spearman': SpearmanrResult(correlation=0.35687581101631866, pvalue=6.035528693948021e-24), 'nsamples': 750}, 'images': {'pearson': (0.14990440368489527, 3.7598006961529614e-05), 'spearman': SpearmanrResult(correlation=0.16621930935785292, pvalue=4.730846074334027e-06), 'nsamples': 750}, 'OnWN': {'pearson': (0.2438253434795928, 1.2993627421067837e-11), 'spearman': SpearmanrResult(correlation=0.24456748084447838, pvalue=1.1216537265570908e-11), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4445089869844141, 1.1461134153345033e-37), 'spearman': SpearmanrResult(correlation=0.40740399483466805, pvalue=2.3568978141337697e-31), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.24886816093593103, 'wmean': 0.26128046303932423}, 'spearman': {'mean': 0.2517032957751504, 'wmean': 0.257907793687905}}}, 'STS15': {'answers-forums': {'pearson': (0.2253303058054386, 1.0543154608333686e-05), 'spearman': SpearmanrResult(correlation=0.2561548808220161, pvalue=4.956443152937436e-07), 'nsamples': 375}, 'answers-students': {'pearson': (0.3695704381941862, 1.0969652682199395e-25), 'spearman': SpearmanrResult(correlation=0.37737713185033006, pvalue=8.523084111837608e-27), 'nsamples': 750}, 'belief': {'pearson': (0.37845277505163466, 3.240587662618194e-14), 'spearman': SpearmanrResult(correlation=0.42809222975576816, pvalue=3.824550087318502e-18), 'nsamples': 375}, 'headlines': {'pearson': (0.43102937000048996, 2.779684915401729e-35), 'spearman': SpearmanrResult(correlation=0.4396747665306025, pvalue=8.449661468367129e-37), 'nsamples': 750}, 'images': {'pearson': (0.12071963728809429, 0.0009241616252749972), 'spearman': SpearmanrResult(correlation=0.13780801331719023, pvalue=0.00015320887713354112), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.30502050526796876, 'wmean': 0.3058027464778268}, 'spearman': {'mean': 0.3278214044551814, 'wmean': 0.3242458667467537}}}, 'STS16': {'answer-answer': {'pearson': (0.3335872687446605, 5.119517623486627e-08), 'spearman': SpearmanrResult(correlation=0.33981937471285095, pvalue=2.7711539446671402e-08), 'nsamples': 254}, 'headlines': {'pearson': (0.5388428371234959, 3.744246360371832e-20), 'spearman': SpearmanrResult(correlation=0.5525752093654931, pvalue=2.617013368887917e-21), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5215463887419741, 1.9166454774640222e-17), 'spearman': SpearmanrResult(correlation=0.6059246937835111, pvalue=1.9301124751035793e-24), 'nsamples': 230}, 'postediting': {'pearson': (0.682021008151072, 9.762787756819518e-35), 'spearman': SpearmanrResult(correlation=0.7568522528853883, pvalue=1.3568113385944333e-46), 'nsamples': 244}, 'question-question': {'pearson': (0.02581113652745789, 0.7106589951969635), 'spearman': SpearmanrResult(correlation=0.10009715276898969, pvalue=0.14929302046105217), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.42036172785773207, 'wmean': 0.4305787147037509}, 'spearman': {'mean': 0.47105373670324663, 'wmean': 0.4796461066711914}}}, 'MR': {'devacc': 74.84, 'acc': 75.52, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.69, 'acc': 75.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.68, 'acc': 86.61, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.62, 'acc': 93.8, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.96, 'acc': 78.69, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.24, 'acc': 42.58, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.13, 'acc': 87.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.51, 'acc': 68.17, 'f1': 76.65, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.4, 'acc': 71.69, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6691442503501241, 'pearson': 0.687198663860213, 'spearman': 0.6351076561112308, 'mse': 0.5385652793527482, 'yhat': array([2.20088921, 4.1497965 , 1.54807265, ..., 3.11174169, 4.30984041,
       4.32978644]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.47728406848883, 'pearson': 0.44280712746696177, 'spearman': 0.44081226467020446, 'mse': 2.0199047320325065, 'yhat': array([1.86005691, 2.18744957, 2.84396434, ..., 3.77856238, 3.66884905,
       3.65768549]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 63.13, 'acc': 62.82, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 17.53, 'acc': 17.53, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.06, 'acc': 30.18, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.91, 'acc': 75.79, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 81.85, 'acc': 81.39, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.58, 'acc': 89.68, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.48, 'acc': 87.43, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.11, 'acc': 78.19, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.48, 'acc': 57.14, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 70.93, 'acc': 69.44, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 02:38:04,495 : ********************************************************************************
2019-02-13 02:38:04,495 : ********************************************************************************
2019-02-13 02:38:04,495 : ********************************************************************************
2019-02-13 02:38:04,495 : layer 9
2019-02-13 02:38:04,495 : ********************************************************************************
2019-02-13 02:38:04,495 : ********************************************************************************
2019-02-13 02:38:04,495 : ********************************************************************************
2019-02-13 02:38:04,578 : ***** Transfer task : STS12 *****


2019-02-13 02:38:04,591 : loading BERT mode bert-base-uncased
2019-02-13 02:38:04,591 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:38:04,608 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:38:04,608 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2ffy39aw
2019-02-13 02:38:07,043 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:38:11,416 : MSRpar : pearson = 0.2373, spearman = 0.2770
2019-02-13 02:38:13,498 : MSRvid : pearson = 0.0012, spearman = 0.0538
2019-02-13 02:38:15,002 : SMTeuroparl : pearson = 0.4065, spearman = 0.4949
2019-02-13 02:38:17,509 : surprise.OnWN : pearson = 0.3251, spearman = 0.3275
2019-02-13 02:38:18,850 : surprise.SMTnews : pearson = 0.5508, spearman = 0.4714
2019-02-13 02:38:18,851 : ALL (weighted average) : Pearson = 0.2668,             Spearman = 0.2925
2019-02-13 02:38:18,851 : ALL (average) : Pearson = 0.3042,             Spearman = 0.3249

2019-02-13 02:38:18,851 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 02:38:18,861 : loading BERT mode bert-base-uncased
2019-02-13 02:38:18,861 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:38:18,878 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:38:18,878 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjynu1g90
2019-02-13 02:38:21,314 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:38:23,750 : FNWN : pearson = 0.1479, spearman = 0.1614
2019-02-13 02:38:25,971 : headlines : pearson = 0.3875, spearman = 0.3759
2019-02-13 02:38:27,704 : OnWN : pearson = 0.0838, spearman = 0.0755
2019-02-13 02:38:27,704 : ALL (weighted average) : Pearson = 0.2437,             Spearman = 0.2365
2019-02-13 02:38:27,704 : ALL (average) : Pearson = 0.2064,             Spearman = 0.2043

2019-02-13 02:38:27,704 : ***** Transfer task : STS14 *****


2019-02-13 02:38:27,720 : loading BERT mode bert-base-uncased
2019-02-13 02:38:27,721 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:38:27,739 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:38:27,739 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7_ypzcs7
2019-02-13 02:38:30,169 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:38:33,076 : deft-forum : pearson = -0.1190, spearman = -0.1098
2019-02-13 02:38:34,472 : deft-news : pearson = 0.3670, spearman = 0.4103
2019-02-13 02:38:36,938 : headlines : pearson = 0.3809, spearman = 0.3560
2019-02-13 02:38:39,434 : images : pearson = 0.1516, spearman = 0.1601
2019-02-13 02:38:41,960 : OnWN : pearson = 0.2906, spearman = 0.2875
2019-02-13 02:38:43,863 : tweet-news : pearson = 0.4698, spearman = 0.4243
2019-02-13 02:38:43,864 : ALL (weighted average) : Pearson = 0.2737,             Spearman = 0.2652
2019-02-13 02:38:43,864 : ALL (average) : Pearson = 0.2568,             Spearman = 0.2547

2019-02-13 02:38:43,864 : ***** Transfer task : STS15 *****


2019-02-13 02:38:43,899 : loading BERT mode bert-base-uncased
2019-02-13 02:38:43,899 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:38:43,917 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:38:43,917 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpacyjrjc0
2019-02-13 02:38:46,352 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:38:49,200 : answers-forums : pearson = 0.2697, spearman = 0.2785
2019-02-13 02:38:51,568 : answers-students : pearson = 0.3904, spearman = 0.3971
2019-02-13 02:38:53,245 : belief : pearson = 0.4040, spearman = 0.4296
2019-02-13 02:38:55,056 : headlines : pearson = 0.4143, spearman = 0.4185
2019-02-13 02:38:56,827 : images : pearson = 0.1693, spearman = 0.1858
2019-02-13 02:38:56,827 : ALL (weighted average) : Pearson = 0.3277,             Spearman = 0.3388
2019-02-13 02:38:56,828 : ALL (average) : Pearson = 0.3295,             Spearman = 0.3419

2019-02-13 02:38:56,828 : ***** Transfer task : STS16 *****


2019-02-13 02:38:56,885 : loading BERT mode bert-base-uncased
2019-02-13 02:38:56,885 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:38:56,903 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:38:56,903 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0idw0znl
2019-02-13 02:38:59,346 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:39:01,587 : answer-answer : pearson = 0.3271, spearman = 0.3360
2019-02-13 02:39:02,342 : headlines : pearson = 0.5373, spearman = 0.5620
2019-02-13 02:39:03,184 : plagiarism : pearson = 0.5936, spearman = 0.6176
2019-02-13 02:39:04,270 : postediting : pearson = 0.7078, spearman = 0.7427
2019-02-13 02:39:05,040 : question-question : pearson = -0.0398, spearman = 0.0092
2019-02-13 02:39:05,040 : ALL (weighted average) : Pearson = 0.4366,             Spearman = 0.4641
2019-02-13 02:39:05,040 : ALL (average) : Pearson = 0.4252,             Spearman = 0.4535

2019-02-13 02:39:05,040 : ***** Transfer task : MR *****


2019-02-13 02:39:05,088 : loading BERT mode bert-base-uncased
2019-02-13 02:39:05,088 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:39:05,108 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:39:05,108 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6_y1xfa3
2019-02-13 02:39:07,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:39:08,974 : Generating sentence embeddings
2019-02-13 02:39:25,906 : Generated sentence embeddings
2019-02-13 02:39:25,907 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:39:42,342 : Best param found at split 1: l2reg = 0.0001                 with score 77.16
2019-02-13 02:40:06,861 : Best param found at split 2: l2reg = 1e-05                 with score 76.4
2019-02-13 02:40:30,381 : Best param found at split 3: l2reg = 0.001                 with score 76.68
2019-02-13 02:41:04,264 : Best param found at split 4: l2reg = 0.0001                 with score 76.34
2019-02-13 02:41:42,872 : Best param found at split 5: l2reg = 0.001                 with score 76.69
2019-02-13 02:41:45,402 : Dev acc : 76.65 Test acc : 77.88

2019-02-13 02:41:45,403 : ***** Transfer task : CR *****


2019-02-13 02:41:45,410 : loading BERT mode bert-base-uncased
2019-02-13 02:41:45,410 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:41:45,430 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:41:45,430 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr51p8ha1
2019-02-13 02:41:47,867 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:41:49,441 : Generating sentence embeddings
2019-02-13 02:41:55,942 : Generated sentence embeddings
2019-02-13 02:41:55,943 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:42:05,337 : Best param found at split 1: l2reg = 0.01                 with score 82.58
2019-02-13 02:42:14,090 : Best param found at split 2: l2reg = 0.001                 with score 83.44
2019-02-13 02:42:22,642 : Best param found at split 3: l2reg = 0.001                 with score 83.41
2019-02-13 02:42:32,682 : Best param found at split 4: l2reg = 1e-05                 with score 83.45
2019-02-13 02:42:43,407 : Best param found at split 5: l2reg = 0.0001                 with score 83.98
2019-02-13 02:42:43,974 : Dev acc : 83.37 Test acc : 81.77

2019-02-13 02:42:43,974 : ***** Transfer task : MPQA *****


2019-02-13 02:42:44,010 : loading BERT mode bert-base-uncased
2019-02-13 02:42:44,010 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:42:44,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:42:44,030 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_trn6y3e
2019-02-13 02:42:46,464 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:42:47,997 : Generating sentence embeddings
2019-02-13 02:43:00,235 : Generated sentence embeddings
2019-02-13 02:43:00,235 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:43:24,069 : Best param found at split 1: l2reg = 1e-05                 with score 87.22
2019-02-13 02:43:43,669 : Best param found at split 2: l2reg = 1e-05                 with score 86.87
2019-02-13 02:44:09,839 : Best param found at split 3: l2reg = 1e-05                 with score 86.45
2019-02-13 02:44:36,205 : Best param found at split 4: l2reg = 0.001                 with score 87.73
2019-02-13 02:45:17,618 : Best param found at split 5: l2reg = 0.001                 with score 87.05
2019-02-13 02:45:19,819 : Dev acc : 87.06 Test acc : 87.76

2019-02-13 02:45:19,820 : ***** Transfer task : SUBJ *****


2019-02-13 02:45:19,835 : loading BERT mode bert-base-uncased
2019-02-13 02:45:19,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:45:19,890 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:45:19,890 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz0rb5tq9
2019-02-13 02:45:22,368 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:45:23,938 : Generating sentence embeddings
2019-02-13 02:45:48,243 : Generated sentence embeddings
2019-02-13 02:45:48,243 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:46:12,297 : Best param found at split 1: l2reg = 0.001                 with score 94.82
2019-02-13 02:46:36,825 : Best param found at split 2: l2reg = 0.0001                 with score 94.82
2019-02-13 02:46:59,770 : Best param found at split 3: l2reg = 0.0001                 with score 94.62
2019-02-13 02:47:25,442 : Best param found at split 4: l2reg = 0.0001                 with score 95.15
2019-02-13 02:47:47,301 : Best param found at split 5: l2reg = 0.001                 with score 94.75
2019-02-13 02:47:48,328 : Dev acc : 94.83 Test acc : 94.58

2019-02-13 02:47:48,329 : ***** Transfer task : SST Binary classification *****


2019-02-13 02:47:48,469 : loading BERT mode bert-base-uncased
2019-02-13 02:47:48,469 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:47:48,492 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:47:48,492 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwlyjchfy
2019-02-13 02:47:50,922 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:47:52,400 : Computing embedding for train
2019-02-13 02:49:28,237 : Computed train embeddings
2019-02-13 02:49:28,237 : Computing embedding for dev
2019-02-13 02:49:29,693 : Computed dev embeddings
2019-02-13 02:49:29,694 : Computing embedding for test
2019-02-13 02:49:32,756 : Computed test embeddings
2019-02-13 02:49:32,757 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:50:04,292 : [('reg:1e-05', 82.0), ('reg:0.0001', 82.0), ('reg:0.001', 81.42), ('reg:0.01', 80.62)]
2019-02-13 02:50:04,292 : Validation : best param found is reg = 1e-05 with score             82.0
2019-02-13 02:50:04,292 : Evaluating...
2019-02-13 02:50:15,079 : 
Dev acc : 82.0 Test acc : 80.83 for             SST Binary classification

2019-02-13 02:50:15,084 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 02:50:15,134 : loading BERT mode bert-base-uncased
2019-02-13 02:50:15,134 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:50:15,157 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:50:15,157 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7ppd3p3l
2019-02-13 02:50:17,609 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:50:19,083 : Computing embedding for train
2019-02-13 02:50:35,210 : Computed train embeddings
2019-02-13 02:50:35,210 : Computing embedding for dev
2019-02-13 02:50:37,405 : Computed dev embeddings
2019-02-13 02:50:37,405 : Computing embedding for test
2019-02-13 02:50:41,793 : Computed test embeddings
2019-02-13 02:50:41,793 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:50:49,101 : [('reg:1e-05', 41.87), ('reg:0.0001', 39.42), ('reg:0.001', 42.69), ('reg:0.01', 41.33)]
2019-02-13 02:50:49,101 : Validation : best param found is reg = 0.001 with score             42.69
2019-02-13 02:50:49,101 : Evaluating...
2019-02-13 02:50:51,083 : 
Dev acc : 42.69 Test acc : 41.67 for             SST Fine-Grained classification

2019-02-13 02:50:51,083 : ***** Transfer task : TREC *****


2019-02-13 02:50:51,096 : loading BERT mode bert-base-uncased
2019-02-13 02:50:51,096 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:50:51,116 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:50:51,116 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmqs6lcg4
2019-02-13 02:50:53,552 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:51:02,832 : Computed train embeddings
2019-02-13 02:51:03,409 : Computed test embeddings
2019-02-13 02:51:03,409 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 02:51:22,463 : [('reg:1e-05', 80.39), ('reg:0.0001', 80.5), ('reg:0.001', 79.23), ('reg:0.01', 75.37)]
2019-02-13 02:51:22,463 : Cross-validation : best param found is reg = 0.0001             with score 80.5
2019-02-13 02:51:22,463 : Evaluating...
2019-02-13 02:51:23,626 : 
Dev acc : 80.5 Test acc : 86.8             for TREC

2019-02-13 02:51:23,627 : ***** Transfer task : MRPC *****


2019-02-13 02:51:23,647 : loading BERT mode bert-base-uncased
2019-02-13 02:51:23,647 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:51:23,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:51:23,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqbzcj_sd
2019-02-13 02:51:26,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:51:27,645 : Computing embedding for train
2019-02-13 02:51:41,491 : Computed train embeddings
2019-02-13 02:51:41,491 : Computing embedding for test
2019-02-13 02:51:47,479 : Computed test embeddings
2019-02-13 02:51:47,495 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 02:52:00,750 : [('reg:1e-05', 70.71), ('reg:0.0001', 70.78), ('reg:0.001', 71.22), ('reg:0.01', 70.53)]
2019-02-13 02:52:00,751 : Cross-validation : best param found is reg = 0.001             with score 71.22
2019-02-13 02:52:00,751 : Evaluating...
2019-02-13 02:52:01,403 : Dev acc : 71.22 Test acc 71.07; Test F1 81.12 for MRPC.

2019-02-13 02:52:01,403 : ***** Transfer task : SICK-Entailment*****


2019-02-13 02:52:01,427 : loading BERT mode bert-base-uncased
2019-02-13 02:52:01,427 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:52:01,448 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:52:01,448 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvlj41h5g
2019-02-13 02:52:03,883 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:52:05,354 : Computing embedding for train
2019-02-13 02:52:16,706 : Computed train embeddings
2019-02-13 02:52:16,706 : Computing embedding for dev
2019-02-13 02:52:18,143 : Computed dev embeddings
2019-02-13 02:52:18,143 : Computing embedding for test
2019-02-13 02:52:31,334 : Computed test embeddings
2019-02-13 02:52:31,362 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:52:35,189 : [('reg:1e-05', 71.2), ('reg:0.0001', 73.2), ('reg:0.001', 73.8), ('reg:0.01', 73.8)]
2019-02-13 02:52:35,189 : Validation : best param found is reg = 0.001 with score             73.8
2019-02-13 02:52:35,189 : Evaluating...
2019-02-13 02:52:36,167 : 
Dev acc : 73.8 Test acc : 71.75 for                        SICK entailment

2019-02-13 02:52:36,167 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 02:52:36,194 : loading BERT mode bert-base-uncased
2019-02-13 02:52:36,194 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:52:36,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:52:36,251 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf11fhc95
2019-02-13 02:52:38,723 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:52:40,236 : Computing embedding for train
2019-02-13 02:52:53,362 : Computed train embeddings
2019-02-13 02:52:53,362 : Computing embedding for dev
2019-02-13 02:52:55,014 : Computed dev embeddings
2019-02-13 02:52:55,014 : Computing embedding for test
2019-02-13 02:53:09,917 : Computed test embeddings
2019-02-13 02:54:08,062 : Dev : Pearson 0.7166140888139473
2019-02-13 02:54:08,062 : Test : Pearson 0.6943244008528043 Spearman 0.6384538811677655 MSE 0.5348548722864234                        for SICK Relatedness

2019-02-13 02:54:08,063 : 

***** Transfer task : STSBenchmark*****


2019-02-13 02:54:08,104 : loading BERT mode bert-base-uncased
2019-02-13 02:54:08,104 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:54:08,133 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:54:08,133 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpksf3fnrd
2019-02-13 02:54:10,603 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:54:12,119 : Computing embedding for train
2019-02-13 02:54:28,768 : Computed train embeddings
2019-02-13 02:54:28,768 : Computing embedding for dev
2019-02-13 02:54:33,666 : Computed dev embeddings
2019-02-13 02:54:33,666 : Computing embedding for test
2019-02-13 02:54:37,964 : Computed test embeddings
2019-02-13 02:55:46,180 : Dev : Pearson 0.4678601789119381
2019-02-13 02:55:46,180 : Test : Pearson 0.42740355164288973 Spearman 0.42865504598877424 MSE 2.0439178769739743                        for SICK Relatedness

2019-02-13 02:55:46,181 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 02:55:46,448 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 02:55:46,458 : loading BERT mode bert-base-uncased
2019-02-13 02:55:46,459 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:55:46,482 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:55:46,483 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpezfcljwt
2019-02-13 02:55:48,913 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:55:50,476 : Computing embeddings for train/dev/test
2019-02-13 02:58:46,869 : Computed embeddings
2019-02-13 02:58:46,869 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:00:06,484 : [('reg:1e-05', 59.82), ('reg:0.0001', 59.45), ('reg:0.001', 57.62), ('reg:0.01', 48.55)]
2019-02-13 03:00:06,484 : Validation : best param found is reg = 1e-05 with score             59.82
2019-02-13 03:00:06,484 : Evaluating...
2019-02-13 03:00:29,745 : 
Dev acc : 59.8 Test acc : 61.1 for LENGTH classification

2019-02-13 03:00:29,752 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 03:00:30,106 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 03:00:30,149 : loading BERT mode bert-base-uncased
2019-02-13 03:00:30,150 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:00:30,179 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:00:30,179 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjekcjm0u
2019-02-13 03:00:32,610 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:00:34,171 : Computing embeddings for train/dev/test
2019-02-13 03:03:29,552 : Computed embeddings
2019-02-13 03:03:29,552 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:04:45,099 : [('reg:1e-05', 21.26), ('reg:0.0001', 5.69), ('reg:0.001', 1.1), ('reg:0.01', 0.51)]
2019-02-13 03:04:45,099 : Validation : best param found is reg = 1e-05 with score             21.26
2019-02-13 03:04:45,099 : Evaluating...
2019-02-13 03:05:07,930 : 
Dev acc : 21.3 Test acc : 21.1 for WORDCONTENT classification

2019-02-13 03:05:07,938 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 03:05:08,288 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 03:05:08,354 : loading BERT mode bert-base-uncased
2019-02-13 03:05:08,354 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:05:08,452 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:05:08,453 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfe65c_fy
2019-02-13 03:05:10,888 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:05:12,372 : Computing embeddings for train/dev/test
2019-02-13 03:07:36,876 : Computed embeddings
2019-02-13 03:07:36,876 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:08:33,018 : [('reg:1e-05', 29.37), ('reg:0.0001', 27.49), ('reg:0.001', 25.64), ('reg:0.01', 22.61)]
2019-02-13 03:08:33,018 : Validation : best param found is reg = 1e-05 with score             29.37
2019-02-13 03:08:33,019 : Evaluating...
2019-02-13 03:08:52,239 : 
Dev acc : 29.4 Test acc : 29.2 for DEPTH classification

2019-02-13 03:08:52,247 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 03:08:52,616 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 03:08:52,679 : loading BERT mode bert-base-uncased
2019-02-13 03:08:52,679 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:08:52,792 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:08:52,792 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcmn26mha
2019-02-13 03:08:55,227 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:08:56,705 : Computing embeddings for train/dev/test
2019-02-13 03:11:30,414 : Computed embeddings
2019-02-13 03:11:30,414 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:12:25,755 : [('reg:1e-05', 72.46), ('reg:0.0001', 70.48), ('reg:0.001', 69.08), ('reg:0.01', 59.62)]
2019-02-13 03:12:25,755 : Validation : best param found is reg = 1e-05 with score             72.46
2019-02-13 03:12:25,755 : Evaluating...
2019-02-13 03:12:44,506 : 
Dev acc : 72.5 Test acc : 72.6 for TOPCONSTITUENTS classification

2019-02-13 03:12:44,513 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 03:12:45,038 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 03:12:45,104 : loading BERT mode bert-base-uncased
2019-02-13 03:12:45,104 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:12:45,135 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:12:45,136 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9qxatdn5
2019-02-13 03:12:47,571 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:12:49,047 : Computing embeddings for train/dev/test
2019-02-13 03:15:46,378 : Computed embeddings
2019-02-13 03:15:46,378 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:16:48,721 : [('reg:1e-05', 85.38), ('reg:0.0001', 85.29), ('reg:0.001', 84.97), ('reg:0.01', 83.44)]
2019-02-13 03:16:48,721 : Validation : best param found is reg = 1e-05 with score             85.38
2019-02-13 03:16:48,721 : Evaluating...
2019-02-13 03:17:04,771 : 
Dev acc : 85.4 Test acc : 84.4 for BIGRAMSHIFT classification

2019-02-13 03:17:04,779 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 03:17:05,365 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 03:17:05,434 : loading BERT mode bert-base-uncased
2019-02-13 03:17:05,434 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:17:05,462 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:17:05,463 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjqzf44j_
2019-02-13 03:17:07,961 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:17:09,516 : Computing embeddings for train/dev/test
2019-02-13 03:19:25,408 : Computed embeddings
2019-02-13 03:19:25,408 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:20:08,458 : [('reg:1e-05', 89.58), ('reg:0.0001', 89.59), ('reg:0.001', 89.9), ('reg:0.01', 89.9)]
2019-02-13 03:20:08,458 : Validation : best param found is reg = 0.001 with score             89.9
2019-02-13 03:20:08,458 : Evaluating...
2019-02-13 03:20:17,176 : 
Dev acc : 89.9 Test acc : 89.2 for TENSE classification

2019-02-13 03:20:17,185 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 03:20:17,611 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 03:20:17,674 : loading BERT mode bert-base-uncased
2019-02-13 03:20:17,674 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:20:17,702 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:20:17,702 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvdb_rc9o
2019-02-13 03:20:20,138 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:20:21,548 : Computing embeddings for train/dev/test
2019-02-13 03:22:57,169 : Computed embeddings
2019-02-13 03:22:57,169 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:24:06,249 : [('reg:1e-05', 86.24), ('reg:0.0001', 86.14), ('reg:0.001', 86.38), ('reg:0.01', 83.2)]
2019-02-13 03:24:06,250 : Validation : best param found is reg = 0.001 with score             86.38
2019-02-13 03:24:06,250 : Evaluating...
2019-02-13 03:24:14,252 : 
Dev acc : 86.4 Test acc : 85.8 for SUBJNUMBER classification

2019-02-13 03:24:14,260 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 03:24:14,670 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 03:24:14,737 : loading BERT mode bert-base-uncased
2019-02-13 03:24:14,737 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:24:14,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:24:14,858 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyj4l0gyk
2019-02-13 03:24:17,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:24:18,712 : Computing embeddings for train/dev/test
2019-02-13 03:26:44,323 : Computed embeddings
2019-02-13 03:26:44,323 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:28:17,143 : [('reg:1e-05', 74.55), ('reg:0.0001', 76.08), ('reg:0.001', 75.86), ('reg:0.01', 71.12)]
2019-02-13 03:28:17,143 : Validation : best param found is reg = 0.0001 with score             76.08
2019-02-13 03:28:17,143 : Evaluating...
2019-02-13 03:28:28,853 : 
Dev acc : 76.1 Test acc : 77.3 for OBJNUMBER classification

2019-02-13 03:28:28,861 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 03:28:29,449 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 03:28:29,518 : loading BERT mode bert-base-uncased
2019-02-13 03:28:29,518 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:28:29,547 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:28:29,547 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1lg7bu57
2019-02-13 03:28:31,991 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:28:33,392 : Computing embeddings for train/dev/test
2019-02-13 03:31:31,580 : Computed embeddings
2019-02-13 03:31:31,580 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:32:41,631 : [('reg:1e-05', 58.86), ('reg:0.0001', 58.76), ('reg:0.001', 57.85), ('reg:0.01', 57.38)]
2019-02-13 03:32:41,631 : Validation : best param found is reg = 1e-05 with score             58.86
2019-02-13 03:32:41,631 : Evaluating...
2019-02-13 03:32:59,553 : 
Dev acc : 58.9 Test acc : 58.6 for ODDMANOUT classification

2019-02-13 03:32:59,561 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 03:32:59,956 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 03:33:00,032 : loading BERT mode bert-base-uncased
2019-02-13 03:33:00,032 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:33:00,160 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:33:00,160 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq6gz1ob6
2019-02-13 03:33:02,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:33:04,010 : Computing embeddings for train/dev/test
2019-02-13 03:35:46,938 : Computed embeddings
2019-02-13 03:35:46,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:37:28,299 : [('reg:1e-05', 71.69), ('reg:0.0001', 71.62), ('reg:0.001', 70.99), ('reg:0.01', 67.67)]
2019-02-13 03:37:28,299 : Validation : best param found is reg = 1e-05 with score             71.69
2019-02-13 03:37:28,299 : Evaluating...
2019-02-13 03:37:37,686 : 
Dev acc : 71.7 Test acc : 70.5 for COORDINATIONINVERSION classification

2019-02-13 03:37:37,695 : {'STS12': {'MSRpar': {'pearson': (0.23732150353416157, 4.618414291200209e-11), 'spearman': SpearmanrResult(correlation=0.2769972218674011, pvalue=1.1195646451082229e-14), 'nsamples': 750}, 'MSRvid': {'pearson': (0.0011992695963712332, 0.9738431840627108), 'spearman': SpearmanrResult(correlation=0.05382554179196728, pvalue=0.1408367546110975), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.40651989750203854, 1.0780762905422355e-19), 'spearman': SpearmanrResult(correlation=0.4949458700028933, pvalue=9.74884781211349e-30), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3251174712003722, 6.346214206110203e-20), 'spearman': SpearmanrResult(correlation=0.3274539569459294, pvalue=3.329015495702171e-20), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.550842359321709, 4.871017156990426e-33), 'spearman': SpearmanrResult(correlation=0.4713873457009087, pvalue=1.827177357304411e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.30420010023093047, 'wmean': 0.26676557836903997}, 'spearman': {'mean': 0.32492198726182, 'wmean': 0.29246179077218915}}}, 'STS13': {'FNWN': {'pearson': (0.14794580970723736, 0.04219219761728595), 'spearman': SpearmanrResult(correlation=0.1613749261246404, pvalue=0.02652885637573362), 'nsamples': 189}, 'headlines': {'pearson': (0.38752522619478624, 2.7695408757452145e-28), 'spearman': SpearmanrResult(correlation=0.3758632003441952, pvalue=1.4065319656358656e-26), 'nsamples': 750}, 'OnWN': {'pearson': (0.08375267396959814, 0.047392127360468), 'spearman': SpearmanrResult(correlation=0.0755315118871364, pvalue=0.07384688465601386), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2064079032905406, 'wmean': 0.2437272851851347}, 'spearman': {'mean': 0.20425654611865732, 'wmean': 0.23651362630959127}}}, 'STS14': {'deft-forum': {'pearson': (-0.11902251841562451, 0.01151023402159769), 'spearman': SpearmanrResult(correlation=-0.10978313840546783, pvalue=0.01983684707464685), 'nsamples': 450}, 'deft-news': {'pearson': (0.3669793271229437, 5.379843687864704e-11), 'spearman': SpearmanrResult(correlation=0.4102703646460207, pvalue=1.311777822707954e-13), 'nsamples': 300}, 'headlines': {'pearson': (0.3809099840849702, 2.6208592996949945e-27), 'spearman': SpearmanrResult(correlation=0.35595298187419916, pvalue=8.020699661417406e-24), 'nsamples': 750}, 'images': {'pearson': (0.1516413455823836, 3.0449158876302396e-05), 'spearman': SpearmanrResult(correlation=0.1600627945056886, pvalue=1.0597077923749478e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.290570470674278, 4.6553631706246e-16), 'spearman': SpearmanrResult(correlation=0.28748754978136143, pvalue=9.734073760387232e-16), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4697584655268745, 1.9684343874804547e-42), 'spearman': SpearmanrResult(correlation=0.4242889700554511, pvalue=3.951653521045761e-34), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.25680617909597087, 'wmean': 0.27365169713366183}, 'spearman': {'mean': 0.2547132537428755, 'wmean': 0.26520611180636555}}}, 'STS15': {'answers-forums': {'pearson': (0.2697440348009397, 1.128075770017156e-07), 'spearman': SpearmanrResult(correlation=0.27851904240016523, pvalue=4.149029714873457e-08), 'nsamples': 375}, 'answers-students': {'pearson': (0.3904189193038716, 1.019539838783138e-28), 'spearman': SpearmanrResult(correlation=0.3970746153317555, pvalue=9.854562840586162e-30), 'nsamples': 750}, 'belief': {'pearson': (0.40403726181142596, 3.6991747569357865e-16), 'spearman': SpearmanrResult(correlation=0.42956337761334984, pvalue=2.8569840882849715e-18), 'nsamples': 375}, 'headlines': {'pearson': (0.4142802971294049, 1.8241888613465465e-32), 'spearman': SpearmanrResult(correlation=0.4184726547258298, pvalue=3.721909085328712e-33), 'nsamples': 750}, 'images': {'pearson': (0.1692525856167979, 3.145088753069313e-06), 'spearman': SpearmanrResult(correlation=0.18577469711184116, pvalue=2.993896667279881e-07), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.329546619732488, 'wmean': 0.32771061258906437}, 'spearman': {'mean': 0.3418808774365883, 'wmean': 0.33884079429404595}}}, 'STS16': {'answer-answer': {'pearson': (0.3270564242805792, 9.600512187421546e-08), 'spearman': SpearmanrResult(correlation=0.336043174393608, pvalue=4.026105750754545e-08), 'nsamples': 254}, 'headlines': {'pearson': (0.5373200251072341, 4.992045394929496e-20), 'spearman': SpearmanrResult(correlation=0.562003829527071, pvalue=3.917736770588799e-22), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5936044050350544, 2.7362731945726126e-23), 'spearman': SpearmanrResult(correlation=0.617585068928873, pvalue=1.4080352052918652e-25), 'nsamples': 230}, 'postediting': {'pearson': (0.7077820764781809, 2.1517119515754857e-38), 'spearman': SpearmanrResult(correlation=0.7427221993123767, pvalue=4.837380175938314e-44), 'nsamples': 244}, 'question-question': {'pearson': (-0.03977361688840963, 0.5674721612690782), 'spearman': SpearmanrResult(correlation=0.00918238364820357, pvalue=0.8950200279578715), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.4251978628025278, 'wmean': 0.43657687344690504}, 'spearman': {'mean': 0.45350733116202635, 'wmean': 0.464149932981916}}}, 'MR': {'devacc': 76.65, 'acc': 77.88, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 83.37, 'acc': 81.77, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.06, 'acc': 87.76, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.83, 'acc': 94.58, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.0, 'acc': 80.83, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.69, 'acc': 41.67, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.5, 'acc': 86.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.22, 'acc': 71.07, 'f1': 81.12, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.8, 'acc': 71.75, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7166140888139473, 'pearson': 0.6943244008528043, 'spearman': 0.6384538811677655, 'mse': 0.5348548722864234, 'yhat': array([1.82579456, 4.48271422, 1.28874683, ..., 2.80794991, 4.10339894,
       4.49364018]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.4678601789119381, 'pearson': 0.42740355164288973, 'spearman': 0.42865504598877424, 'mse': 2.0439178769739743, 'yhat': array([3.70161049, 1.9628877 , 2.56194076, ..., 3.79305473, 3.66688984,
       3.52281093]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 59.82, 'acc': 61.15, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 21.26, 'acc': 21.07, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.37, 'acc': 29.22, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.46, 'acc': 72.64, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.38, 'acc': 84.39, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.9, 'acc': 89.22, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.38, 'acc': 85.85, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.08, 'acc': 77.32, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.86, 'acc': 58.6, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.69, 'acc': 70.48, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 03:37:37,695 : ********************************************************************************
2019-02-13 03:37:37,695 : ********************************************************************************
2019-02-13 03:37:37,695 : ********************************************************************************
2019-02-13 03:37:37,695 : layer 10
2019-02-13 03:37:37,695 : ********************************************************************************
2019-02-13 03:37:37,695 : ********************************************************************************
2019-02-13 03:37:37,695 : ********************************************************************************
2019-02-13 03:37:37,798 : ***** Transfer task : STS12 *****


2019-02-13 03:37:37,835 : loading BERT mode bert-base-uncased
2019-02-13 03:37:37,835 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:37:37,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:37:37,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk60a0l4_
2019-02-13 03:37:40,291 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:37:43,947 : MSRpar : pearson = 0.2409, spearman = 0.2847
2019-02-13 03:37:45,158 : MSRvid : pearson = -0.0263, spearman = 0.0278
2019-02-13 03:37:46,081 : SMTeuroparl : pearson = 0.4152, spearman = 0.4968
2019-02-13 03:37:47,697 : surprise.OnWN : pearson = 0.3317, spearman = 0.3377
2019-02-13 03:37:48,580 : surprise.SMTnews : pearson = 0.5211, spearman = 0.4446
2019-02-13 03:37:48,580 : ALL (weighted average) : Pearson = 0.2601,             Spearman = 0.2874
2019-02-13 03:37:48,580 : ALL (average) : Pearson = 0.2965,             Spearman = 0.3183

2019-02-13 03:37:48,580 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 03:37:48,592 : loading BERT mode bert-base-uncased
2019-02-13 03:37:48,592 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:37:48,609 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:37:48,609 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqd63xfu4
2019-02-13 03:37:51,045 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:37:53,197 : FNWN : pearson = 0.0803, spearman = 0.0936
2019-02-13 03:37:54,524 : headlines : pearson = 0.4338, spearman = 0.4200
2019-02-13 03:37:55,521 : OnWN : pearson = 0.1325, spearman = 0.1288
2019-02-13 03:37:55,521 : ALL (weighted average) : Pearson = 0.2766,             Spearman = 0.2699
2019-02-13 03:37:55,521 : ALL (average) : Pearson = 0.2155,             Spearman = 0.2141

2019-02-13 03:37:55,521 : ***** Transfer task : STS14 *****


2019-02-13 03:37:55,539 : loading BERT mode bert-base-uncased
2019-02-13 03:37:55,539 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:37:55,557 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:37:55,557 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr2to_b7a
2019-02-13 03:37:57,992 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:38:00,331 : deft-forum : pearson = -0.1069, spearman = -0.1025
2019-02-13 03:38:01,233 : deft-news : pearson = 0.3967, spearman = 0.4350
2019-02-13 03:38:02,681 : headlines : pearson = 0.4195, spearman = 0.3922
2019-02-13 03:38:04,100 : images : pearson = 0.1291, spearman = 0.1345
2019-02-13 03:38:05,527 : OnWN : pearson = 0.3353, spearman = 0.3326
2019-02-13 03:38:07,269 : tweet-news : pearson = 0.4993, spearman = 0.4554
2019-02-13 03:38:07,269 : ALL (weighted average) : Pearson = 0.2956,             Spearman = 0.2855
2019-02-13 03:38:07,269 : ALL (average) : Pearson = 0.2789,             Spearman = 0.2745

2019-02-13 03:38:07,270 : ***** Transfer task : STS15 *****


2019-02-13 03:38:07,345 : loading BERT mode bert-base-uncased
2019-02-13 03:38:07,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:38:07,362 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:38:07,363 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_3uq14xw
2019-02-13 03:38:09,797 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:38:12,323 : answers-forums : pearson = 0.2934, spearman = 0.2895
2019-02-13 03:38:13,702 : answers-students : pearson = 0.4098, spearman = 0.4134
2019-02-13 03:38:14,668 : belief : pearson = 0.4061, spearman = 0.4135
2019-02-13 03:38:15,893 : headlines : pearson = 0.4736, spearman = 0.4661
2019-02-13 03:38:17,069 : images : pearson = 0.1876, spearman = 0.1945
2019-02-13 03:38:17,069 : ALL (weighted average) : Pearson = 0.3552,             Spearman = 0.3564
2019-02-13 03:38:17,070 : ALL (average) : Pearson = 0.3541,             Spearman = 0.3554

2019-02-13 03:38:17,070 : ***** Transfer task : STS16 *****


2019-02-13 03:38:17,111 : loading BERT mode bert-base-uncased
2019-02-13 03:38:17,111 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:38:17,129 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:38:17,129 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmko71qbe
2019-02-13 03:38:19,564 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:38:21,641 : answer-answer : pearson = 0.2953, spearman = 0.3040
2019-02-13 03:38:22,163 : headlines : pearson = 0.5584, spearman = 0.5771
2019-02-13 03:38:22,746 : plagiarism : pearson = 0.6252, spearman = 0.6505
2019-02-13 03:38:23,628 : postediting : pearson = 0.7214, spearman = 0.7442
2019-02-13 03:38:24,112 : question-question : pearson = -0.0076, spearman = 0.0378
2019-02-13 03:38:24,112 : ALL (weighted average) : Pearson = 0.4488,             Spearman = 0.4722
2019-02-13 03:38:24,112 : ALL (average) : Pearson = 0.4385,             Spearman = 0.4627

2019-02-13 03:38:24,112 : ***** Transfer task : MR *****


2019-02-13 03:38:24,165 : loading BERT mode bert-base-uncased
2019-02-13 03:38:24,165 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:38:24,183 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:38:24,183 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoay1fvc7
2019-02-13 03:38:26,618 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:38:28,098 : Generating sentence embeddings
2019-02-13 03:38:47,299 : Generated sentence embeddings
2019-02-13 03:38:47,300 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:39:11,347 : Best param found at split 1: l2reg = 0.001                 with score 78.08
2019-02-13 03:39:45,469 : Best param found at split 2: l2reg = 0.0001                 with score 78.76
2019-02-13 03:40:24,253 : Best param found at split 3: l2reg = 0.001                 with score 78.34
2019-02-13 03:41:02,686 : Best param found at split 4: l2reg = 0.001                 with score 79.07
2019-02-13 03:41:31,777 : Best param found at split 5: l2reg = 0.001                 with score 78.9
2019-02-13 03:41:33,023 : Dev acc : 78.63 Test acc : 79.3

2019-02-13 03:41:33,024 : ***** Transfer task : CR *****


2019-02-13 03:41:33,031 : loading BERT mode bert-base-uncased
2019-02-13 03:41:33,031 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:41:33,053 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:41:33,053 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwskajv5e
2019-02-13 03:41:35,490 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:41:37,039 : Generating sentence embeddings
2019-02-13 03:41:43,785 : Generated sentence embeddings
2019-02-13 03:41:43,785 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:41:49,209 : Best param found at split 1: l2reg = 0.001                 with score 86.15
2019-02-13 03:41:54,499 : Best param found at split 2: l2reg = 0.01                 with score 85.23
2019-02-13 03:41:59,558 : Best param found at split 3: l2reg = 1e-05                 with score 85.99
2019-02-13 03:42:05,099 : Best param found at split 4: l2reg = 0.01                 with score 85.47
2019-02-13 03:42:10,449 : Best param found at split 5: l2reg = 1e-05                 with score 85.6
2019-02-13 03:42:10,740 : Dev acc : 85.69 Test acc : 84.21

2019-02-13 03:42:10,740 : ***** Transfer task : MPQA *****


2019-02-13 03:42:10,777 : loading BERT mode bert-base-uncased
2019-02-13 03:42:10,777 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:42:10,796 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:42:10,797 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp40933uq0
2019-02-13 03:42:13,231 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:42:14,639 : Generating sentence embeddings
2019-02-13 03:42:22,117 : Generated sentence embeddings
2019-02-13 03:42:22,118 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:42:48,905 : Best param found at split 1: l2reg = 0.001                 with score 87.13
2019-02-13 03:43:19,160 : Best param found at split 2: l2reg = 0.001                 with score 86.83
2019-02-13 03:44:02,363 : Best param found at split 3: l2reg = 0.001                 with score 86.67
2019-02-13 03:44:39,511 : Best param found at split 4: l2reg = 0.01                 with score 87.75
2019-02-13 03:45:10,174 : Best param found at split 5: l2reg = 1e-05                 with score 86.91
2019-02-13 03:45:12,367 : Dev acc : 87.06 Test acc : 87.27

2019-02-13 03:45:12,368 : ***** Transfer task : SUBJ *****


2019-02-13 03:45:12,382 : loading BERT mode bert-base-uncased
2019-02-13 03:45:12,382 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:45:12,403 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:45:12,403 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxxt2sb04
2019-02-13 03:45:14,838 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:45:16,421 : Generating sentence embeddings
2019-02-13 03:45:36,922 : Generated sentence embeddings
2019-02-13 03:45:36,923 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:45:54,544 : Best param found at split 1: l2reg = 0.001                 with score 95.01
2019-02-13 03:46:10,159 : Best param found at split 2: l2reg = 1e-05                 with score 95.11
2019-02-13 03:46:34,195 : Best param found at split 3: l2reg = 0.001                 with score 95.04
2019-02-13 03:47:04,224 : Best param found at split 4: l2reg = 0.001                 with score 95.26
2019-02-13 03:47:42,602 : Best param found at split 5: l2reg = 0.001                 with score 94.96
2019-02-13 03:47:44,048 : Dev acc : 95.08 Test acc : 94.85

2019-02-13 03:47:44,049 : ***** Transfer task : SST Binary classification *****


2019-02-13 03:47:44,139 : loading BERT mode bert-base-uncased
2019-02-13 03:47:44,139 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:47:44,215 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:47:44,215 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvv9jgnds
2019-02-13 03:47:46,665 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:47:48,220 : Computing embedding for train
2019-02-13 03:49:33,512 : Computed train embeddings
2019-02-13 03:49:33,512 : Computing embedding for dev
2019-02-13 03:49:35,028 : Computed dev embeddings
2019-02-13 03:49:35,028 : Computing embedding for test
2019-02-13 03:49:38,626 : Computed test embeddings
2019-02-13 03:49:38,626 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:49:59,825 : [('reg:1e-05', 84.75), ('reg:0.0001', 84.86), ('reg:0.001', 84.29), ('reg:0.01', 79.93)]
2019-02-13 03:49:59,825 : Validation : best param found is reg = 0.0001 with score             84.86
2019-02-13 03:49:59,825 : Evaluating...
2019-02-13 03:50:04,381 : 
Dev acc : 84.86 Test acc : 82.04 for             SST Binary classification

2019-02-13 03:50:04,385 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 03:50:04,438 : loading BERT mode bert-base-uncased
2019-02-13 03:50:04,439 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:50:04,458 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:50:04,458 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph3epecvw
2019-02-13 03:50:06,901 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:50:08,365 : Computing embedding for train
2019-02-13 03:50:21,748 : Computed train embeddings
2019-02-13 03:50:21,748 : Computing embedding for dev
2019-02-13 03:50:23,538 : Computed dev embeddings
2019-02-13 03:50:23,538 : Computing embedding for test
2019-02-13 03:50:27,073 : Computed test embeddings
2019-02-13 03:50:27,074 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:50:32,719 : [('reg:1e-05', 42.05), ('reg:0.0001', 39.33), ('reg:0.001', 42.14), ('reg:0.01', 35.97)]
2019-02-13 03:50:32,719 : Validation : best param found is reg = 0.001 with score             42.14
2019-02-13 03:50:32,720 : Evaluating...
2019-02-13 03:50:34,231 : 
Dev acc : 42.14 Test acc : 42.04 for             SST Fine-Grained classification

2019-02-13 03:50:34,232 : ***** Transfer task : TREC *****


2019-02-13 03:50:34,244 : loading BERT mode bert-base-uncased
2019-02-13 03:50:34,244 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:50:34,264 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:50:34,264 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppa4oj7q8
2019-02-13 03:50:36,703 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:50:44,872 : Computed train embeddings
2019-02-13 03:50:45,469 : Computed test embeddings
2019-02-13 03:50:45,469 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 03:51:04,485 : [('reg:1e-05', 79.77), ('reg:0.0001', 79.69), ('reg:0.001', 78.1), ('reg:0.01', 71.66)]
2019-02-13 03:51:04,485 : Cross-validation : best param found is reg = 1e-05             with score 79.77
2019-02-13 03:51:04,485 : Evaluating...
2019-02-13 03:51:06,146 : 
Dev acc : 79.77 Test acc : 88.4             for TREC

2019-02-13 03:51:06,147 : ***** Transfer task : MRPC *****


2019-02-13 03:51:06,167 : loading BERT mode bert-base-uncased
2019-02-13 03:51:06,167 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:51:06,189 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:51:06,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa4sz0hz6
2019-02-13 03:51:08,624 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:51:10,179 : Computing embedding for train
2019-02-13 03:51:28,523 : Computed train embeddings
2019-02-13 03:51:28,523 : Computing embedding for test
2019-02-13 03:51:36,545 : Computed test embeddings
2019-02-13 03:51:36,560 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 03:51:56,207 : [('reg:1e-05', 70.98), ('reg:0.0001', 70.73), ('reg:0.001', 70.22), ('reg:0.01', 70.54)]
2019-02-13 03:51:56,207 : Cross-validation : best param found is reg = 1e-05             with score 70.98
2019-02-13 03:51:56,207 : Evaluating...
2019-02-13 03:51:57,548 : Dev acc : 70.98 Test acc 69.68; Test F1 77.75 for MRPC.

2019-02-13 03:51:57,548 : ***** Transfer task : SICK-Entailment*****


2019-02-13 03:51:57,607 : loading BERT mode bert-base-uncased
2019-02-13 03:51:57,607 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:51:57,627 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:51:57,627 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1vevkkrw
2019-02-13 03:52:00,064 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:52:01,642 : Computing embedding for train
2019-02-13 03:52:18,669 : Computed train embeddings
2019-02-13 03:52:18,670 : Computing embedding for dev
2019-02-13 03:52:20,382 : Computed dev embeddings
2019-02-13 03:52:20,382 : Computing embedding for test
2019-02-13 03:52:37,558 : Computed test embeddings
2019-02-13 03:52:37,586 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:52:42,711 : [('reg:1e-05', 72.2), ('reg:0.0001', 70.8), ('reg:0.001', 72.4), ('reg:0.01', 73.2)]
2019-02-13 03:52:42,711 : Validation : best param found is reg = 0.01 with score             73.2
2019-02-13 03:52:42,712 : Evaluating...
2019-02-13 03:52:43,766 : 
Dev acc : 73.2 Test acc : 69.92 for                        SICK entailment

2019-02-13 03:52:43,767 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 03:52:43,794 : loading BERT mode bert-base-uncased
2019-02-13 03:52:43,794 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:52:43,849 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:52:43,849 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbmzs848n
2019-02-13 03:52:46,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:52:47,874 : Computing embedding for train
2019-02-13 03:53:00,732 : Computed train embeddings
2019-02-13 03:53:00,732 : Computing embedding for dev
2019-02-13 03:53:02,298 : Computed dev embeddings
2019-02-13 03:53:02,299 : Computing embedding for test
2019-02-13 03:53:16,876 : Computed test embeddings
2019-02-13 03:54:00,145 : Dev : Pearson 0.6613608896247289
2019-02-13 03:54:00,145 : Test : Pearson 0.6786826123360956 Spearman 0.6358473936506438 MSE 0.5519720149382482                        for SICK Relatedness

2019-02-13 03:54:00,146 : 

***** Transfer task : STSBenchmark*****


2019-02-13 03:54:00,185 : loading BERT mode bert-base-uncased
2019-02-13 03:54:00,185 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:54:00,215 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:54:00,215 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4vur00go
2019-02-13 03:54:02,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:54:04,070 : Computing embedding for train
2019-02-13 03:54:15,715 : Computed train embeddings
2019-02-13 03:54:15,715 : Computing embedding for dev
2019-02-13 03:54:19,102 : Computed dev embeddings
2019-02-13 03:54:19,102 : Computing embedding for test
2019-02-13 03:54:21,926 : Computed test embeddings
2019-02-13 03:55:13,143 : Dev : Pearson 0.4242663873189794
2019-02-13 03:55:13,143 : Test : Pearson 0.4290266265485483 Spearman 0.4301109170742621 MSE 1.9940691059024633                        for SICK Relatedness

2019-02-13 03:55:13,144 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 03:55:13,412 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 03:55:13,422 : loading BERT mode bert-base-uncased
2019-02-13 03:55:13,422 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:55:13,447 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:55:13,448 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwge2ms79
2019-02-13 03:55:15,890 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:55:17,354 : Computing embeddings for train/dev/test
2019-02-13 03:58:49,335 : Computed embeddings
2019-02-13 03:58:49,335 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:59:41,551 : [('reg:1e-05', 58.81), ('reg:0.0001', 57.44), ('reg:0.001', 52.55), ('reg:0.01', 44.88)]
2019-02-13 03:59:41,551 : Validation : best param found is reg = 1e-05 with score             58.81
2019-02-13 03:59:41,551 : Evaluating...
2019-02-13 04:00:13,702 : 
Dev acc : 58.8 Test acc : 57.9 for LENGTH classification

2019-02-13 04:00:13,709 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 04:00:14,073 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 04:00:14,117 : loading BERT mode bert-base-uncased
2019-02-13 04:00:14,118 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:00:14,149 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:00:14,149 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp603jxtof
2019-02-13 04:00:16,604 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:00:18,178 : Computing embeddings for train/dev/test
2019-02-13 04:03:53,816 : Computed embeddings
2019-02-13 04:03:53,816 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:04:42,544 : [('reg:1e-05', 26.4), ('reg:0.0001', 8.7), ('reg:0.001', 1.44), ('reg:0.01', 0.57)]
2019-02-13 04:04:42,545 : Validation : best param found is reg = 1e-05 with score             26.4
2019-02-13 04:04:42,545 : Evaluating...
2019-02-13 04:05:10,408 : 
Dev acc : 26.4 Test acc : 25.8 for WORDCONTENT classification

2019-02-13 04:05:10,416 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 04:05:10,772 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 04:05:10,837 : loading BERT mode bert-base-uncased
2019-02-13 04:05:10,837 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:05:10,860 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:05:10,861 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgtc6pa7q
2019-02-13 04:05:13,325 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:05:14,943 : Computing embeddings for train/dev/test
2019-02-13 04:08:00,667 : Computed embeddings
2019-02-13 04:08:00,667 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:08:35,126 : [('reg:1e-05', 25.97), ('reg:0.0001', 25.45), ('reg:0.001', 26.01), ('reg:0.01', 22.19)]
2019-02-13 04:08:35,126 : Validation : best param found is reg = 0.001 with score             26.01
2019-02-13 04:08:35,126 : Evaluating...
2019-02-13 04:08:52,759 : 
Dev acc : 26.0 Test acc : 25.6 for DEPTH classification

2019-02-13 04:08:52,766 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 04:08:53,155 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 04:08:53,217 : loading BERT mode bert-base-uncased
2019-02-13 04:08:53,217 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:08:53,244 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:08:53,244 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpghsdpcn0
2019-02-13 04:08:55,727 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:08:57,256 : Computing embeddings for train/dev/test
2019-02-13 04:11:43,805 : Computed embeddings
2019-02-13 04:11:43,805 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:12:24,827 : [('reg:1e-05', 74.5), ('reg:0.0001', 73.63), ('reg:0.001', 69.38), ('reg:0.01', 58.32)]
2019-02-13 04:12:24,827 : Validation : best param found is reg = 1e-05 with score             74.5
2019-02-13 04:12:24,827 : Evaluating...
2019-02-13 04:12:44,254 : 
Dev acc : 74.5 Test acc : 74.4 for TOPCONSTITUENTS classification

2019-02-13 04:12:44,262 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 04:12:44,618 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 04:12:44,683 : loading BERT mode bert-base-uncased
2019-02-13 04:12:44,683 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:12:44,805 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:12:44,805 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb5rby1sa
2019-02-13 04:12:47,239 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:12:48,848 : Computing embeddings for train/dev/test
2019-02-13 04:15:37,299 : Computed embeddings
2019-02-13 04:15:37,300 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:16:35,821 : [('reg:1e-05', 85.53), ('reg:0.0001', 85.45), ('reg:0.001', 85.35), ('reg:0.01', 84.28)]
2019-02-13 04:16:35,821 : Validation : best param found is reg = 1e-05 with score             85.53
2019-02-13 04:16:35,821 : Evaluating...
2019-02-13 04:17:05,327 : 
Dev acc : 85.5 Test acc : 84.8 for BIGRAMSHIFT classification

2019-02-13 04:17:05,334 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 04:17:05,912 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 04:17:05,977 : loading BERT mode bert-base-uncased
2019-02-13 04:17:05,977 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:17:06,007 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:17:06,007 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2n9gk74z
2019-02-13 04:17:08,453 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:17:10,068 : Computing embeddings for train/dev/test
2019-02-13 04:19:28,660 : Computed embeddings
2019-02-13 04:19:28,660 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:20:22,701 : [('reg:1e-05', 89.16), ('reg:0.0001', 88.98), ('reg:0.001', 88.3), ('reg:0.01', 88.15)]
2019-02-13 04:20:22,701 : Validation : best param found is reg = 1e-05 with score             89.16
2019-02-13 04:20:22,702 : Evaluating...
2019-02-13 04:20:38,550 : 
Dev acc : 89.2 Test acc : 88.7 for TENSE classification

2019-02-13 04:20:38,559 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 04:20:38,971 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 04:20:39,036 : loading BERT mode bert-base-uncased
2019-02-13 04:20:39,036 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:20:39,159 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:20:39,159 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdbjk6hzp
2019-02-13 04:20:41,602 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:20:43,076 : Computing embeddings for train/dev/test
2019-02-13 04:22:51,615 : Computed embeddings
2019-02-13 04:22:51,615 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:23:58,178 : [('reg:1e-05', 84.89), ('reg:0.0001', 84.94), ('reg:0.001', 85.01), ('reg:0.01', 81.39)]
2019-02-13 04:23:58,178 : Validation : best param found is reg = 0.001 with score             85.01
2019-02-13 04:23:58,178 : Evaluating...
2019-02-13 04:24:14,625 : 
Dev acc : 85.0 Test acc : 85.4 for SUBJNUMBER classification

2019-02-13 04:24:14,633 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 04:24:15,225 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 04:24:15,292 : loading BERT mode bert-base-uncased
2019-02-13 04:24:15,292 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:24:15,322 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:24:15,322 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp07xy6s4y
2019-02-13 04:24:17,765 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:24:19,256 : Computing embeddings for train/dev/test
2019-02-13 04:26:52,800 : Computed embeddings
2019-02-13 04:26:52,800 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:28:10,361 : [('reg:1e-05', 72.96), ('reg:0.0001', 72.83), ('reg:0.001', 72.28), ('reg:0.01', 62.65)]
2019-02-13 04:28:10,361 : Validation : best param found is reg = 1e-05 with score             72.96
2019-02-13 04:28:10,361 : Evaluating...
2019-02-13 04:28:26,969 : 
Dev acc : 73.0 Test acc : 73.5 for OBJNUMBER classification

2019-02-13 04:28:26,976 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 04:28:27,545 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 04:28:27,614 : loading BERT mode bert-base-uncased
2019-02-13 04:28:27,615 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:28:27,645 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:28:27,645 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxgqadwun
2019-02-13 04:28:30,102 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:28:31,608 : Computing embeddings for train/dev/test
2019-02-13 04:31:40,139 : Computed embeddings
2019-02-13 04:31:40,139 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:32:53,236 : [('reg:1e-05', 60.7), ('reg:0.0001', 60.78), ('reg:0.001', 60.92), ('reg:0.01', 57.89)]
2019-02-13 04:32:53,236 : Validation : best param found is reg = 0.001 with score             60.92
2019-02-13 04:32:53,236 : Evaluating...
2019-02-13 04:33:16,528 : 
Dev acc : 60.9 Test acc : 60.3 for ODDMANOUT classification

2019-02-13 04:33:16,535 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 04:33:16,964 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 04:33:17,041 : loading BERT mode bert-base-uncased
2019-02-13 04:33:17,041 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:33:17,072 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:33:17,072 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa709l7xm
2019-02-13 04:33:19,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:33:20,987 : Computing embeddings for train/dev/test
2019-02-13 04:36:38,363 : Computed embeddings
2019-02-13 04:36:38,363 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:37:56,331 : [('reg:1e-05', 71.03), ('reg:0.0001', 71.01), ('reg:0.001', 70.8), ('reg:0.01', 61.52)]
2019-02-13 04:37:56,332 : Validation : best param found is reg = 1e-05 with score             71.03
2019-02-13 04:37:56,332 : Evaluating...
2019-02-13 04:38:18,118 : 
Dev acc : 71.0 Test acc : 70.5 for COORDINATIONINVERSION classification

2019-02-13 04:38:18,126 : {'STS12': {'MSRpar': {'pearson': (0.2409490836819592, 2.2871015986600694e-11), 'spearman': SpearmanrResult(correlation=0.2846612115178702, pvalue=1.898996325176434e-15), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.02628816737023693, 0.47223054890475913), 'spearman': SpearmanrResult(correlation=0.027820780704932366, pvalue=0.44678853962086973), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4151831368788611, 1.4921344239578373e-20), 'spearman': SpearmanrResult(correlation=0.4968180476299375, pvalue=5.533505946043748e-30), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3317446544465511, 1.0032063632058529e-20), 'spearman': SpearmanrResult(correlation=0.33772936481588484, pvalue=1.82345157177117e-21), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5211017454362185, 3.7083237693002093e-29), 'spearman': SpearmanrResult(correlation=0.4445808091407956, pvalue=9.247175079631886e-21), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.29653809061467057, 'wmean': 0.26006847951259765}, 'spearman': {'mean': 0.31832204276188414, 'wmean': 0.2873506256397472}}}, 'STS13': {'FNWN': {'pearson': (0.08032809067351498, 0.27186438641853555), 'spearman': SpearmanrResult(correlation=0.09358448529240816, pvalue=0.20024574568481457), 'nsamples': 189}, 'headlines': {'pearson': (0.43380046472807676, 9.172066104274262e-36), 'spearman': SpearmanrResult(correlation=0.4199776149214178, pvalue=2.0920561011328347e-33), 'nsamples': 750}, 'OnWN': {'pearson': (0.13251575039350869, 0.0016576021076611669), 'spearman': SpearmanrResult(correlation=0.12878020164661208, pvalue=0.002242013210975531), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.21554810193170015, 'wmean': 0.27658246243607354}, 'spearman': {'mean': 0.21411410062014602, 'wmean': 0.26994424802338524}}}, 'STS14': {'deft-forum': {'pearson': (-0.10685918748572, 0.023389280452958412), 'spearman': SpearmanrResult(correlation=-0.10249423678121661, pvalue=0.029711762598810927), 'nsamples': 450}, 'deft-news': {'pearson': (0.39667887058499035, 9.543606762988496e-13), 'spearman': SpearmanrResult(correlation=0.43498954434205156, pvalue=2.798140144940878e-15), 'nsamples': 300}, 'headlines': {'pearson': (0.4195496061355736, 2.465226360079837e-33), 'spearman': SpearmanrResult(correlation=0.39224966580162096, pvalue=5.389942702717733e-29), 'nsamples': 750}, 'images': {'pearson': (0.1291165177557772, 0.0003926841573323477), 'spearman': SpearmanrResult(correlation=0.13453411132166, pvalue=0.00021985775769272592), 'nsamples': 750}, 'OnWN': {'pearson': (0.335345402595391, 3.612574624985909e-21), 'spearman': SpearmanrResult(correlation=0.3326309377683195, pvalue=7.811864719090602e-21), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4992692162421108, 1.5695201451939058e-48), 'spearman': SpearmanrResult(correlation=0.45535959493842076, pvalue=1.1489133012034523e-39), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.27885007097135384, 'wmean': 0.29556735569428333}, 'spearman': {'mean': 0.27454493623180937, 'wmean': 0.28545471709962233}}}, 'STS15': {'answers-forums': {'pearson': (0.29337201699588356, 7.03652495617096e-09), 'spearman': SpearmanrResult(correlation=0.28949153937233374, pvalue=1.1298507649042734e-08), 'nsamples': 375}, 'answers-students': {'pearson': (0.4097966082498831, 9.74103594340724e-32), 'spearman': SpearmanrResult(correlation=0.4134229297555132, pvalue=2.517925299989329e-32), 'nsamples': 750}, 'belief': {'pearson': (0.4060567288081899, 2.5556036195261646e-16), 'spearman': SpearmanrResult(correlation=0.4135104980540808, pvalue=6.385118508323894e-17), 'nsamples': 375}, 'headlines': {'pearson': (0.4735922859510122, 3.4284973795316332e-43), 'spearman': SpearmanrResult(correlation=0.46613796485881576, pvalue=1.0048894052293287e-41), 'nsamples': 750}, 'images': {'pearson': (0.18761054963263032, 2.274490856210665e-07), 'spearman': SpearmanrResult(correlation=0.194526897078301, pvalue=7.881074129186982e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.35408563792751985, 'wmean': 0.3551784541838906}, 'spearman': {'mean': 0.3554179658238089, 'wmean': 0.35639720260145924}}}, 'STS16': {'answer-answer': {'pearson': (0.29529295482966733, 1.6659059872639162e-06), 'spearman': SpearmanrResult(correlation=0.3039807806397207, pvalue=7.888277338873299e-07), 'nsamples': 254}, 'headlines': {'pearson': (0.5583644502771391, 8.212922828870873e-22), 'spearman': SpearmanrResult(correlation=0.5771491099812783, pvalue=1.6287315421830653e-23), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6251662040481382, 2.4186294868600877e-26), 'spearman': SpearmanrResult(correlation=0.6505106618527603, pvalue=4.6415499502531147e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.7213867140765519, 1.7230696958821009e-40), 'spearman': SpearmanrResult(correlation=0.7442319743813697, pvalue=2.630044927844188e-44), 'nsamples': 244}, 'question-question': {'pearson': (-0.007623951381694058, 0.9127593208688329), 'spearman': SpearmanrResult(correlation=0.03777650171578507, pvalue=0.5870973739543479), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.43851727436996046, 'wmean': 0.4487776880039794}, 'spearman': {'mean': 0.46272980571418276, 'wmean': 0.4721977989052407}}}, 'MR': {'devacc': 78.63, 'acc': 79.3, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.69, 'acc': 84.21, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.06, 'acc': 87.27, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.08, 'acc': 94.85, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.86, 'acc': 82.04, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.14, 'acc': 42.04, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.77, 'acc': 88.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.98, 'acc': 69.68, 'f1': 77.75, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.2, 'acc': 69.92, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6613608896247289, 'pearson': 0.6786826123360956, 'spearman': 0.6358473936506438, 'mse': 0.5519720149382482, 'yhat': array([2.51058448, 4.01781434, 1.87656338, ..., 2.70772311, 4.41204237,
       4.08646382]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.4242663873189794, 'pearson': 0.4290266265485483, 'spearman': 0.4301109170742621, 'mse': 1.9940691059024633, 'yhat': array([4.38696312, 2.19003039, 2.31827114, ..., 3.49858737, 3.29488146,
       3.35040229]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 58.81, 'acc': 57.91, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 26.4, 'acc': 25.76, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 26.01, 'acc': 25.56, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.5, 'acc': 74.39, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.53, 'acc': 84.84, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.16, 'acc': 88.69, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.01, 'acc': 85.38, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 72.96, 'acc': 73.55, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.92, 'acc': 60.29, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.03, 'acc': 70.49, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 04:38:18,126 : ********************************************************************************
2019-02-13 04:38:18,126 : ********************************************************************************
2019-02-13 04:38:18,127 : ********************************************************************************
2019-02-13 04:38:18,127 : layer 11
2019-02-13 04:38:18,127 : ********************************************************************************
2019-02-13 04:38:18,127 : ********************************************************************************
2019-02-13 04:38:18,127 : ********************************************************************************
2019-02-13 04:38:18,214 : ***** Transfer task : STS12 *****


2019-02-13 04:38:18,226 : loading BERT mode bert-base-uncased
2019-02-13 04:38:18,227 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:38:18,244 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:38:18,244 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptb7d4hfb
2019-02-13 04:38:20,683 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:38:25,165 : MSRpar : pearson = 0.2490, spearman = 0.2924
2019-02-13 04:38:27,375 : MSRvid : pearson = -0.0172, spearman = 0.0323
2019-02-13 04:38:28,987 : SMTeuroparl : pearson = 0.4101, spearman = 0.4875
2019-02-13 04:38:31,578 : surprise.OnWN : pearson = 0.3413, spearman = 0.3420
2019-02-13 04:38:33,030 : surprise.SMTnews : pearson = 0.5047, spearman = 0.4478
2019-02-13 04:38:33,030 : ALL (weighted average) : Pearson = 0.2636,             Spearman = 0.2904
2019-02-13 04:38:33,030 : ALL (average) : Pearson = 0.2976,             Spearman = 0.3204

2019-02-13 04:38:33,030 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 04:38:33,040 : loading BERT mode bert-base-uncased
2019-02-13 04:38:33,040 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:38:33,058 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:38:33,058 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp55n105x_
2019-02-13 04:38:35,525 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:38:37,806 : FNWN : pearson = 0.0624, spearman = 0.0766
2019-02-13 04:38:39,705 : headlines : pearson = 0.4510, spearman = 0.4318
2019-02-13 04:38:41,186 : OnWN : pearson = 0.1385, spearman = 0.1465
2019-02-13 04:38:41,186 : ALL (weighted average) : Pearson = 0.2851,             Spearman = 0.2803
2019-02-13 04:38:41,186 : ALL (average) : Pearson = 0.2173,             Spearman = 0.2183

2019-02-13 04:38:41,186 : ***** Transfer task : STS14 *****


2019-02-13 04:38:41,205 : loading BERT mode bert-base-uncased
2019-02-13 04:38:41,205 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:38:41,222 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:38:41,222 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp83ef7qay
2019-02-13 04:38:43,658 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:38:46,265 : deft-forum : pearson = -0.0824, spearman = -0.0821
2019-02-13 04:38:47,356 : deft-news : pearson = 0.4267, spearman = 0.4553
2019-02-13 04:38:49,311 : headlines : pearson = 0.4271, spearman = 0.4005
2019-02-13 04:38:51,310 : images : pearson = 0.1442, spearman = 0.1449
2019-02-13 04:38:53,458 : OnWN : pearson = 0.3534, spearman = 0.3530
2019-02-13 04:38:56,122 : tweet-news : pearson = 0.4855, spearman = 0.4533
2019-02-13 04:38:56,122 : ALL (weighted average) : Pearson = 0.3063,             Spearman = 0.2969
2019-02-13 04:38:56,122 : ALL (average) : Pearson = 0.2924,             Spearman = 0.2875

2019-02-13 04:38:56,123 : ***** Transfer task : STS15 *****


2019-02-13 04:38:56,156 : loading BERT mode bert-base-uncased
2019-02-13 04:38:56,156 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:38:56,174 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:38:56,174 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwa0o6ilv
2019-02-13 04:38:58,607 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:39:00,972 : answers-forums : pearson = 0.3020, spearman = 0.2921
2019-02-13 04:39:02,132 : answers-students : pearson = 0.4124, spearman = 0.4142
2019-02-13 04:39:03,105 : belief : pearson = 0.4281, spearman = 0.4267
2019-02-13 04:39:04,353 : headlines : pearson = 0.4904, spearman = 0.4813
2019-02-13 04:39:05,824 : images : pearson = 0.2079, spearman = 0.2104
2019-02-13 04:39:05,824 : ALL (weighted average) : Pearson = 0.3689,             Spearman = 0.3663
2019-02-13 04:39:05,824 : ALL (average) : Pearson = 0.3682,             Spearman = 0.3649

2019-02-13 04:39:05,824 : ***** Transfer task : STS16 *****


2019-02-13 04:39:05,893 : loading BERT mode bert-base-uncased
2019-02-13 04:39:05,893 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:39:05,911 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:39:05,911 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_kghy_ao
2019-02-13 04:39:08,344 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:39:10,407 : answer-answer : pearson = 0.3400, spearman = 0.3416
2019-02-13 04:39:10,969 : headlines : pearson = 0.5665, spearman = 0.5728
2019-02-13 04:39:11,568 : plagiarism : pearson = 0.6225, spearman = 0.6526
2019-02-13 04:39:12,464 : postediting : pearson = 0.7281, spearman = 0.7504
2019-02-13 04:39:12,996 : question-question : pearson = 0.0320, spearman = 0.0830
2019-02-13 04:39:12,996 : ALL (weighted average) : Pearson = 0.4679,             Spearman = 0.4890
2019-02-13 04:39:12,996 : ALL (average) : Pearson = 0.4578,             Spearman = 0.4801

2019-02-13 04:39:12,996 : ***** Transfer task : MR *****


2019-02-13 04:39:13,016 : loading BERT mode bert-base-uncased
2019-02-13 04:39:13,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:39:13,035 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:39:13,036 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzh_hclrq
2019-02-13 04:39:15,481 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:39:17,011 : Generating sentence embeddings
2019-02-13 04:39:36,652 : Generated sentence embeddings
2019-02-13 04:39:36,652 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:40:02,809 : Best param found at split 1: l2reg = 1e-05                 with score 78.93
2019-02-13 04:40:29,694 : Best param found at split 2: l2reg = 0.01                 with score 78.71
2019-02-13 04:40:56,549 : Best param found at split 3: l2reg = 0.001                 with score 78.83
2019-02-13 04:41:30,315 : Best param found at split 4: l2reg = 0.001                 with score 78.55
2019-02-13 04:41:58,292 : Best param found at split 5: l2reg = 0.0001                 with score 79.52
2019-02-13 04:41:59,930 : Dev acc : 78.91 Test acc : 78.31

2019-02-13 04:41:59,931 : ***** Transfer task : CR *****


2019-02-13 04:41:59,938 : loading BERT mode bert-base-uncased
2019-02-13 04:41:59,938 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:41:59,959 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:41:59,959 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpltjet950
2019-02-13 04:42:02,406 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:42:03,874 : Generating sentence embeddings
2019-02-13 04:42:10,797 : Generated sentence embeddings
2019-02-13 04:42:10,797 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:42:21,595 : Best param found at split 1: l2reg = 1e-05                 with score 85.49
2019-02-13 04:42:31,474 : Best param found at split 2: l2reg = 1e-05                 with score 85.33
2019-02-13 04:42:41,539 : Best param found at split 3: l2reg = 0.001                 with score 86.03
2019-02-13 04:42:50,992 : Best param found at split 4: l2reg = 0.01                 with score 84.74
2019-02-13 04:42:59,761 : Best param found at split 5: l2reg = 1e-05                 with score 85.63
2019-02-13 04:43:00,251 : Dev acc : 85.44 Test acc : 85.09

2019-02-13 04:43:00,252 : ***** Transfer task : MPQA *****


2019-02-13 04:43:00,258 : loading BERT mode bert-base-uncased
2019-02-13 04:43:00,258 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:43:00,277 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:43:00,277 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps6y40kd0
2019-02-13 04:43:02,713 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:43:04,324 : Generating sentence embeddings
2019-02-13 04:43:11,160 : Generated sentence embeddings
2019-02-13 04:43:11,161 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:43:34,766 : Best param found at split 1: l2reg = 0.001                 with score 86.94
2019-02-13 04:43:58,770 : Best param found at split 2: l2reg = 0.01                 with score 87.09
2019-02-13 04:44:23,407 : Best param found at split 3: l2reg = 1e-05                 with score 86.65
2019-02-13 04:44:46,997 : Best param found at split 4: l2reg = 0.01                 with score 87.26
2019-02-13 04:45:11,093 : Best param found at split 5: l2reg = 0.01                 with score 86.95
2019-02-13 04:45:11,977 : Dev acc : 86.98 Test acc : 86.14

2019-02-13 04:45:11,978 : ***** Transfer task : SUBJ *****


2019-02-13 04:45:11,995 : loading BERT mode bert-base-uncased
2019-02-13 04:45:11,995 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:45:12,015 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:45:12,015 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp74kpr01c
2019-02-13 04:45:14,456 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:45:15,931 : Generating sentence embeddings
2019-02-13 04:45:34,997 : Generated sentence embeddings
2019-02-13 04:45:34,997 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:45:56,015 : Best param found at split 1: l2reg = 0.001                 with score 94.98
2019-02-13 04:46:21,263 : Best param found at split 2: l2reg = 1e-05                 with score 95.06
2019-02-13 04:46:57,776 : Best param found at split 3: l2reg = 0.001                 with score 94.96
2019-02-13 04:47:31,490 : Best param found at split 4: l2reg = 0.001                 with score 95.55
2019-02-13 04:47:56,989 : Best param found at split 5: l2reg = 1e-05                 with score 95.05
2019-02-13 04:47:58,292 : Dev acc : 95.12 Test acc : 94.1

2019-02-13 04:47:58,293 : ***** Transfer task : SST Binary classification *****


2019-02-13 04:47:58,419 : loading BERT mode bert-base-uncased
2019-02-13 04:47:58,420 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:47:58,443 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:47:58,443 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpljmkd3wv
2019-02-13 04:48:00,879 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:48:02,392 : Computing embedding for train
2019-02-13 04:49:23,756 : Computed train embeddings
2019-02-13 04:49:23,756 : Computing embedding for dev
2019-02-13 04:49:25,191 : Computed dev embeddings
2019-02-13 04:49:25,191 : Computing embedding for test
2019-02-13 04:49:28,126 : Computed test embeddings
2019-02-13 04:49:28,126 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:50:06,222 : [('reg:1e-05', 83.37), ('reg:0.0001', 83.03), ('reg:0.001', 82.68), ('reg:0.01', 83.03)]
2019-02-13 04:50:06,222 : Validation : best param found is reg = 1e-05 with score             83.37
2019-02-13 04:50:06,222 : Evaluating...
2019-02-13 04:50:20,393 : 
Dev acc : 83.37 Test acc : 83.8 for             SST Binary classification

2019-02-13 04:50:20,399 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 04:50:20,461 : loading BERT mode bert-base-uncased
2019-02-13 04:50:20,462 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:50:20,485 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:50:20,486 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpotexdml6
2019-02-13 04:50:22,963 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:50:24,489 : Computing embedding for train
2019-02-13 04:50:43,393 : Computed train embeddings
2019-02-13 04:50:43,393 : Computing embedding for dev
2019-02-13 04:50:45,846 : Computed dev embeddings
2019-02-13 04:50:45,846 : Computing embedding for test
2019-02-13 04:50:50,955 : Computed test embeddings
2019-02-13 04:50:50,955 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:51:00,927 : [('reg:1e-05', 40.69), ('reg:0.0001', 41.87), ('reg:0.001', 39.6), ('reg:0.01', 38.69)]
2019-02-13 04:51:00,927 : Validation : best param found is reg = 0.0001 with score             41.87
2019-02-13 04:51:00,927 : Evaluating...
2019-02-13 04:51:04,035 : 
Dev acc : 41.87 Test acc : 43.71 for             SST Fine-Grained classification

2019-02-13 04:51:04,036 : ***** Transfer task : TREC *****


2019-02-13 04:51:04,048 : loading BERT mode bert-base-uncased
2019-02-13 04:51:04,048 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:51:04,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:51:04,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_r1t7969
2019-02-13 04:51:06,504 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:51:16,756 : Computed train embeddings
2019-02-13 04:51:17,543 : Computed test embeddings
2019-02-13 04:51:17,544 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 04:51:40,694 : [('reg:1e-05', 80.77), ('reg:0.0001', 80.65), ('reg:0.001', 79.56), ('reg:0.01', 72.03)]
2019-02-13 04:51:40,694 : Cross-validation : best param found is reg = 1e-05             with score 80.77
2019-02-13 04:51:40,694 : Evaluating...
2019-02-13 04:51:41,397 : 
Dev acc : 80.77 Test acc : 87.0             for TREC

2019-02-13 04:51:41,398 : ***** Transfer task : MRPC *****


2019-02-13 04:51:41,418 : loading BERT mode bert-base-uncased
2019-02-13 04:51:41,419 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:51:41,440 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:51:41,440 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp37f05t29
2019-02-13 04:51:43,874 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:51:45,422 : Computing embedding for train
2019-02-13 04:52:00,369 : Computed train embeddings
2019-02-13 04:52:00,369 : Computing embedding for test
2019-02-13 04:52:07,054 : Computed test embeddings
2019-02-13 04:52:07,070 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 04:52:18,449 : [('reg:1e-05', 70.27), ('reg:0.0001', 70.76), ('reg:0.001', 69.87), ('reg:0.01', 70.61)]
2019-02-13 04:52:18,449 : Cross-validation : best param found is reg = 0.0001             with score 70.76
2019-02-13 04:52:18,449 : Evaluating...
2019-02-13 04:52:19,383 : Dev acc : 70.76 Test acc 66.55; Test F1 79.9 for MRPC.

2019-02-13 04:52:19,383 : ***** Transfer task : SICK-Entailment*****


2019-02-13 04:52:19,448 : loading BERT mode bert-base-uncased
2019-02-13 04:52:19,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:52:19,468 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:52:19,468 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgrg6dpb4
2019-02-13 04:52:21,898 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:52:23,373 : Computing embedding for train
2019-02-13 04:52:36,727 : Computed train embeddings
2019-02-13 04:52:36,727 : Computing embedding for dev
2019-02-13 04:52:38,389 : Computed dev embeddings
2019-02-13 04:52:38,389 : Computing embedding for test
2019-02-13 04:52:51,869 : Computed test embeddings
2019-02-13 04:52:51,897 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:52:54,931 : [('reg:1e-05', 71.4), ('reg:0.0001', 72.2), ('reg:0.001', 71.2), ('reg:0.01', 64.4)]
2019-02-13 04:52:54,931 : Validation : best param found is reg = 0.0001 with score             72.2
2019-02-13 04:52:54,931 : Evaluating...
2019-02-13 04:52:55,743 : 
Dev acc : 72.2 Test acc : 71.4 for                        SICK entailment

2019-02-13 04:52:55,744 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 04:52:55,770 : loading BERT mode bert-base-uncased
2019-02-13 04:52:55,770 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:52:55,826 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:52:55,827 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbj5flh4n
2019-02-13 04:52:58,298 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:52:59,764 : Computing embedding for train
2019-02-13 04:53:11,630 : Computed train embeddings
2019-02-13 04:53:11,630 : Computing embedding for dev
2019-02-13 04:53:12,756 : Computed dev embeddings
2019-02-13 04:53:12,756 : Computing embedding for test
2019-02-13 04:53:20,541 : Computed test embeddings
2019-02-13 04:54:05,798 : Dev : Pearson 0.6614904315726032
2019-02-13 04:54:05,802 : Test : Pearson 0.6806312749941023 Spearman 0.6284273875003842 MSE 0.5579650792507292                        for SICK Relatedness

2019-02-13 04:54:05,803 : 

***** Transfer task : STSBenchmark*****


2019-02-13 04:54:05,861 : loading BERT mode bert-base-uncased
2019-02-13 04:54:05,861 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:54:05,880 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:54:05,881 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp35m50lxh
2019-02-13 04:54:08,318 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:54:09,808 : Computing embedding for train
2019-02-13 04:54:26,496 : Computed train embeddings
2019-02-13 04:54:26,496 : Computing embedding for dev
2019-02-13 04:54:31,393 : Computed dev embeddings
2019-02-13 04:54:31,393 : Computing embedding for test
2019-02-13 04:54:35,719 : Computed test embeddings
2019-02-13 04:55:43,025 : Dev : Pearson 0.43988369008862993
2019-02-13 04:55:43,025 : Test : Pearson 0.41027518369570704 Spearman 0.4093809362927808 MSE 2.0883983920712836                        for SICK Relatedness

2019-02-13 04:55:43,026 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 04:55:43,378 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 04:55:43,388 : loading BERT mode bert-base-uncased
2019-02-13 04:55:43,388 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:55:43,413 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:55:43,413 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplfiv9x6_
2019-02-13 04:55:45,868 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:55:47,391 : Computing embeddings for train/dev/test
2019-02-13 04:58:39,135 : Computed embeddings
2019-02-13 04:58:39,135 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:00:16,018 : [('reg:1e-05', 56.28), ('reg:0.0001', 52.9), ('reg:0.001', 50.1), ('reg:0.01', 45.43)]
2019-02-13 05:00:16,018 : Validation : best param found is reg = 1e-05 with score             56.28
2019-02-13 05:00:16,018 : Evaluating...
2019-02-13 05:00:45,062 : 
Dev acc : 56.3 Test acc : 56.0 for LENGTH classification

2019-02-13 05:00:45,068 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 05:00:45,405 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 05:00:45,449 : loading BERT mode bert-base-uncased
2019-02-13 05:00:45,450 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:00:45,547 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:00:45,547 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjug_ng44
2019-02-13 05:00:47,979 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:00:49,439 : Computing embeddings for train/dev/test
2019-02-13 05:03:37,152 : Computed embeddings
2019-02-13 05:03:37,152 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:05:21,650 : [('reg:1e-05', 25.15), ('reg:0.0001', 8.71), ('reg:0.001', 1.61), ('reg:0.01', 0.57)]
2019-02-13 05:05:21,651 : Validation : best param found is reg = 1e-05 with score             25.15
2019-02-13 05:05:21,651 : Evaluating...
2019-02-13 05:05:54,670 : 
Dev acc : 25.1 Test acc : 24.7 for WORDCONTENT classification

2019-02-13 05:05:54,678 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 05:05:55,023 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 05:05:55,088 : loading BERT mode bert-base-uncased
2019-02-13 05:05:55,089 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:05:55,189 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:05:55,189 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzow1pnx5
2019-02-13 05:05:57,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:05:59,073 : Computing embeddings for train/dev/test
2019-02-13 05:08:27,940 : Computed embeddings
2019-02-13 05:08:27,940 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:09:23,195 : [('reg:1e-05', 27.29), ('reg:0.0001', 27.23), ('reg:0.001', 26.26), ('reg:0.01', 23.53)]
2019-02-13 05:09:23,195 : Validation : best param found is reg = 1e-05 with score             27.29
2019-02-13 05:09:23,195 : Evaluating...
2019-02-13 05:09:36,322 : 
Dev acc : 27.3 Test acc : 26.9 for DEPTH classification

2019-02-13 05:09:36,331 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 05:09:36,896 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 05:09:36,960 : loading BERT mode bert-base-uncased
2019-02-13 05:09:36,960 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:09:36,989 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:09:36,990 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2_p17k51
2019-02-13 05:09:39,427 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:09:40,946 : Computing embeddings for train/dev/test
2019-02-13 05:11:55,297 : Computed embeddings
2019-02-13 05:11:55,298 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:13:02,154 : [('reg:1e-05', 72.64), ('reg:0.0001', 72.16), ('reg:0.001', 70.24), ('reg:0.01', 58.78)]
2019-02-13 05:13:02,154 : Validation : best param found is reg = 1e-05 with score             72.64
2019-02-13 05:13:02,154 : Evaluating...
2019-02-13 05:13:21,448 : 
Dev acc : 72.6 Test acc : 73.1 for TOPCONSTITUENTS classification

2019-02-13 05:13:21,456 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 05:13:21,996 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 05:13:22,063 : loading BERT mode bert-base-uncased
2019-02-13 05:13:22,064 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:13:22,094 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:13:22,094 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2oykn4lu
2019-02-13 05:13:24,549 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:13:26,045 : Computing embeddings for train/dev/test
2019-02-13 05:15:56,138 : Computed embeddings
2019-02-13 05:15:56,138 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:16:46,398 : [('reg:1e-05', 84.73), ('reg:0.0001', 84.68), ('reg:0.001', 84.7), ('reg:0.01', 83.59)]
2019-02-13 05:16:46,399 : Validation : best param found is reg = 1e-05 with score             84.73
2019-02-13 05:16:46,399 : Evaluating...
2019-02-13 05:17:01,052 : 
Dev acc : 84.7 Test acc : 84.0 for BIGRAMSHIFT classification

2019-02-13 05:17:01,059 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 05:17:01,477 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 05:17:01,543 : loading BERT mode bert-base-uncased
2019-02-13 05:17:01,544 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:17:01,573 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:17:01,573 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdk2jz8df
2019-02-13 05:17:04,031 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:17:05,601 : Computing embeddings for train/dev/test
2019-02-13 05:19:32,494 : Computed embeddings
2019-02-13 05:19:32,494 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:20:27,070 : [('reg:1e-05', 88.78), ('reg:0.0001', 88.76), ('reg:0.001', 89.24), ('reg:0.01', 89.59)]
2019-02-13 05:20:27,071 : Validation : best param found is reg = 0.01 with score             89.59
2019-02-13 05:20:27,071 : Evaluating...
2019-02-13 05:20:36,939 : 
Dev acc : 89.6 Test acc : 88.7 for TENSE classification

2019-02-13 05:20:36,947 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 05:20:37,363 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 05:20:37,430 : loading BERT mode bert-base-uncased
2019-02-13 05:20:37,430 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:20:37,552 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:20:37,552 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph_bmtco3
2019-02-13 05:20:40,000 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:20:41,446 : Computing embeddings for train/dev/test
2019-02-13 05:23:02,445 : Computed embeddings
2019-02-13 05:23:02,445 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:23:56,268 : [('reg:1e-05', 82.26), ('reg:0.0001', 82.34), ('reg:0.001', 81.45), ('reg:0.01', 81.93)]
2019-02-13 05:23:56,268 : Validation : best param found is reg = 0.0001 with score             82.34
2019-02-13 05:23:56,268 : Evaluating...
2019-02-13 05:24:06,214 : 
Dev acc : 82.3 Test acc : 82.1 for SUBJNUMBER classification

2019-02-13 05:24:06,221 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 05:24:06,835 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 05:24:06,902 : loading BERT mode bert-base-uncased
2019-02-13 05:24:06,903 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:24:06,932 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:24:06,932 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ysycl4s
2019-02-13 05:24:09,382 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:24:10,813 : Computing embeddings for train/dev/test
2019-02-13 05:27:20,569 : Computed embeddings
2019-02-13 05:27:20,569 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:28:06,948 : [('reg:1e-05', 70.09), ('reg:0.0001', 70.03), ('reg:0.001', 69.61), ('reg:0.01', 63.73)]
2019-02-13 05:28:06,948 : Validation : best param found is reg = 1e-05 with score             70.09
2019-02-13 05:28:06,948 : Evaluating...
2019-02-13 05:28:13,983 : 
Dev acc : 70.1 Test acc : 71.2 for OBJNUMBER classification

2019-02-13 05:28:13,990 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 05:28:14,382 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 05:28:14,458 : loading BERT mode bert-base-uncased
2019-02-13 05:28:14,458 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:28:14,595 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:28:14,595 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnn1aa1m7
2019-02-13 05:28:17,081 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:28:18,561 : Computing embeddings for train/dev/test
2019-02-13 05:31:22,637 : Computed embeddings
2019-02-13 05:31:22,637 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:32:22,044 : [('reg:1e-05', 60.46), ('reg:0.0001', 60.48), ('reg:0.001', 60.42), ('reg:0.01', 59.56)]
2019-02-13 05:32:22,044 : Validation : best param found is reg = 0.0001 with score             60.48
2019-02-13 05:32:22,045 : Evaluating...
2019-02-13 05:32:38,110 : 
Dev acc : 60.5 Test acc : 59.8 for ODDMANOUT classification

2019-02-13 05:32:38,118 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 05:32:38,716 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 05:32:38,792 : loading BERT mode bert-base-uncased
2019-02-13 05:32:38,792 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:32:38,824 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:32:38,824 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1bs669j1
2019-02-13 05:32:41,296 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:32:42,734 : Computing embeddings for train/dev/test
2019-02-13 05:35:29,014 : Computed embeddings
2019-02-13 05:35:29,014 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:37:23,046 : [('reg:1e-05', 69.54), ('reg:0.0001', 69.67), ('reg:0.001', 69.47), ('reg:0.01', 68.05)]
2019-02-13 05:37:23,046 : Validation : best param found is reg = 0.0001 with score             69.67
2019-02-13 05:37:23,046 : Evaluating...
2019-02-13 05:37:34,880 : 
Dev acc : 69.7 Test acc : 69.2 for COORDINATIONINVERSION classification

2019-02-13 05:37:34,888 : {'STS12': {'MSRpar': {'pearson': (0.24897112093145368, 4.6399511802918166e-12), 'spearman': SpearmanrResult(correlation=0.29235046206098525, pvalue=3.0283127158216606e-16), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.017167280811295028, 0.6387867772162574), 'spearman': SpearmanrResult(correlation=0.03231417013984031, pvalue=0.3768502316159331), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.41006766245448883, 4.8301450062286876e-20), 'spearman': SpearmanrResult(correlation=0.4874623054038768, pvalue=9.058794315610078e-29), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.34126362016771117, 6.545058953716754e-22), 'spearman': SpearmanrResult(correlation=0.34203476102777663, pvalue=5.22467309092397e-22), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5047334432986987, 3.526538125366472e-27), 'spearman': SpearmanrResult(correlation=0.44782320244588064, pvalue=4.482034295560564e-21), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.29757371320821147, 'wmean': 0.2636455264345861}, 'spearman': {'mean': 0.3203969802156719, 'wmean': 0.2903639642463762}}}, 'STS13': {'FNWN': {'pearson': (0.062393810423665194, 0.3937069793271627), 'spearman': SpearmanrResult(correlation=0.0766435570176691, pvalue=0.2945338692242989), 'nsamples': 189}, 'headlines': {'pearson': (0.45097597585047977, 7.52555822637532e-39), 'spearman': SpearmanrResult(correlation=0.4317594600014223, pvalue=2.077599243950802e-35), 'nsamples': 750}, 'OnWN': {'pearson': (0.13848929883743816, 0.0010064272334124704), 'spearman': SpearmanrResult(correlation=0.14652312636206116, pvalue=0.0004985555978027848), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.21728636170386104, 'wmean': 0.28514460580382356}, 'spearman': {'mean': 0.21830871446038422, 'wmean': 0.2803364674443483}}}, 'STS14': {'deft-forum': {'pearson': (-0.08240180485577009, 0.08079097812224358), 'spearman': SpearmanrResult(correlation=-0.08208005108896498, pvalue=0.08198661916363537), 'nsamples': 450}, 'deft-news': {'pearson': (0.42667966903744536, 1.056661043108385e-14), 'spearman': SpearmanrResult(correlation=0.4553462932834323, pvalue=9.219146902058772e-17), 'nsamples': 300}, 'headlines': {'pearson': (0.4271260026948364, 1.3024025662667834e-34), 'spearman': SpearmanrResult(correlation=0.4005398662349647, pvalue=2.8581261792905352e-30), 'nsamples': 750}, 'images': {'pearson': (0.1441775587302927, 7.41377465769117e-05), 'spearman': SpearmanrResult(correlation=0.1449269765366459, pvalue=6.793130348316458e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.35343496101313754, 1.734291164230278e-23), 'spearman': SpearmanrResult(correlation=0.3529623868229145, pvalue=2.0028175449672187e-23), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4854584202913478, 1.330205326396094e-45), 'spearman': SpearmanrResult(correlation=0.45334046816547474, pvalue=2.739897181289704e-39), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2924124678185483, 'wmean': 0.3062855454862261}, 'spearman': {'mean': 0.28750598999241117, 'wmean': 0.2969320368839988}}}, 'STS15': {'answers-forums': {'pearson': (0.302048270758448, 2.378186981588214e-09), 'spearman': SpearmanrResult(correlation=0.29208193831968227, pvalue=8.242808190641836e-09), 'nsamples': 375}, 'answers-students': {'pearson': (0.4124173737599264, 3.6702281462709573e-32), 'spearman': SpearmanrResult(correlation=0.4141890594389426, pvalue=1.887923830991198e-32), 'nsamples': 750}, 'belief': {'pearson': (0.42808240579073176, 3.8319882056659656e-18), 'spearman': SpearmanrResult(correlation=0.42667596062314, pvalue=5.0575998960079974e-18), 'nsamples': 375}, 'headlines': {'pearson': (0.4903708963830672, 1.252215604125707e-46), 'spearman': SpearmanrResult(correlation=0.4812824612827008, pvalue=9.622326598162417e-45), 'nsamples': 750}, 'images': {'pearson': (0.20788168877003324, 9.112575618196672e-09), 'spearman': SpearmanrResult(correlation=0.21040724778533623, pvalue=5.959907508250104e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3681601270924413, 'wmean': 0.36893382429690413}, 'spearman': {'mean': 0.3649273334899604, 'wmean': 0.3663144294945977}}}, 'STS16': {'answer-answer': {'pearson': (0.33998258746252147, 2.726465844426062e-08), 'spearman': SpearmanrResult(correlation=0.34164614878461075, pvalue=2.308881551231742e-08), 'nsamples': 254}, 'headlines': {'pearson': (0.5665374640923926, 1.5382029615977046e-22), 'spearman': SpearmanrResult(correlation=0.5728356290984573, pvalue=4.0973055034003894e-23), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6224669658796115, 4.553631292240472e-26), 'spearman': SpearmanrResult(correlation=0.6525676973134201, pvalue=2.720938271395754e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.728111483233633, 1.423310098294675e-41), 'spearman': SpearmanrResult(correlation=0.7503892091105759, pvalue=2.0945117265875948e-45), 'nsamples': 244}, 'question-question': {'pearson': (0.031990282591026376, 0.6456438393521091), 'spearman': SpearmanrResult(correlation=0.08295607850377859, pvalue=0.23241970450147348), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.457817756651837, 'wmean': 0.46790554713096777}, 'spearman': {'mean': 0.4800789525621686, 'wmean': 0.4889861309014873}}}, 'MR': {'devacc': 78.91, 'acc': 78.31, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.44, 'acc': 85.09, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.98, 'acc': 86.14, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.12, 'acc': 94.1, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.37, 'acc': 83.8, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.87, 'acc': 43.71, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.77, 'acc': 87.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.76, 'acc': 66.55, 'f1': 79.9, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 72.2, 'acc': 71.4, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6614904315726032, 'pearson': 0.6806312749941023, 'spearman': 0.6284273875003842, 'mse': 0.5579650792507292, 'yhat': array([2.63094702, 4.23390679, 2.10152186, ..., 3.05305561, 4.67553791,
       4.41310092]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.43988369008862993, 'pearson': 0.41027518369570704, 'spearman': 0.4093809362927808, 'mse': 2.0883983920712836, 'yhat': array([4.12377873, 1.73844707, 2.54401346, ..., 3.80695643, 2.88646699,
       3.6986703 ]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 56.28, 'acc': 56.04, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 25.15, 'acc': 24.7, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.29, 'acc': 26.95, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.64, 'acc': 73.11, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 84.73, 'acc': 84.01, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.59, 'acc': 88.73, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.34, 'acc': 82.14, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 70.09, 'acc': 71.25, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.48, 'acc': 59.84, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.67, 'acc': 69.2, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 05:37:34,888 : ********************************************************************************
2019-02-13 05:37:34,888 : ********************************************************************************
2019-02-13 05:37:34,888 : ********************************************************************************
2019-02-13 05:37:34,888 : layer 12
2019-02-13 05:37:34,888 : ********************************************************************************
2019-02-13 05:37:34,888 : ********************************************************************************
2019-02-13 05:37:34,888 : ********************************************************************************
2019-02-13 05:37:34,976 : ***** Transfer task : STS12 *****


2019-02-13 05:37:34,988 : loading BERT mode bert-base-uncased
2019-02-13 05:37:34,988 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:37:35,006 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:37:35,006 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8fndvi4w
2019-02-13 05:37:37,443 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:37:41,114 : MSRpar : pearson = 0.2970, spearman = 0.3495
2019-02-13 05:37:42,326 : MSRvid : pearson = -0.0315, spearman = 0.0273
2019-02-13 05:37:43,255 : SMTeuroparl : pearson = 0.3484, spearman = 0.4575
2019-02-13 05:37:44,885 : surprise.OnWN : pearson = 0.4068, spearman = 0.3982
2019-02-13 05:37:45,763 : surprise.SMTnews : pearson = 0.4595, spearman = 0.4577
2019-02-13 05:37:45,763 : ALL (weighted average) : Pearson = 0.2727,             Spearman = 0.3133
2019-02-13 05:37:45,763 : ALL (average) : Pearson = 0.2961,             Spearman = 0.3380

2019-02-13 05:37:45,763 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 05:37:45,772 : loading BERT mode bert-base-uncased
2019-02-13 05:37:45,772 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:37:45,789 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:37:45,789 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwr6ooj_y
2019-02-13 05:37:48,227 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:37:50,608 : FNWN : pearson = 0.0313, spearman = 0.0416
2019-02-13 05:37:51,933 : headlines : pearson = 0.4751, spearman = 0.4636
2019-02-13 05:37:52,925 : OnWN : pearson = 0.0347, spearman = 0.0616
2019-02-13 05:37:52,925 : ALL (weighted average) : Pearson = 0.2545,             Spearman = 0.2601
2019-02-13 05:37:52,925 : ALL (average) : Pearson = 0.1804,             Spearman = 0.1890

2019-02-13 05:37:52,926 : ***** Transfer task : STS14 *****


2019-02-13 05:37:52,941 : loading BERT mode bert-base-uncased
2019-02-13 05:37:52,941 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:37:52,958 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:37:52,958 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfyy4tx5h
2019-02-13 05:37:55,407 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:37:57,809 : deft-forum : pearson = -0.0140, spearman = -0.0155
2019-02-13 05:37:58,712 : deft-news : pearson = 0.4460, spearman = 0.4791
2019-02-13 05:38:00,150 : headlines : pearson = 0.4369, spearman = 0.4114
2019-02-13 05:38:01,547 : images : pearson = 0.1312, spearman = 0.1509
2019-02-13 05:38:02,968 : OnWN : pearson = 0.2723, spearman = 0.2851
2019-02-13 05:38:04,703 : tweet-news : pearson = 0.3765, spearman = 0.3784
2019-02-13 05:38:04,703 : ALL (weighted average) : Pearson = 0.2774,             Spearman = 0.2816
2019-02-13 05:38:04,703 : ALL (average) : Pearson = 0.2748,             Spearman = 0.2816

2019-02-13 05:38:04,703 : ***** Transfer task : STS15 *****


2019-02-13 05:38:04,769 : loading BERT mode bert-base-uncased
2019-02-13 05:38:04,769 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:38:04,787 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:38:04,787 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwjtrxdse
2019-02-13 05:38:07,223 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:38:09,732 : answers-forums : pearson = 0.2080, spearman = 0.2363
2019-02-13 05:38:11,120 : answers-students : pearson = 0.2880, spearman = 0.3260
2019-02-13 05:38:12,221 : belief : pearson = 0.4258, spearman = 0.4645
2019-02-13 05:38:13,714 : headlines : pearson = 0.5052, spearman = 0.5057
2019-02-13 05:38:15,154 : images : pearson = 0.1966, spearman = 0.2060
2019-02-13 05:38:15,155 : ALL (weighted average) : Pearson = 0.3267,             Spearman = 0.3470
2019-02-13 05:38:15,155 : ALL (average) : Pearson = 0.3247,             Spearman = 0.3477

2019-02-13 05:38:15,155 : ***** Transfer task : STS16 *****


2019-02-13 05:38:15,196 : loading BERT mode bert-base-uncased
2019-02-13 05:38:15,196 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:38:15,246 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:38:15,246 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl29ycsm7
2019-02-13 05:38:17,684 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:38:19,634 : answer-answer : pearson = 0.3938, spearman = 0.4327
2019-02-13 05:38:20,094 : headlines : pearson = 0.5661, spearman = 0.5892
2019-02-13 05:38:20,627 : plagiarism : pearson = 0.6415, spearman = 0.6915
2019-02-13 05:38:21,408 : postediting : pearson = 0.6503, spearman = 0.6819
2019-02-13 05:38:21,815 : question-question : pearson = 0.1060, spearman = 0.2086
2019-02-13 05:38:21,815 : ALL (weighted average) : Pearson = 0.4801,             Spearman = 0.5275
2019-02-13 05:38:21,815 : ALL (average) : Pearson = 0.4715,             Spearman = 0.5208

2019-02-13 05:38:21,815 : ***** Transfer task : MR *****


2019-02-13 05:38:21,831 : loading BERT mode bert-base-uncased
2019-02-13 05:38:21,831 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:38:21,851 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:38:21,851 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9py1l7v3
2019-02-13 05:38:24,299 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:38:25,723 : Generating sentence embeddings
2019-02-13 05:38:46,499 : Generated sentence embeddings
2019-02-13 05:38:46,499 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:39:21,615 : Best param found at split 1: l2reg = 0.0001                 with score 78.67
2019-02-13 05:39:56,648 : Best param found at split 2: l2reg = 0.0001                 with score 79.39
2019-02-13 05:40:34,710 : Best param found at split 3: l2reg = 0.01                 with score 79.61
2019-02-13 05:41:12,326 : Best param found at split 4: l2reg = 0.01                 with score 79.24
2019-02-13 05:41:34,280 : Best param found at split 5: l2reg = 0.01                 with score 79.32
2019-02-13 05:41:35,507 : Dev acc : 79.25 Test acc : 79.23

2019-02-13 05:41:35,508 : ***** Transfer task : CR *****


2019-02-13 05:41:35,516 : loading BERT mode bert-base-uncased
2019-02-13 05:41:35,516 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:41:35,569 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:41:35,569 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl3gsmst_
2019-02-13 05:41:38,005 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:41:39,439 : Generating sentence embeddings
2019-02-13 05:41:44,696 : Generated sentence embeddings
2019-02-13 05:41:44,697 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:41:50,979 : Best param found at split 1: l2reg = 0.0001                 with score 86.16
2019-02-13 05:41:57,257 : Best param found at split 2: l2reg = 0.0001                 with score 85.62
2019-02-13 05:42:03,440 : Best param found at split 3: l2reg = 1e-05                 with score 86.59
2019-02-13 05:42:10,448 : Best param found at split 4: l2reg = 0.0001                 with score 85.5
2019-02-13 05:42:16,375 : Best param found at split 5: l2reg = 1e-05                 with score 85.53
2019-02-13 05:42:16,677 : Dev acc : 85.88 Test acc : 84.74

2019-02-13 05:42:16,678 : ***** Transfer task : MPQA *****


2019-02-13 05:42:16,683 : loading BERT mode bert-base-uncased
2019-02-13 05:42:16,684 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:42:16,703 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:42:16,703 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpusmvhckr
2019-02-13 05:42:19,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:42:20,615 : Generating sentence embeddings
2019-02-13 05:42:27,275 : Generated sentence embeddings
2019-02-13 05:42:27,275 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:42:50,110 : Best param found at split 1: l2reg = 1e-05                 with score 86.69
2019-02-13 05:43:28,352 : Best param found at split 2: l2reg = 0.0001                 with score 86.53
2019-02-13 05:44:14,496 : Best param found at split 3: l2reg = 1e-05                 with score 86.56
2019-02-13 05:44:57,830 : Best param found at split 4: l2reg = 0.0001                 with score 87.14
2019-02-13 05:45:27,151 : Best param found at split 5: l2reg = 0.001                 with score 86.6
2019-02-13 05:45:28,277 : Dev acc : 86.7 Test acc : 87.57

2019-02-13 05:45:28,278 : ***** Transfer task : SUBJ *****


2019-02-13 05:45:28,294 : loading BERT mode bert-base-uncased
2019-02-13 05:45:28,294 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:45:28,314 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:45:28,315 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpys4dufxc
2019-02-13 05:45:30,761 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:45:32,204 : Generating sentence embeddings
2019-02-13 05:45:48,709 : Generated sentence embeddings
2019-02-13 05:45:48,709 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:46:01,524 : Best param found at split 1: l2reg = 0.001                 with score 95.35
2019-02-13 05:46:16,924 : Best param found at split 2: l2reg = 1e-05                 with score 95.14
2019-02-13 05:46:42,104 : Best param found at split 3: l2reg = 0.001                 with score 95.43
2019-02-13 05:47:15,964 : Best param found at split 4: l2reg = 1e-05                 with score 95.6
2019-02-13 05:47:54,823 : Best param found at split 5: l2reg = 0.001                 with score 95.1
2019-02-13 05:47:56,554 : Dev acc : 95.32 Test acc : 94.7

2019-02-13 05:47:56,555 : ***** Transfer task : SST Binary classification *****


2019-02-13 05:47:56,685 : loading BERT mode bert-base-uncased
2019-02-13 05:47:56,685 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:47:56,708 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:47:56,708 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8slomzg1
2019-02-13 05:47:59,140 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:48:00,684 : Computing embedding for train
2019-02-13 05:49:42,756 : Computed train embeddings
2019-02-13 05:49:42,756 : Computing embedding for dev
2019-02-13 05:49:44,016 : Computed dev embeddings
2019-02-13 05:49:44,016 : Computing embedding for test
2019-02-13 05:49:46,633 : Computed test embeddings
2019-02-13 05:49:46,633 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:50:16,937 : [('reg:1e-05', 82.45), ('reg:0.0001', 82.68), ('reg:0.001', 81.88), ('reg:0.01', 83.14)]
2019-02-13 05:50:16,938 : Validation : best param found is reg = 0.01 with score             83.14
2019-02-13 05:50:16,938 : Evaluating...
2019-02-13 05:50:24,650 : 
Dev acc : 83.14 Test acc : 83.2 for             SST Binary classification

2019-02-13 05:50:24,650 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 05:50:24,703 : loading BERT mode bert-base-uncased
2019-02-13 05:50:24,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:50:24,758 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:50:24,759 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4sjiq9p_
2019-02-13 05:50:27,194 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:50:28,685 : Computing embedding for train
2019-02-13 05:50:41,852 : Computed train embeddings
2019-02-13 05:50:41,852 : Computing embedding for dev
2019-02-13 05:50:44,061 : Computed dev embeddings
2019-02-13 05:50:44,061 : Computing embedding for test
2019-02-13 05:50:48,426 : Computed test embeddings
2019-02-13 05:50:48,426 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:50:57,207 : [('reg:1e-05', 41.05), ('reg:0.0001', 41.51), ('reg:0.001', 41.51), ('reg:0.01', 43.69)]
2019-02-13 05:50:57,208 : Validation : best param found is reg = 0.01 with score             43.69
2019-02-13 05:50:57,208 : Evaluating...
2019-02-13 05:50:59,466 : 
Dev acc : 43.69 Test acc : 43.21 for             SST Fine-Grained classification

2019-02-13 05:50:59,467 : ***** Transfer task : TREC *****


2019-02-13 05:50:59,479 : loading BERT mode bert-base-uncased
2019-02-13 05:50:59,479 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:50:59,499 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:50:59,499 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpixjsnnvm
2019-02-13 05:51:01,935 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:51:12,543 : Computed train embeddings
2019-02-13 05:51:13,388 : Computed test embeddings
2019-02-13 05:51:13,388 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 05:51:39,424 : [('reg:1e-05', 81.44), ('reg:0.0001', 81.33), ('reg:0.001', 79.23), ('reg:0.01', 69.3)]
2019-02-13 05:51:39,425 : Cross-validation : best param found is reg = 1e-05             with score 81.44
2019-02-13 05:51:39,425 : Evaluating...
2019-02-13 05:51:40,971 : 
Dev acc : 81.44 Test acc : 87.0             for TREC

2019-02-13 05:51:40,971 : ***** Transfer task : MRPC *****


2019-02-13 05:51:41,022 : loading BERT mode bert-base-uncased
2019-02-13 05:51:41,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:51:41,045 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:51:41,045 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprii46550
2019-02-13 05:51:43,483 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:51:45,086 : Computing embedding for train
2019-02-13 05:52:04,972 : Computed train embeddings
2019-02-13 05:52:04,973 : Computing embedding for test
2019-02-13 05:52:14,011 : Computed test embeddings
2019-02-13 05:52:14,027 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 05:52:32,550 : [('reg:1e-05', 69.95), ('reg:0.0001', 70.24), ('reg:0.001', 70.41), ('reg:0.01', 70.78)]
2019-02-13 05:52:32,550 : Cross-validation : best param found is reg = 0.01             with score 70.78
2019-02-13 05:52:32,550 : Evaluating...
2019-02-13 05:52:33,570 : Dev acc : 70.78 Test acc 69.74; Test F1 78.52 for MRPC.

2019-02-13 05:52:33,570 : ***** Transfer task : SICK-Entailment*****


2019-02-13 05:52:33,594 : loading BERT mode bert-base-uncased
2019-02-13 05:52:33,594 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:52:33,649 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:52:33,649 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5ula0hae
2019-02-13 05:52:36,116 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:52:37,692 : Computing embedding for train
2019-02-13 05:52:54,857 : Computed train embeddings
2019-02-13 05:52:54,857 : Computing embedding for dev
2019-02-13 05:52:56,499 : Computed dev embeddings
2019-02-13 05:52:56,499 : Computing embedding for test
2019-02-13 05:53:11,325 : Computed test embeddings
2019-02-13 05:53:11,353 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:53:15,383 : [('reg:1e-05', 71.6), ('reg:0.0001', 72.0), ('reg:0.001', 72.2), ('reg:0.01', 66.8)]
2019-02-13 05:53:15,383 : Validation : best param found is reg = 0.001 with score             72.2
2019-02-13 05:53:15,383 : Evaluating...
2019-02-13 05:53:16,144 : 
Dev acc : 72.2 Test acc : 68.56 for                        SICK entailment

2019-02-13 05:53:16,144 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 05:53:16,171 : loading BERT mode bert-base-uncased
2019-02-13 05:53:16,171 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:53:16,190 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:53:16,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7t3ypomv
2019-02-13 05:53:18,627 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:53:20,076 : Computing embedding for train
2019-02-13 05:53:32,020 : Computed train embeddings
2019-02-13 05:53:32,020 : Computing embedding for dev
2019-02-13 05:53:33,203 : Computed dev embeddings
2019-02-13 05:53:33,203 : Computing embedding for test
2019-02-13 05:53:45,900 : Computed test embeddings
2019-02-13 05:54:15,466 : Dev : Pearson 0.6809848077441052
2019-02-13 05:54:15,466 : Test : Pearson 0.6945593971919223 Spearman 0.6415164105592593 MSE 0.5267593878003589                        for SICK Relatedness

2019-02-13 05:54:15,467 : 

***** Transfer task : STSBenchmark*****


2019-02-13 05:54:15,552 : loading BERT mode bert-base-uncased
2019-02-13 05:54:15,552 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:54:15,572 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:54:15,572 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppdnd9np_
2019-02-13 05:54:18,021 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:54:19,464 : Computing embedding for train
2019-02-13 05:54:32,771 : Computed train embeddings
2019-02-13 05:54:32,771 : Computing embedding for dev
2019-02-13 05:54:36,645 : Computed dev embeddings
2019-02-13 05:54:36,645 : Computing embedding for test
2019-02-13 05:54:40,038 : Computed test embeddings
2019-02-13 05:55:55,095 : Dev : Pearson 0.5469116428416618
2019-02-13 05:55:55,095 : Test : Pearson 0.4650535822098155 Spearman 0.46686339421803735 MSE 2.0349728254746275                        for SICK Relatedness

2019-02-13 05:55:55,095 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 05:55:55,345 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 05:55:55,355 : loading BERT mode bert-base-uncased
2019-02-13 05:55:55,355 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:55:55,452 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:55:55,452 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbk3oqqm6
2019-02-13 05:55:57,920 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:55:59,464 : Computing embeddings for train/dev/test
2019-02-13 05:59:04,164 : Computed embeddings
2019-02-13 05:59:04,165 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:00:26,128 : [('reg:1e-05', 53.52), ('reg:0.0001', 53.35), ('reg:0.001', 47.9), ('reg:0.01', 40.88)]
2019-02-13 06:00:26,128 : Validation : best param found is reg = 1e-05 with score             53.52
2019-02-13 06:00:26,129 : Evaluating...
2019-02-13 06:00:56,225 : 
Dev acc : 53.5 Test acc : 54.9 for LENGTH classification

2019-02-13 06:00:56,232 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 06:00:56,579 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 06:00:56,623 : loading BERT mode bert-base-uncased
2019-02-13 06:00:56,623 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:00:56,713 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:00:56,713 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpndhgdad7
2019-02-13 06:00:59,147 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:01:00,692 : Computing embeddings for train/dev/test
2019-02-13 06:03:55,909 : Computed embeddings
2019-02-13 06:03:55,909 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:06:05,595 : [('reg:1e-05', 28.89), ('reg:0.0001', 8.82), ('reg:0.001', 1.36), ('reg:0.01', 0.54)]
2019-02-13 06:06:05,595 : Validation : best param found is reg = 1e-05 with score             28.89
2019-02-13 06:06:05,595 : Evaluating...
2019-02-13 06:06:37,459 : 
Dev acc : 28.9 Test acc : 28.5 for WORDCONTENT classification

2019-02-13 06:06:37,461 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 06:06:37,838 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 06:06:37,903 : loading BERT mode bert-base-uncased
2019-02-13 06:06:37,903 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:06:37,928 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:06:37,928 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptmz0c5ea
2019-02-13 06:06:40,394 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:06:41,850 : Computing embeddings for train/dev/test
2019-02-13 06:09:07,212 : Computed embeddings
2019-02-13 06:09:07,212 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:10:09,919 : [('reg:1e-05', 27.16), ('reg:0.0001', 26.92), ('reg:0.001', 25.68), ('reg:0.01', 22.78)]
2019-02-13 06:10:09,919 : Validation : best param found is reg = 1e-05 with score             27.16
2019-02-13 06:10:09,919 : Evaluating...
2019-02-13 06:10:29,741 : 
Dev acc : 27.2 Test acc : 27.5 for DEPTH classification

2019-02-13 06:10:29,742 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 06:10:30,126 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 06:10:30,189 : loading BERT mode bert-base-uncased
2019-02-13 06:10:30,189 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:10:30,300 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:10:30,300 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpncy5sk83
2019-02-13 06:10:32,731 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:10:34,199 : Computing embeddings for train/dev/test
2019-02-13 06:12:40,708 : Computed embeddings
2019-02-13 06:12:40,709 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:13:53,227 : [('reg:1e-05', 69.81), ('reg:0.0001', 69.85), ('reg:0.001', 64.36), ('reg:0.01', 53.66)]
2019-02-13 06:13:53,227 : Validation : best param found is reg = 0.0001 with score             69.85
2019-02-13 06:13:53,227 : Evaluating...
2019-02-13 06:14:13,598 : 
Dev acc : 69.8 Test acc : 69.1 for TOPCONSTITUENTS classification

2019-02-13 06:14:13,599 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 06:14:14,139 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 06:14:14,204 : loading BERT mode bert-base-uncased
2019-02-13 06:14:14,205 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:14:14,236 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:14:14,236 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp1_l_lfm
2019-02-13 06:14:16,693 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:14:18,253 : Computing embeddings for train/dev/test
2019-02-13 06:16:22,711 : Computed embeddings
2019-02-13 06:16:22,711 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:17:06,281 : [('reg:1e-05', 85.46), ('reg:0.0001', 85.49), ('reg:0.001', 84.95), ('reg:0.01', 83.17)]
2019-02-13 06:17:06,281 : Validation : best param found is reg = 0.0001 with score             85.49
2019-02-13 06:17:06,281 : Evaluating...
2019-02-13 06:17:14,986 : 
Dev acc : 85.5 Test acc : 84.8 for BIGRAMSHIFT classification

2019-02-13 06:17:14,987 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 06:17:15,361 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 06:17:15,427 : loading BERT mode bert-base-uncased
2019-02-13 06:17:15,428 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:17:15,545 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:17:15,545 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprjfqho8k
2019-02-13 06:17:17,987 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:17:19,403 : Computing embeddings for train/dev/test
2019-02-13 06:18:55,926 : Computed embeddings
2019-02-13 06:18:55,926 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:19:51,974 : [('reg:1e-05', 90.24), ('reg:0.0001', 90.29), ('reg:0.001', 90.32), ('reg:0.01', 90.19)]
2019-02-13 06:19:51,974 : Validation : best param found is reg = 0.001 with score             90.32
2019-02-13 06:19:51,974 : Evaluating...
2019-02-13 06:19:58,911 : 
Dev acc : 90.3 Test acc : 89.0 for TENSE classification

2019-02-13 06:19:58,912 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 06:19:59,493 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 06:19:59,556 : loading BERT mode bert-base-uncased
2019-02-13 06:19:59,556 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:19:59,585 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:19:59,585 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqt7mmv0_
2019-02-13 06:20:02,024 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:20:03,428 : Computing embeddings for train/dev/test
2019-02-13 06:21:58,749 : Computed embeddings
2019-02-13 06:21:58,749 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:22:35,988 : [('reg:1e-05', 80.68), ('reg:0.0001', 80.74), ('reg:0.001', 81.43), ('reg:0.01', 79.85)]
2019-02-13 06:22:35,988 : Validation : best param found is reg = 0.001 with score             81.43
2019-02-13 06:22:35,988 : Evaluating...
2019-02-13 06:22:43,749 : 
Dev acc : 81.4 Test acc : 81.3 for SUBJNUMBER classification

2019-02-13 06:22:43,750 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 06:22:44,333 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 06:22:44,400 : loading BERT mode bert-base-uncased
2019-02-13 06:22:44,400 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:22:44,428 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:22:44,429 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg5sz134y
2019-02-13 06:22:46,871 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:22:48,296 : Computing embeddings for train/dev/test
2019-02-13 06:24:36,717 : Computed embeddings
2019-02-13 06:24:36,717 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:25:39,745 : [('reg:1e-05', 72.84), ('reg:0.0001', 72.84), ('reg:0.001', 73.08), ('reg:0.01', 66.8)]
2019-02-13 06:25:39,745 : Validation : best param found is reg = 0.001 with score             73.08
2019-02-13 06:25:39,745 : Evaluating...
2019-02-13 06:25:53,322 : 
Dev acc : 73.1 Test acc : 74.4 for OBJNUMBER classification

2019-02-13 06:25:53,323 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 06:25:53,744 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 06:25:53,815 : loading BERT mode bert-base-uncased
2019-02-13 06:25:53,815 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:25:53,843 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:25:53,843 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzi1kpjla
2019-02-13 06:25:56,285 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:25:57,789 : Computing embeddings for train/dev/test
2019-02-13 06:27:54,429 : Computed embeddings
2019-02-13 06:27:54,429 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:29:01,551 : [('reg:1e-05', 62.11), ('reg:0.0001', 62.12), ('reg:0.001', 61.66), ('reg:0.01', 60.82)]
2019-02-13 06:29:01,551 : Validation : best param found is reg = 0.0001 with score             62.12
2019-02-13 06:29:01,551 : Evaluating...
2019-02-13 06:29:21,146 : 
Dev acc : 62.1 Test acc : 62.2 for ODDMANOUT classification

2019-02-13 06:29:21,147 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 06:29:21,558 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 06:29:21,636 : loading BERT mode bert-base-uncased
2019-02-13 06:29:21,636 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:29:21,762 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:29:21,762 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphv8wxxs7
2019-02-13 06:29:24,204 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:29:25,711 : Computing embeddings for train/dev/test
2019-02-13 06:31:26,856 : Computed embeddings
2019-02-13 06:31:26,856 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:32:39,790 : [('reg:1e-05', 69.45), ('reg:0.0001', 69.34), ('reg:0.001', 69.31), ('reg:0.01', 60.04)]
2019-02-13 06:32:39,790 : Validation : best param found is reg = 1e-05 with score             69.45
2019-02-13 06:32:39,790 : Evaluating...
2019-02-13 06:32:57,630 : 
Dev acc : 69.5 Test acc : 69.4 for COORDINATIONINVERSION classification

2019-02-13 06:32:57,632 : {'STS12': {'MSRpar': {'pearson': (0.29696611181983773, 9.789745050444839e-17), 'spearman': SpearmanrResult(correlation=0.34952268284469634, pvalue=5.669177216148451e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.03147126639491536, 0.3894298724184839), 'spearman': SpearmanrResult(correlation=0.027273368876600593, pvalue=0.45578528347759606), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.34842014570280677, 1.512572831761387e-14), 'spearman': SpearmanrResult(correlation=0.45746496632984246, pvalue=4.0439218050821923e-25), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4068001994080315, 2.9423100526868423e-31), 'spearman': SpearmanrResult(correlation=0.3982428548476992, pvalue=6.503119130486331e-30), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4595496294903302, 3.05801078360474e-22), 'spearman': SpearmanrResult(correlation=0.4576865015715348, pvalue=4.717647094321368e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.29605296400521813, 'wmean': 0.27268546739670063}, 'spearman': {'mean': 0.33803807489407467, 'wmean': 0.3133437945943331}}}, 'STS13': {'FNWN': {'pearson': (0.03131713509440084, 0.6688053291486091), 'spearman': SpearmanrResult(correlation=0.041586571128491734, pvalue=0.5699168710967685), 'nsamples': 189}, 'headlines': {'pearson': (0.47512121071712954, 1.6971218780118689e-43), 'spearman': SpearmanrResult(correlation=0.4636290347968956, pvalue=3.0746186358902073e-41), 'nsamples': 750}, 'OnWN': {'pearson': (0.03467525115630963, 0.41237963035070524), 'spearman': SpearmanrResult(correlation=0.061638402826436185, pvalue=0.14482319515855174), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.18037119898928, 'wmean': 0.2544751083129191}, 'spearman': {'mean': 0.18895133625060787, 'wmean': 0.2601071880177249}}}, 'STS14': {'deft-forum': {'pearson': (-0.014009338782729584, 0.7669468206482611), 'spearman': SpearmanrResult(correlation=-0.015481846691131099, pvalue=0.7432702707074741), 'nsamples': 450}, 'deft-news': {'pearson': (0.4460124349488527, 4.535321266677203e-16), 'spearman': SpearmanrResult(correlation=0.4790685685228266, pvalue=1.2820343303813202e-18), 'nsamples': 300}, 'headlines': {'pearson': (0.4368649427931535, 2.6591299551602905e-36), 'spearman': SpearmanrResult(correlation=0.41135166249681226, pvalue=5.464204430758958e-32), 'nsamples': 750}, 'images': {'pearson': (0.1311918517659196, 0.0003152623145532143), 'spearman': SpearmanrResult(correlation=0.1508690600162816, pvalue=3.3452022175758596e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.27229924964290747, 3.2328696544609047e-14), 'spearman': SpearmanrResult(correlation=0.28509114387847023, pvalue=1.7162778233833135e-15), 'nsamples': 750}, 'tweet-news': {'pearson': (0.3764645614485085, 1.153110859594805e-26), 'spearman': SpearmanrResult(correlation=0.37843615396517494, pvalue=5.994202282581746e-27), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2748039503027687, 'wmean': 0.27736399527207845}, 'spearman': {'mean': 0.28155579036473904, 'wmean': 0.28161726795023817}}}, 'STS15': {'answers-forums': {'pearson': (0.20796222365792594, 4.945652961217275e-05), 'spearman': SpearmanrResult(correlation=0.23625017820655794, pvalue=3.740037018000821e-06), 'nsamples': 375}, 'answers-students': {'pearson': (0.28804021378631395, 8.534100857034731e-16), 'spearman': SpearmanrResult(correlation=0.3260434103215193, pvalue=4.917728877651064e-20), 'nsamples': 750}, 'belief': {'pearson': (0.4257999694257162, 6.00793665059707e-18), 'spearman': SpearmanrResult(correlation=0.46447796639784555, pvalue=1.826169334774916e-21), 'nsamples': 375}, 'headlines': {'pearson': (0.5052043117790275, 7.864880530700966e-50), 'spearman': SpearmanrResult(correlation=0.5057454645278043, pvalue=5.968650296485303e-50), 'nsamples': 750}, 'images': {'pearson': (0.19656566244645005, 5.723626765434872e-08), 'spearman': SpearmanrResult(correlation=0.20602717287533573, pvalue=1.2404458117127073e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.32471447621908667, 'wmean': 0.3266728211384031}, 'spearman': {'mean': 0.3477088384658126, 'wmean': 0.34704503000671527}}}, 'STS16': {'answer-answer': {'pearson': (0.39376091541705005, 7.543406124119522e-11), 'spearman': SpearmanrResult(correlation=0.432681566992628, pvalue=5.182696900660818e-13), 'nsamples': 254}, 'headlines': {'pearson': (0.5660811728348878, 1.691066671302775e-22), 'spearman': SpearmanrResult(correlation=0.5891884647535829, pvalue=1.1526673810240406e-24), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6414668442853152, 4.6287733863116365e-28), 'spearman': SpearmanrResult(correlation=0.6915045830995717, pvalue=4.766071043217876e-34), 'nsamples': 230}, 'postediting': {'pearson': (0.6503123854886803, 1.0113027053858838e-30), 'spearman': SpearmanrResult(correlation=0.6819206583818435, pvalue=1.0071235594923312e-34), 'nsamples': 244}, 'question-question': {'pearson': (0.10602713138871137, 0.12652902144955758), 'spearman': SpearmanrResult(correlation=0.20856669544015055, pvalue=0.0024412575976705347), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.47152968988292904, 'wmean': 0.4800529099974022}, 'spearman': {'mean': 0.5207723937335553, 'wmean': 0.5275161718759127}}}, 'MR': {'devacc': 79.25, 'acc': 79.23, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.88, 'acc': 84.74, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.7, 'acc': 87.57, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.32, 'acc': 94.7, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.14, 'acc': 83.2, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.69, 'acc': 43.21, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 81.44, 'acc': 87.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.78, 'acc': 69.74, 'f1': 78.52, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 72.2, 'acc': 68.56, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6809848077441052, 'pearson': 0.6945593971919223, 'spearman': 0.6415164105592593, 'mse': 0.5267593878003589, 'yhat': array([3.63630735, 4.11016887, 2.73366217, ..., 3.20377216, 4.51483387,
       4.22536561]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5469116428416618, 'pearson': 0.4650535822098155, 'spearman': 0.46686339421803735, 'mse': 2.0349728254746275, 'yhat': array([3.01103473, 1.81525694, 3.21587664, ..., 3.84357727, 2.82794652,
       4.51361995]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 53.52, 'acc': 54.9, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 28.89, 'acc': 28.52, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.16, 'acc': 27.54, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 69.85, 'acc': 69.12, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.49, 'acc': 84.82, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.32, 'acc': 89.01, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.43, 'acc': 81.33, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 73.08, 'acc': 74.38, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.12, 'acc': 62.2, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.45, 'acc': 69.44, 'ndev': 10002, 'ntest': 10002}}
