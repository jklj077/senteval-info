2019-02-12 17:49:59,882 : ********************************************************************************
2019-02-12 17:49:59,882 : ********************************************************************************
2019-02-12 17:49:59,882 : ********************************************************************************
2019-02-12 17:49:59,882 : layer 0
2019-02-12 17:49:59,883 : ********************************************************************************
2019-02-12 17:49:59,883 : ********************************************************************************
2019-02-12 17:49:59,883 : ********************************************************************************
2019-02-12 17:49:59,883 : ***** Transfer task : STS12 *****


2019-02-12 17:49:59,917 : loading BERT mode bert-base-uncased
2019-02-12 17:49:59,918 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:49:59,935 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:49:59,936 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1w7_nw8v
2019-02-12 17:50:02,350 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:50:09,785 : MSRpar : pearson = 0.3923, spearman = 0.4113
2019-02-12 17:50:10,953 : MSRvid : pearson = 0.7107, spearman = 0.7137
2019-02-12 17:50:11,859 : SMTeuroparl : pearson = 0.4897, spearman = 0.5861
2019-02-12 17:50:13,447 : surprise.OnWN : pearson = 0.6185, spearman = 0.6561
2019-02-12 17:50:14,322 : surprise.SMTnews : pearson = 0.4909, spearman = 0.4206
2019-02-12 17:50:14,322 : ALL (weighted average) : Pearson = 0.5508,             Spearman = 0.5704
2019-02-12 17:50:14,322 : ALL (average) : Pearson = 0.5404,             Spearman = 0.5576

2019-02-12 17:50:14,322 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 17:50:14,331 : loading BERT mode bert-base-uncased
2019-02-12 17:50:14,331 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:50:14,349 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:50:14,349 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyj6_jc8i
2019-02-12 17:50:16,815 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:50:18,980 : FNWN : pearson = 0.2444, spearman = 0.2393
2019-02-12 17:50:20,047 : headlines : pearson = 0.6611, spearman = 0.6567
2019-02-12 17:50:20,931 : OnWN : pearson = 0.5890, spearman = 0.5899
2019-02-12 17:50:20,932 : ALL (weighted average) : Pearson = 0.5816,             Spearman = 0.5791
2019-02-12 17:50:20,932 : ALL (average) : Pearson = 0.4982,             Spearman = 0.4953

2019-02-12 17:50:20,932 : ***** Transfer task : STS14 *****


2019-02-12 17:50:20,950 : loading BERT mode bert-base-uncased
2019-02-12 17:50:20,950 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:50:20,999 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:50:20,999 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_hf1xsa6
2019-02-12 17:50:23,414 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:50:26,046 : deft-forum : pearson = 0.3605, spearman = 0.3567
2019-02-12 17:50:27,124 : deft-news : pearson = 0.6842, spearman = 0.6511
2019-02-12 17:50:29,078 : headlines : pearson = 0.6246, spearman = 0.6106
2019-02-12 17:50:30,772 : images : pearson = 0.6964, spearman = 0.6920
2019-02-12 17:50:32,409 : OnWN : pearson = 0.6505, spearman = 0.6802
2019-02-12 17:50:34,437 : tweet-news : pearson = 0.5959, spearman = 0.5888
2019-02-12 17:50:34,437 : ALL (weighted average) : Pearson = 0.6115,             Spearman = 0.6092
2019-02-12 17:50:34,437 : ALL (average) : Pearson = 0.6020,             Spearman = 0.5966

2019-02-12 17:50:34,437 : ***** Transfer task : STS15 *****


2019-02-12 17:50:34,470 : loading BERT mode bert-base-uncased
2019-02-12 17:50:34,470 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:50:34,488 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:50:34,488 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgdrjxrq5
2019-02-12 17:50:36,901 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:50:39,563 : answers-forums : pearson = 0.5416, spearman = 0.5191
2019-02-12 17:50:40,943 : answers-students : pearson = 0.6904, spearman = 0.6997
2019-02-12 17:50:42,023 : belief : pearson = 0.5498, spearman = 0.5968
2019-02-12 17:50:43,467 : headlines : pearson = 0.6513, spearman = 0.6541
2019-02-12 17:50:44,652 : images : pearson = 0.7736, spearman = 0.7847
2019-02-12 17:50:44,652 : ALL (weighted average) : Pearson = 0.6652,             Spearman = 0.6741
2019-02-12 17:50:44,652 : ALL (average) : Pearson = 0.6413,             Spearman = 0.6509

2019-02-12 17:50:44,652 : ***** Transfer task : STS16 *****


2019-02-12 17:50:44,720 : loading BERT mode bert-base-uncased
2019-02-12 17:50:44,720 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:50:44,737 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:50:44,737 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2t3l_g2q
2019-02-12 17:50:47,169 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:50:49,277 : answer-answer : pearson = 0.4273, spearman = 0.4269
2019-02-12 17:50:49,852 : headlines : pearson = 0.6523, spearman = 0.6671
2019-02-12 17:50:50,493 : plagiarism : pearson = 0.6856, spearman = 0.6913
2019-02-12 17:50:51,379 : postediting : pearson = 0.7946, spearman = 0.8281
2019-02-12 17:50:51,974 : question-question : pearson = 0.6301, spearman = 0.6317
2019-02-12 17:50:51,974 : ALL (weighted average) : Pearson = 0.6359,             Spearman = 0.6472
2019-02-12 17:50:51,974 : ALL (average) : Pearson = 0.6380,             Spearman = 0.6490

2019-02-12 17:50:51,974 : ***** Transfer task : MR *****


2019-02-12 17:50:51,992 : loading BERT mode bert-base-uncased
2019-02-12 17:50:51,992 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:50:52,011 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:50:52,012 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpez3gkoma
2019-02-12 17:50:54,430 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:50:55,942 : Generating sentence embeddings
2019-02-12 17:51:14,090 : Generated sentence embeddings
2019-02-12 17:51:14,090 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:51:29,081 : Best param found at split 1: l2reg = 1e-05                 with score 51.25
2019-02-12 17:51:42,800 : Best param found at split 2: l2reg = 0.01                 with score 50.28
2019-02-12 17:52:02,837 : Best param found at split 3: l2reg = 0.001                 with score 52.52
2019-02-12 17:52:17,600 : Best param found at split 4: l2reg = 0.0001                 with score 52.42
2019-02-12 17:52:32,060 : Best param found at split 5: l2reg = 1e-05                 with score 51.35
2019-02-12 17:52:33,008 : Dev acc : 51.56 Test acc : 50.32

2019-02-12 17:52:33,009 : ***** Transfer task : CR *****


2019-02-12 17:52:33,017 : loading BERT mode bert-base-uncased
2019-02-12 17:52:33,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:52:33,037 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:52:33,037 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj6gz9kx0
2019-02-12 17:52:35,476 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:52:36,900 : Generating sentence embeddings
2019-02-12 17:52:41,540 : Generated sentence embeddings
2019-02-12 17:52:41,540 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:52:45,504 : Best param found at split 1: l2reg = 0.001                 with score 66.08
2019-02-12 17:52:50,344 : Best param found at split 2: l2reg = 0.001                 with score 64.89
2019-02-12 17:52:57,648 : Best param found at split 3: l2reg = 0.001                 with score 66.19
2019-02-12 17:53:05,303 : Best param found at split 4: l2reg = 0.0001                 with score 66.1
2019-02-12 17:53:13,445 : Best param found at split 5: l2reg = 0.001                 with score 67.53
2019-02-12 17:53:13,904 : Dev acc : 66.16 Test acc : 65.11

2019-02-12 17:53:13,904 : ***** Transfer task : MPQA *****


2019-02-12 17:53:13,910 : loading BERT mode bert-base-uncased
2019-02-12 17:53:13,910 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:53:13,929 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:53:13,929 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp881ddh2s
2019-02-12 17:53:16,352 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:53:17,875 : Generating sentence embeddings
2019-02-12 17:53:32,130 : Generated sentence embeddings
2019-02-12 17:53:32,130 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:54:08,093 : Best param found at split 1: l2reg = 0.001                 with score 84.3
2019-02-12 17:54:38,582 : Best param found at split 2: l2reg = 1e-05                 with score 84.37
2019-02-12 17:55:06,868 : Best param found at split 3: l2reg = 0.001                 with score 85.1
2019-02-12 17:55:27,434 : Best param found at split 4: l2reg = 0.01                 with score 85.6
2019-02-12 17:55:57,569 : Best param found at split 5: l2reg = 0.001                 with score 85.98
2019-02-12 17:55:58,460 : Dev acc : 85.07 Test acc : 84.82

2019-02-12 17:55:58,461 : ***** Transfer task : SUBJ *****


2019-02-12 17:55:58,478 : loading BERT mode bert-base-uncased
2019-02-12 17:55:58,478 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:55:58,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:55:58,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbg1nwbld
2019-02-12 17:56:00,951 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:56:02,449 : Generating sentence embeddings
2019-02-12 17:56:25,412 : Generated sentence embeddings
2019-02-12 17:56:25,412 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:56:39,507 : Best param found at split 1: l2reg = 1e-05                 with score 50.39
2019-02-12 17:57:03,948 : Best param found at split 2: l2reg = 0.0001                 with score 54.22
2019-02-12 17:57:33,935 : Best param found at split 3: l2reg = 1e-05                 with score 53.97
2019-02-12 17:58:00,832 : Best param found at split 4: l2reg = 1e-05                 with score 52.05
2019-02-12 17:58:32,085 : Best param found at split 5: l2reg = 1e-05                 with score 52.1
2019-02-12 17:58:34,196 : Dev acc : 52.55 Test acc : 55.43

2019-02-12 17:58:34,197 : ***** Transfer task : SST Binary classification *****


2019-02-12 17:58:34,325 : loading BERT mode bert-base-uncased
2019-02-12 17:58:34,325 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:58:34,347 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:58:34,347 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8qhjrxe7
2019-02-12 17:58:36,727 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:58:38,191 : Computing embedding for train
2019-02-12 17:59:58,542 : Computed train embeddings
2019-02-12 17:59:58,542 : Computing embedding for dev
2019-02-12 17:59:59,825 : Computed dev embeddings
2019-02-12 17:59:59,825 : Computing embedding for test
2019-02-12 18:00:02,532 : Computed test embeddings
2019-02-12 18:00:02,532 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:00:32,595 : [('reg:1e-05', 69.5), ('reg:0.0001', 68.92), ('reg:0.001', 68.46), ('reg:0.01', 69.04)]
2019-02-12 18:00:32,595 : Validation : best param found is reg = 1e-05 with score             69.5
2019-02-12 18:00:32,595 : Evaluating...
2019-02-12 18:00:38,834 : 
Dev acc : 69.5 Test acc : 68.81 for             SST Binary classification

2019-02-12 18:00:38,835 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 18:00:38,882 : loading BERT mode bert-base-uncased
2019-02-12 18:00:38,882 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:00:38,903 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:00:38,903 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_j4su3hs
2019-02-12 18:00:41,318 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:00:42,863 : Computing embedding for train
2019-02-12 18:01:00,552 : Computed train embeddings
2019-02-12 18:01:00,552 : Computing embedding for dev
2019-02-12 18:01:02,789 : Computed dev embeddings
2019-02-12 18:01:02,789 : Computing embedding for test
2019-02-12 18:01:06,287 : Computed test embeddings
2019-02-12 18:01:06,288 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:01:12,705 : [('reg:1e-05', 31.7), ('reg:0.0001', 27.97), ('reg:0.001', 26.43), ('reg:0.01', 27.79)]
2019-02-12 18:01:12,705 : Validation : best param found is reg = 1e-05 with score             31.7
2019-02-12 18:01:12,705 : Evaluating...
2019-02-12 18:01:14,271 : 
Dev acc : 31.7 Test acc : 32.67 for             SST Fine-Grained classification

2019-02-12 18:01:14,271 : ***** Transfer task : TREC *****


2019-02-12 18:01:14,284 : loading BERT mode bert-base-uncased
2019-02-12 18:01:14,284 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:01:14,303 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:01:14,303 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbxg9pexu
2019-02-12 18:01:16,723 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:01:25,036 : Computed train embeddings
2019-02-12 18:01:25,611 : Computed test embeddings
2019-02-12 18:01:25,612 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 18:01:43,505 : [('reg:1e-05', 50.8), ('reg:0.0001', 53.27), ('reg:0.001', 49.97), ('reg:0.01', 49.84)]
2019-02-12 18:01:43,505 : Cross-validation : best param found is reg = 0.0001             with score 53.27
2019-02-12 18:01:43,505 : Evaluating...
2019-02-12 18:01:44,495 : 
Dev acc : 53.27 Test acc : 65.4             for TREC

2019-02-12 18:01:44,496 : ***** Transfer task : MRPC *****


2019-02-12 18:01:44,517 : loading BERT mode bert-base-uncased
2019-02-12 18:01:44,517 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:01:44,537 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:01:44,538 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpklfc5mwd
2019-02-12 18:01:46,973 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:01:48,428 : Computing embedding for train
2019-02-12 18:02:02,911 : Computed train embeddings
2019-02-12 18:02:02,912 : Computing embedding for test
2019-02-12 18:02:09,307 : Computed test embeddings
2019-02-12 18:02:09,323 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 18:02:21,562 : [('reg:1e-05', 71.57), ('reg:0.0001', 70.24), ('reg:0.001', 69.97), ('reg:0.01', 71.37)]
2019-02-12 18:02:21,562 : Cross-validation : best param found is reg = 1e-05             with score 71.57
2019-02-12 18:02:21,562 : Evaluating...
2019-02-12 18:02:22,631 : Dev acc : 71.57 Test acc 69.39; Test F1 80.97 for MRPC.

2019-02-12 18:02:22,631 : ***** Transfer task : SICK-Entailment*****


2019-02-12 18:02:22,655 : loading BERT mode bert-base-uncased
2019-02-12 18:02:22,655 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:02:22,708 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:02:22,708 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9ydig89h
2019-02-12 18:02:25,099 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:02:26,630 : Computing embedding for train
2019-02-12 18:02:40,721 : Computed train embeddings
2019-02-12 18:02:40,722 : Computing embedding for dev
2019-02-12 18:02:42,237 : Computed dev embeddings
2019-02-12 18:02:42,237 : Computing embedding for test
2019-02-12 18:02:57,266 : Computed test embeddings
2019-02-12 18:02:57,293 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:03:02,103 : [('reg:1e-05', 75.4), ('reg:0.0001', 72.2), ('reg:0.001', 71.6), ('reg:0.01', 72.6)]
2019-02-12 18:03:02,103 : Validation : best param found is reg = 1e-05 with score             75.4
2019-02-12 18:03:02,103 : Evaluating...
2019-02-12 18:03:03,288 : 
Dev acc : 75.4 Test acc : 74.93 for                        SICK entailment

2019-02-12 18:03:03,289 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 18:03:03,315 : loading BERT mode bert-base-uncased
2019-02-12 18:03:03,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:03:03,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:03:03,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa6p25g8y
2019-02-12 18:03:05,724 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:03:07,255 : Computing embedding for train
2019-02-12 18:03:18,539 : Computed train embeddings
2019-02-12 18:03:18,539 : Computing embedding for dev
2019-02-12 18:03:19,954 : Computed dev embeddings
2019-02-12 18:03:19,954 : Computing embedding for test
2019-02-12 18:03:32,894 : Computed test embeddings
2019-02-12 18:04:25,031 : Dev : Pearson 0.788086601269239
2019-02-12 18:04:25,031 : Test : Pearson 0.7968666730442842 Spearman 0.7240256291999169 MSE 0.3764490400607171                        for SICK Relatedness

2019-02-12 18:04:25,032 : 

***** Transfer task : STSBenchmark*****


2019-02-12 18:04:25,070 : loading BERT mode bert-base-uncased
2019-02-12 18:04:25,071 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:04:25,099 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:04:25,099 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxlktps_y
2019-02-12 18:04:27,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:04:28,996 : Computing embedding for train
2019-02-12 18:04:45,054 : Computed train embeddings
2019-02-12 18:04:45,054 : Computing embedding for dev
2019-02-12 18:04:50,311 : Computed dev embeddings
2019-02-12 18:04:50,311 : Computing embedding for test
2019-02-12 18:04:55,115 : Computed test embeddings
2019-02-12 18:05:54,387 : Dev : Pearson 0.6965712054703311
2019-02-12 18:05:54,387 : Test : Pearson 0.6436464192455066 Spearman 0.6408201822625577 MSE 1.4333519342854175                        for SICK Relatedness

2019-02-12 18:05:54,387 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 18:05:54,635 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 18:05:54,645 : loading BERT mode bert-base-uncased
2019-02-12 18:05:54,646 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:05:54,735 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:05:54,735 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0mpv0mu7
2019-02-12 18:05:57,153 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:05:58,631 : Computing embeddings for train/dev/test
2019-02-12 18:09:01,406 : Computed embeddings
2019-02-12 18:09:01,406 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:10:11,015 : [('reg:1e-05', 82.13), ('reg:0.0001', 82.54), ('reg:0.001', 79.82), ('reg:0.01', 64.53)]
2019-02-12 18:10:11,015 : Validation : best param found is reg = 0.0001 with score             82.54
2019-02-12 18:10:11,015 : Evaluating...
2019-02-12 18:10:37,945 : 
Dev acc : 82.5 Test acc : 82.4 for LENGTH classification

2019-02-12 18:10:37,952 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 18:10:38,314 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 18:10:38,358 : loading BERT mode bert-base-uncased
2019-02-12 18:10:38,358 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:10:38,385 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:10:38,385 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7jmnmkho
2019-02-12 18:10:40,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:10:42,296 : Computing embeddings for train/dev/test
2019-02-12 18:13:35,935 : Computed embeddings
2019-02-12 18:13:35,936 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:15:22,146 : [('reg:1e-05', 56.42), ('reg:0.0001', 12.92), ('reg:0.001', 0.42), ('reg:0.01', 0.19)]
2019-02-12 18:15:22,146 : Validation : best param found is reg = 1e-05 with score             56.42
2019-02-12 18:15:22,146 : Evaluating...
2019-02-12 18:16:00,227 : 
Dev acc : 56.4 Test acc : 55.9 for WORDCONTENT classification

2019-02-12 18:16:00,235 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 18:16:00,581 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 18:16:00,645 : loading BERT mode bert-base-uncased
2019-02-12 18:16:00,646 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:16:00,739 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:16:00,739 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgzx7l19f
2019-02-12 18:16:03,147 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:16:04,610 : Computing embeddings for train/dev/test
2019-02-12 18:18:28,836 : Computed embeddings
2019-02-12 18:18:28,836 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:20:07,739 : [('reg:1e-05', 23.0), ('reg:0.0001', 22.93), ('reg:0.001', 21.73), ('reg:0.01', 18.98)]
2019-02-12 18:20:07,739 : Validation : best param found is reg = 1e-05 with score             23.0
2019-02-12 18:20:07,739 : Evaluating...
2019-02-12 18:20:29,039 : 
Dev acc : 23.0 Test acc : 22.6 for DEPTH classification

2019-02-12 18:20:29,047 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 18:20:29,411 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 18:20:29,472 : loading BERT mode bert-base-uncased
2019-02-12 18:20:29,472 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:20:29,577 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:20:29,578 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvp5nvs92
2019-02-12 18:20:32,038 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:20:33,528 : Computing embeddings for train/dev/test
2019-02-12 18:22:46,073 : Computed embeddings
2019-02-12 18:22:46,073 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:24:37,726 : [('reg:1e-05', 43.53), ('reg:0.0001', 44.54), ('reg:0.001', 44.43), ('reg:0.01', 24.0)]
2019-02-12 18:24:37,726 : Validation : best param found is reg = 0.0001 with score             44.54
2019-02-12 18:24:37,726 : Evaluating...
2019-02-12 18:24:53,533 : 
Dev acc : 44.5 Test acc : 44.3 for TOPCONSTITUENTS classification

2019-02-12 18:24:53,541 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 18:24:54,057 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 18:24:54,122 : loading BERT mode bert-base-uncased
2019-02-12 18:24:54,122 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:24:54,152 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:24:54,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptpc68n3m
2019-02-12 18:24:56,590 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:24:58,009 : Computing embeddings for train/dev/test
2019-02-12 18:27:42,171 : Computed embeddings
2019-02-12 18:27:42,171 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:29:11,123 : [('reg:1e-05', 51.79), ('reg:0.0001', 51.79), ('reg:0.001', 50.18), ('reg:0.01', 50.0)]
2019-02-12 18:29:11,124 : Validation : best param found is reg = 1e-05 with score             51.79
2019-02-12 18:29:11,124 : Evaluating...
2019-02-12 18:29:22,786 : 
Dev acc : 51.8 Test acc : 50.0 for BIGRAMSHIFT classification

2019-02-12 18:29:22,794 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 18:29:23,191 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 18:29:23,255 : loading BERT mode bert-base-uncased
2019-02-12 18:29:23,255 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:29:23,286 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:29:23,286 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu5eep6bb
2019-02-12 18:29:25,716 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:29:27,135 : Computing embeddings for train/dev/test
2019-02-12 18:32:19,562 : Computed embeddings
2019-02-12 18:32:19,562 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:33:52,297 : [('reg:1e-05', 74.41), ('reg:0.0001', 74.51), ('reg:0.001', 76.33), ('reg:0.01', 72.98)]
2019-02-12 18:33:52,297 : Validation : best param found is reg = 0.001 with score             76.33
2019-02-12 18:33:52,297 : Evaluating...
2019-02-12 18:34:09,742 : 
Dev acc : 76.3 Test acc : 73.5 for TENSE classification

2019-02-12 18:34:09,749 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 18:34:10,167 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 18:34:10,230 : loading BERT mode bert-base-uncased
2019-02-12 18:34:10,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:34:10,255 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:34:10,255 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbyw6k5eg
2019-02-12 18:34:12,636 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:34:14,167 : Computing embeddings for train/dev/test
2019-02-12 18:37:14,699 : Computed embeddings
2019-02-12 18:37:14,699 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:38:05,180 : [('reg:1e-05', 72.33), ('reg:0.0001', 72.29), ('reg:0.001', 72.08), ('reg:0.01', 68.78)]
2019-02-12 18:38:05,181 : Validation : best param found is reg = 1e-05 with score             72.33
2019-02-12 18:38:05,181 : Evaluating...
2019-02-12 18:38:24,064 : 
Dev acc : 72.3 Test acc : 71.1 for SUBJNUMBER classification

2019-02-12 18:38:24,071 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 18:38:24,475 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 18:38:24,541 : loading BERT mode bert-base-uncased
2019-02-12 18:38:24,541 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:38:24,654 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:38:24,654 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkwqibpwn
2019-02-12 18:38:27,075 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:38:28,560 : Computing embeddings for train/dev/test
2019-02-12 18:41:07,900 : Computed embeddings
2019-02-12 18:41:07,900 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:42:11,682 : [('reg:1e-05', 62.0), ('reg:0.0001', 61.64), ('reg:0.001', 67.98), ('reg:0.01', 56.01)]
2019-02-12 18:42:11,682 : Validation : best param found is reg = 0.001 with score             67.98
2019-02-12 18:42:11,682 : Evaluating...
2019-02-12 18:42:28,191 : 
Dev acc : 68.0 Test acc : 68.5 for OBJNUMBER classification

2019-02-12 18:42:28,199 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 18:42:28,581 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 18:42:28,649 : loading BERT mode bert-base-uncased
2019-02-12 18:42:28,649 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:42:28,770 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:42:28,770 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzu69uk07
2019-02-12 18:42:31,191 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:42:32,593 : Computing embeddings for train/dev/test
2019-02-12 18:44:52,588 : Computed embeddings
2019-02-12 18:44:52,588 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:45:41,182 : [('reg:1e-05', 50.23), ('reg:0.0001', 50.24), ('reg:0.001', 50.19), ('reg:0.01', 50.31)]
2019-02-12 18:45:41,182 : Validation : best param found is reg = 0.01 with score             50.31
2019-02-12 18:45:41,182 : Evaluating...
2019-02-12 18:45:50,724 : 
Dev acc : 50.3 Test acc : 49.8 for ODDMANOUT classification

2019-02-12 18:45:50,733 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 18:45:51,316 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 18:45:51,392 : loading BERT mode bert-base-uncased
2019-02-12 18:45:51,392 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:45:51,421 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:45:51,421 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjzs6el1j
2019-02-12 18:45:53,870 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:45:55,387 : Computing embeddings for train/dev/test
2019-02-12 18:48:26,852 : Computed embeddings
2019-02-12 18:48:26,852 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:49:09,167 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-12 18:49:09,168 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-12 18:49:09,168 : Evaluating...
2019-02-12 18:49:18,754 : 
Dev acc : 50.0 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-02-12 18:49:18,765 : {'STS12': {'MSRpar': {'pearson': (0.39231158157851875, 5.274625528025561e-29), 'spearman': SpearmanrResult(correlation=0.41133509330874707, pvalue=5.498056061923203e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7107481932308285, 2.1948997187354176e-116), 'spearman': SpearmanrResult(correlation=0.7136955104174543, pvalue=9.030146640820396e-118), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4897457082726784, 4.6153305190058883e-29), 'spearman': SpearmanrResult(correlation=0.5860508017604955, pvalue=1.113973036028007e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6184714853260592, 2.338744754187602e-80), 'spearman': SpearmanrResult(correlation=0.6561186784128675, pvalue=1.5999812174613138e-93), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4909052252305271, 1.37174231664998e-25), 'spearman': SpearmanrResult(correlation=0.4205804010956017, pvalue=1.5563522533347399e-18), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5404364387277225, 'wmean': 0.5507763545899917}, 'spearman': {'mean': 0.5575560969990332, 'wmean': 0.5703574194496506}}}, 'STS13': {'FNWN': {'pearson': (0.2443752076325935, 0.0007018781033651067), 'spearman': SpearmanrResult(correlation=0.23928715954847454, pvalue=0.0009127017679584543), 'nsamples': 189}, 'headlines': {'pearson': (0.6611179772258328, 2.0499908212567183e-95), 'spearman': SpearmanrResult(correlation=0.6567408395182543, pvalue=9.345034613667561e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.5889810947294624, 1.1054796413677528e-53), 'spearman': SpearmanrResult(correlation=0.5898880190531226, pvalue=6.982570013511401e-54), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4981580931959629, 'wmean': 0.5816291942034421}, 'spearman': {'mean': 0.4953053393732838, 'wmean': 0.5791387209881028}}}, 'STS14': {'deft-forum': {'pearson': (0.36045651660363986, 2.9732327689101337e-15), 'spearman': SpearmanrResult(correlation=0.3567293793570378, pvalue=5.970503042233836e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.6842009741600031, 9.387601617302659e-43), 'spearman': SpearmanrResult(correlation=0.6511114862633486, pvalue=1.4383490238246118e-37), 'nsamples': 300}, 'headlines': {'pearson': (0.6245594771673342, 2.3007982677098274e-82), 'spearman': SpearmanrResult(correlation=0.6106461229923437, pvalue=7.673253966081052e-78), 'nsamples': 750}, 'images': {'pearson': (0.6963990208220597, 7.018347547260832e-110), 'spearman': SpearmanrResult(correlation=0.6919959875925529, pvalue=5.8283485420049815e-108), 'nsamples': 750}, 'OnWN': {'pearson': (0.6504567223997998, 2.0143979016798115e-91), 'spearman': SpearmanrResult(correlation=0.680159885748345, pvalue=5.722796515125585e-103), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5959294164627614, 2.6960387912890152e-73), 'spearman': SpearmanrResult(correlation=0.5888364709087633, pvalue=3.4581263114371886e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6020003546025997, 'wmean': 0.611459787295628}, 'spearman': {'mean': 0.5965798888103985, 'wmean': 0.6092241378723134}}}, 'STS15': {'answers-forums': {'pearson': (0.5415993896194873, 5.7317499662483094e-30), 'spearman': SpearmanrResult(correlation=0.5190650625420267, pvalue=2.955160375324995e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.6903602108687453, 2.9499038399067926e-107), 'spearman': SpearmanrResult(correlation=0.6996805484472093, pvalue=2.4713215748660936e-111), 'nsamples': 750}, 'belief': {'pearson': (0.5498365987491456, 5.185490373985195e-31), 'spearman': SpearmanrResult(correlation=0.5968070579954796, pvalue=1.4881003551127607e-37), 'nsamples': 375}, 'headlines': {'pearson': (0.6512991316747175, 9.875186961608847e-92), 'spearman': SpearmanrResult(correlation=0.6541133643998895, pvalue=8.97564872468903e-93), 'nsamples': 750}, 'images': {'pearson': (0.7735549847298093, 2.511545613079099e-150), 'spearman': SpearmanrResult(correlation=0.7846970457649921, pvalue=1.6428659469558314e-157), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.641330063128381, 'wmean': 0.6652330803643971}, 'spearman': {'mean': 0.6508726158299194, 'wmean': 0.674106754720211}}}, 'STS16': {'answer-answer': {'pearson': (0.42731274377393036, 1.0708706668731246e-12), 'spearman': SpearmanrResult(correlation=0.4269161713871067, pvalue=1.1292706557556726e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.6522747329842059, 1.4595974600703958e-31), 'spearman': SpearmanrResult(correlation=0.6670536830455703, pvalue=2.0083125371534743e-33), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6855602956462044, 2.834374749923506e-33), 'spearman': SpearmanrResult(correlation=0.691332245009346, pvalue=5.021953185071478e-34), 'nsamples': 230}, 'postediting': {'pearson': (0.7945551784965781, 2.3622190130892694e-54), 'spearman': SpearmanrResult(correlation=0.8280797876885901, pvalue=9.261298717895888e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.6301465046425283, 1.5755139317840046e-24), 'spearman': SpearmanrResult(correlation=0.631724833191767, pvalue=1.1159144029650878e-24), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6379698911086894, 'wmean': 0.6359230998766662}, 'spearman': {'mean': 0.649021344064476, 'wmean': 0.6472356233523753}}}, 'MR': {'devacc': 51.56, 'acc': 50.32, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 66.16, 'acc': 65.11, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.07, 'acc': 84.82, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 52.55, 'acc': 55.43, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 69.5, 'acc': 68.81, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 31.7, 'acc': 32.67, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 53.27, 'acc': 65.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.57, 'acc': 69.39, 'f1': 80.97, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 75.4, 'acc': 74.93, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.788086601269239, 'pearson': 0.7968666730442842, 'spearman': 0.7240256291999169, 'mse': 0.3764490400607171, 'yhat': array([2.89386674, 4.46304165, 1.5904312 , ..., 3.1766855 , 4.6135616 ,
       4.69510072]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6965712054703311, 'pearson': 0.6436464192455066, 'spearman': 0.6408201822625577, 'mse': 1.4333519342854175, 'yhat': array([1.82179285, 1.77609072, 2.57698044, ..., 3.34350985, 3.75869264,
       3.00943342]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 82.54, 'acc': 82.37, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 56.42, 'acc': 55.9, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 23.0, 'acc': 22.65, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 44.54, 'acc': 44.28, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 51.79, 'acc': 50.01, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 76.33, 'acc': 73.48, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 72.33, 'acc': 71.06, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 67.98, 'acc': 68.47, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.31, 'acc': 49.75, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 18:49:18,765 : ********************************************************************************
2019-02-12 18:49:18,765 : ********************************************************************************
2019-02-12 18:49:18,765 : ********************************************************************************
2019-02-12 18:49:18,765 : layer 1
2019-02-12 18:49:18,765 : ********************************************************************************
2019-02-12 18:49:18,765 : ********************************************************************************
2019-02-12 18:49:18,765 : ********************************************************************************
2019-02-12 18:49:18,847 : ***** Transfer task : STS12 *****


2019-02-12 18:49:18,860 : loading BERT mode bert-base-uncased
2019-02-12 18:49:18,860 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:49:18,877 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:49:18,877 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt9x869ox
2019-02-12 18:49:21,310 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:49:25,677 : MSRpar : pearson = 0.3807, spearman = 0.4126
2019-02-12 18:49:27,647 : MSRvid : pearson = 0.7642, spearman = 0.7660
2019-02-12 18:49:29,093 : SMTeuroparl : pearson = 0.5010, spearman = 0.5961
2019-02-12 18:49:31,274 : surprise.OnWN : pearson = 0.6267, spearman = 0.6508
2019-02-12 18:49:32,579 : surprise.SMTnews : pearson = 0.5402, spearman = 0.4553
2019-02-12 18:49:32,579 : ALL (weighted average) : Pearson = 0.5708,             Spearman = 0.5879
2019-02-12 18:49:32,579 : ALL (average) : Pearson = 0.5626,             Spearman = 0.5761

2019-02-12 18:49:32,579 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 18:49:32,587 : loading BERT mode bert-base-uncased
2019-02-12 18:49:32,587 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:49:32,604 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:49:32,604 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkohf8_s9
2019-02-12 18:49:35,029 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:49:37,197 : FNWN : pearson = 0.2299, spearman = 0.2380
2019-02-12 18:49:38,495 : headlines : pearson = 0.6854, spearman = 0.6782
2019-02-12 18:49:39,478 : OnWN : pearson = 0.6428, spearman = 0.6410
2019-02-12 18:49:39,478 : ALL (weighted average) : Pearson = 0.6121,             Spearman = 0.6088
2019-02-12 18:49:39,478 : ALL (average) : Pearson = 0.5194,             Spearman = 0.5191

2019-02-12 18:49:39,478 : ***** Transfer task : STS14 *****


2019-02-12 18:49:39,499 : loading BERT mode bert-base-uncased
2019-02-12 18:49:39,499 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:49:39,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:49:39,517 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmiutwnhm
2019-02-12 18:49:41,964 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:49:44,295 : deft-forum : pearson = 0.3957, spearman = 0.4066
2019-02-12 18:49:45,170 : deft-news : pearson = 0.7135, spearman = 0.6762
2019-02-12 18:49:46,550 : headlines : pearson = 0.6616, spearman = 0.6395
2019-02-12 18:49:47,894 : images : pearson = 0.7106, spearman = 0.6974
2019-02-12 18:49:49,251 : OnWN : pearson = 0.7054, spearman = 0.7297
2019-02-12 18:49:50,913 : tweet-news : pearson = 0.6203, spearman = 0.5995
2019-02-12 18:49:50,913 : ALL (weighted average) : Pearson = 0.6442,             Spearman = 0.6361
2019-02-12 18:49:50,913 : ALL (average) : Pearson = 0.6345,             Spearman = 0.6248

2019-02-12 18:49:50,913 : ***** Transfer task : STS15 *****


2019-02-12 18:49:50,948 : loading BERT mode bert-base-uncased
2019-02-12 18:49:50,948 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:49:50,965 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:49:50,966 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1_1knhcg
2019-02-12 18:49:53,395 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:49:55,877 : answers-forums : pearson = 0.5281, spearman = 0.4979
2019-02-12 18:49:57,224 : answers-students : pearson = 0.7205, spearman = 0.7333
2019-02-12 18:49:58,293 : belief : pearson = 0.5512, spearman = 0.6034
2019-02-12 18:49:59,702 : headlines : pearson = 0.6862, spearman = 0.6878
2019-02-12 18:50:01,065 : images : pearson = 0.7865, spearman = 0.7969
2019-02-12 18:50:01,065 : ALL (weighted average) : Pearson = 0.6832,             Spearman = 0.6922
2019-02-12 18:50:01,065 : ALL (average) : Pearson = 0.6545,             Spearman = 0.6639

2019-02-12 18:50:01,065 : ***** Transfer task : STS16 *****


2019-02-12 18:50:01,134 : loading BERT mode bert-base-uncased
2019-02-12 18:50:01,135 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:50:01,152 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:50:01,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb0g_ea4v
2019-02-12 18:50:03,587 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:50:05,519 : answer-answer : pearson = 0.4181, spearman = 0.4365
2019-02-12 18:50:05,964 : headlines : pearson = 0.6912, spearman = 0.7036
2019-02-12 18:50:06,483 : plagiarism : pearson = 0.7137, spearman = 0.7265
2019-02-12 18:50:07,245 : postediting : pearson = 0.7915, spearman = 0.8410
2019-02-12 18:50:07,643 : question-question : pearson = 0.5770, spearman = 0.5761
2019-02-12 18:50:07,643 : ALL (weighted average) : Pearson = 0.6376,             Spearman = 0.6566
2019-02-12 18:50:07,643 : ALL (average) : Pearson = 0.6383,             Spearman = 0.6567

2019-02-12 18:50:07,643 : ***** Transfer task : MR *****


2019-02-12 18:50:07,660 : loading BERT mode bert-base-uncased
2019-02-12 18:50:07,660 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:50:07,681 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:50:07,681 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm26tdv3m
2019-02-12 18:50:10,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:50:11,559 : Generating sentence embeddings
2019-02-12 18:50:29,502 : Generated sentence embeddings
2019-02-12 18:50:29,502 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:50:53,053 : Best param found at split 1: l2reg = 0.01                 with score 59.37
2019-02-12 18:51:24,543 : Best param found at split 2: l2reg = 0.001                 with score 57.25
2019-02-12 18:52:17,583 : Best param found at split 3: l2reg = 0.0001                 with score 59.25
2019-02-12 18:52:55,967 : Best param found at split 4: l2reg = 0.001                 with score 56.15
2019-02-12 18:53:16,971 : Best param found at split 5: l2reg = 1e-05                 with score 57.51
2019-02-12 18:53:18,048 : Dev acc : 57.91 Test acc : 56.86

2019-02-12 18:53:18,049 : ***** Transfer task : CR *****


2019-02-12 18:53:18,056 : loading BERT mode bert-base-uncased
2019-02-12 18:53:18,056 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:53:18,076 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:53:18,076 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyjdb6ksw
2019-02-12 18:53:20,509 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:53:21,929 : Generating sentence embeddings
2019-02-12 18:53:26,653 : Generated sentence embeddings
2019-02-12 18:53:26,653 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:53:31,036 : Best param found at split 1: l2reg = 0.01                 with score 67.7
2019-02-12 18:53:40,037 : Best param found at split 2: l2reg = 0.01                 with score 72.11
2019-02-12 18:53:49,357 : Best param found at split 3: l2reg = 0.001                 with score 67.62
2019-02-12 18:53:53,704 : Best param found at split 4: l2reg = 1e-05                 with score 72.23
2019-02-12 18:54:00,368 : Best param found at split 5: l2reg = 0.0001                 with score 70.28
2019-02-12 18:54:00,886 : Dev acc : 69.99 Test acc : 64.93

2019-02-12 18:54:00,886 : ***** Transfer task : MPQA *****


2019-02-12 18:54:00,891 : loading BERT mode bert-base-uncased
2019-02-12 18:54:00,892 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:54:00,911 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:54:00,911 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuq_cl1la
2019-02-12 18:54:03,315 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:54:04,801 : Generating sentence embeddings
2019-02-12 18:54:14,488 : Generated sentence embeddings
2019-02-12 18:54:14,489 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:54:49,944 : Best param found at split 1: l2reg = 0.0001                 with score 86.83
2019-02-12 18:55:30,569 : Best param found at split 2: l2reg = 1e-05                 with score 87.03
2019-02-12 18:56:08,166 : Best param found at split 3: l2reg = 0.01                 with score 87.11
2019-02-12 18:56:27,587 : Best param found at split 4: l2reg = 0.0001                 with score 86.34
2019-02-12 18:56:48,780 : Best param found at split 5: l2reg = 0.0001                 with score 87.27
2019-02-12 18:56:49,635 : Dev acc : 86.92 Test acc : 84.98

2019-02-12 18:56:49,636 : ***** Transfer task : SUBJ *****


2019-02-12 18:56:49,650 : loading BERT mode bert-base-uncased
2019-02-12 18:56:49,650 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:56:49,672 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:56:49,672 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3j_nxtp4
2019-02-12 18:56:52,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:56:53,546 : Generating sentence embeddings
2019-02-12 18:57:10,620 : Generated sentence embeddings
2019-02-12 18:57:10,621 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:57:33,615 : Best param found at split 1: l2reg = 1e-05                 with score 83.07
2019-02-12 18:58:14,625 : Best param found at split 2: l2reg = 1e-05                 with score 83.85
2019-02-12 18:58:51,027 : Best param found at split 3: l2reg = 0.0001                 with score 85.81
2019-02-12 18:59:25,893 : Best param found at split 4: l2reg = 0.0001                 with score 87.28
2019-02-12 18:59:51,340 : Best param found at split 5: l2reg = 1e-05                 with score 85.49
2019-02-12 18:59:52,731 : Dev acc : 85.1 Test acc : 85.1

2019-02-12 18:59:52,732 : ***** Transfer task : SST Binary classification *****


2019-02-12 18:59:52,856 : loading BERT mode bert-base-uncased
2019-02-12 18:59:52,857 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:59:52,878 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:59:52,878 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl4ct5g21
2019-02-12 18:59:55,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:59:56,783 : Computing embedding for train
2019-02-12 19:01:36,091 : Computed train embeddings
2019-02-12 19:01:36,091 : Computing embedding for dev
2019-02-12 19:01:38,029 : Computed dev embeddings
2019-02-12 19:01:38,029 : Computing embedding for test
2019-02-12 19:01:42,302 : Computed test embeddings
2019-02-12 19:01:42,302 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:02:25,047 : [('reg:1e-05', 74.89), ('reg:0.0001', 75.0), ('reg:0.001', 74.31), ('reg:0.01', 65.71)]
2019-02-12 19:02:25,047 : Validation : best param found is reg = 0.0001 with score             75.0
2019-02-12 19:02:25,047 : Evaluating...
2019-02-12 19:02:31,838 : 
Dev acc : 75.0 Test acc : 76.06 for             SST Binary classification

2019-02-12 19:02:31,838 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 19:02:31,887 : loading BERT mode bert-base-uncased
2019-02-12 19:02:31,887 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:02:31,908 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:02:31,908 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpby9kq_4o
2019-02-12 19:02:34,328 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:02:35,798 : Computing embedding for train
2019-02-12 19:02:50,586 : Computed train embeddings
2019-02-12 19:02:50,586 : Computing embedding for dev
2019-02-12 19:02:52,603 : Computed dev embeddings
2019-02-12 19:02:52,603 : Computing embedding for test
2019-02-12 19:02:56,642 : Computed test embeddings
2019-02-12 19:02:56,642 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:03:01,765 : [('reg:1e-05', 32.24), ('reg:0.0001', 28.52), ('reg:0.001', 31.7), ('reg:0.01', 29.06)]
2019-02-12 19:03:01,765 : Validation : best param found is reg = 1e-05 with score             32.24
2019-02-12 19:03:01,766 : Evaluating...
2019-02-12 19:03:03,196 : 
Dev acc : 32.24 Test acc : 28.33 for             SST Fine-Grained classification

2019-02-12 19:03:03,196 : ***** Transfer task : TREC *****


2019-02-12 19:03:03,209 : loading BERT mode bert-base-uncased
2019-02-12 19:03:03,209 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:03:03,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:03:03,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjhczfpy0
2019-02-12 19:03:05,648 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:03:15,517 : Computed train embeddings
2019-02-12 19:03:16,286 : Computed test embeddings
2019-02-12 19:03:16,286 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 19:03:35,451 : [('reg:1e-05', 61.07), ('reg:0.0001', 57.27), ('reg:0.001', 60.85), ('reg:0.01', 58.04)]
2019-02-12 19:03:35,451 : Cross-validation : best param found is reg = 1e-05             with score 61.07
2019-02-12 19:03:35,451 : Evaluating...
2019-02-12 19:03:36,925 : 
Dev acc : 61.07 Test acc : 72.0             for TREC

2019-02-12 19:03:36,925 : ***** Transfer task : MRPC *****


2019-02-12 19:03:36,947 : loading BERT mode bert-base-uncased
2019-02-12 19:03:36,947 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:03:36,967 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:03:36,967 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq0e4z2sy
2019-02-12 19:03:39,394 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:03:40,852 : Computing embedding for train
2019-02-12 19:03:52,319 : Computed train embeddings
2019-02-12 19:03:52,319 : Computing embedding for test
2019-02-12 19:03:58,284 : Computed test embeddings
2019-02-12 19:03:58,300 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 19:04:10,335 : [('reg:1e-05', 70.51), ('reg:0.0001', 71.05), ('reg:0.001', 72.2), ('reg:0.01', 71.71)]
2019-02-12 19:04:10,335 : Cross-validation : best param found is reg = 0.001             with score 72.2
2019-02-12 19:04:10,335 : Evaluating...
2019-02-12 19:04:11,091 : Dev acc : 72.2 Test acc 72.0; Test F1 81.4 for MRPC.

2019-02-12 19:04:11,092 : ***** Transfer task : SICK-Entailment*****


2019-02-12 19:04:11,154 : loading BERT mode bert-base-uncased
2019-02-12 19:04:11,154 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:04:11,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:04:11,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6smpzynz
2019-02-12 19:04:13,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:04:15,044 : Computing embedding for train
2019-02-12 19:04:25,545 : Computed train embeddings
2019-02-12 19:04:25,545 : Computing embedding for dev
2019-02-12 19:04:26,830 : Computed dev embeddings
2019-02-12 19:04:26,830 : Computing embedding for test
2019-02-12 19:04:38,673 : Computed test embeddings
2019-02-12 19:04:38,700 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:04:42,324 : [('reg:1e-05', 74.4), ('reg:0.0001', 70.6), ('reg:0.001', 73.6), ('reg:0.01', 74.6)]
2019-02-12 19:04:42,324 : Validation : best param found is reg = 0.01 with score             74.6
2019-02-12 19:04:42,324 : Evaluating...
2019-02-12 19:04:43,342 : 
Dev acc : 74.6 Test acc : 72.8 for                        SICK entailment

2019-02-12 19:04:43,343 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 19:04:43,369 : loading BERT mode bert-base-uncased
2019-02-12 19:04:43,369 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:04:43,424 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:04:43,424 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzn3n6c6u
2019-02-12 19:04:45,851 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:04:47,326 : Computing embedding for train
2019-02-12 19:04:58,924 : Computed train embeddings
2019-02-12 19:04:58,924 : Computing embedding for dev
2019-02-12 19:05:00,394 : Computed dev embeddings
2019-02-12 19:05:00,394 : Computing embedding for test
2019-02-12 19:05:13,786 : Computed test embeddings
2019-02-12 19:06:11,301 : Dev : Pearson 0.7992206545120942
2019-02-12 19:06:11,301 : Test : Pearson 0.8061241353457513 Spearman 0.7395668721473344 MSE 0.35651854256819004                        for SICK Relatedness

2019-02-12 19:06:11,302 : 

***** Transfer task : STSBenchmark*****


2019-02-12 19:06:11,366 : loading BERT mode bert-base-uncased
2019-02-12 19:06:11,366 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:06:11,385 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:06:11,385 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgjjo7lxl
2019-02-12 19:06:13,795 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:06:15,319 : Computing embedding for train
2019-02-12 19:06:31,150 : Computed train embeddings
2019-02-12 19:06:31,150 : Computing embedding for dev
2019-02-12 19:06:35,659 : Computed dev embeddings
2019-02-12 19:06:35,659 : Computing embedding for test
2019-02-12 19:06:39,592 : Computed test embeddings
2019-02-12 19:07:48,056 : Dev : Pearson 0.7017670416428679
2019-02-12 19:07:48,056 : Test : Pearson 0.6680418259700962 Spearman 0.6642506073298712 MSE 1.3527185259098462                        for SICK Relatedness

2019-02-12 19:07:48,057 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 19:07:48,372 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 19:07:48,382 : loading BERT mode bert-base-uncased
2019-02-12 19:07:48,383 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:07:48,405 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:07:48,405 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq5pwcm_3
2019-02-12 19:07:50,788 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:07:52,309 : Computing embeddings for train/dev/test
2019-02-12 19:10:40,625 : Computed embeddings
2019-02-12 19:10:40,626 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:11:49,775 : [('reg:1e-05', 70.47), ('reg:0.0001', 77.76), ('reg:0.001', 74.24), ('reg:0.01', 57.84)]
2019-02-12 19:11:49,775 : Validation : best param found is reg = 0.0001 with score             77.76
2019-02-12 19:11:49,775 : Evaluating...
2019-02-12 19:12:17,141 : 
Dev acc : 77.8 Test acc : 78.4 for LENGTH classification

2019-02-12 19:12:17,148 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 19:12:17,487 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 19:12:17,531 : loading BERT mode bert-base-uncased
2019-02-12 19:12:17,531 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:12:17,559 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:12:17,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpix0xoy0g
2019-02-12 19:12:19,945 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:12:21,507 : Computing embeddings for train/dev/test
2019-02-12 19:14:58,892 : Computed embeddings
2019-02-12 19:14:58,893 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:16:42,381 : [('reg:1e-05', 33.49), ('reg:0.0001', 13.49), ('reg:0.001', 0.46), ('reg:0.01', 0.28)]
2019-02-12 19:16:42,381 : Validation : best param found is reg = 1e-05 with score             33.49
2019-02-12 19:16:42,381 : Evaluating...
2019-02-12 19:17:19,330 : 
Dev acc : 33.5 Test acc : 34.1 for WORDCONTENT classification

2019-02-12 19:17:19,338 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 19:17:19,697 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 19:17:19,763 : loading BERT mode bert-base-uncased
2019-02-12 19:17:19,763 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:17:19,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:17:19,789 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn5huxxqj
2019-02-12 19:17:22,230 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:17:23,730 : Computing embeddings for train/dev/test
2019-02-12 19:20:00,014 : Computed embeddings
2019-02-12 19:20:00,014 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:21:28,544 : [('reg:1e-05', 25.61), ('reg:0.0001', 25.18), ('reg:0.001', 27.54), ('reg:0.01', 24.65)]
2019-02-12 19:21:28,544 : Validation : best param found is reg = 0.001 with score             27.54
2019-02-12 19:21:28,544 : Evaluating...
2019-02-12 19:21:47,229 : 
Dev acc : 27.5 Test acc : 27.6 for DEPTH classification

2019-02-12 19:21:47,237 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 19:21:47,611 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 19:21:47,672 : loading BERT mode bert-base-uncased
2019-02-12 19:21:47,672 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:21:47,779 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:21:47,779 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpatdpevfq
2019-02-12 19:21:50,197 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:21:51,672 : Computing embeddings for train/dev/test
2019-02-12 19:24:22,656 : Computed embeddings
2019-02-12 19:24:22,656 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:25:40,143 : [('reg:1e-05', 49.58), ('reg:0.0001', 48.09), ('reg:0.001', 46.62), ('reg:0.01', 34.79)]
2019-02-12 19:25:40,143 : Validation : best param found is reg = 1e-05 with score             49.58
2019-02-12 19:25:40,143 : Evaluating...
2019-02-12 19:25:58,461 : 
Dev acc : 49.6 Test acc : 49.9 for TOPCONSTITUENTS classification

2019-02-12 19:25:58,468 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 19:25:58,803 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 19:25:58,868 : loading BERT mode bert-base-uncased
2019-02-12 19:25:58,868 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:25:58,986 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:25:58,986 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7y0s7xfh
2019-02-12 19:26:01,407 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:26:02,880 : Computing embeddings for train/dev/test
2019-02-12 19:28:54,237 : Computed embeddings
2019-02-12 19:28:54,237 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:30:12,786 : [('reg:1e-05', 54.65), ('reg:0.0001', 54.62), ('reg:0.001', 54.62), ('reg:0.01', 50.16)]
2019-02-12 19:30:12,786 : Validation : best param found is reg = 1e-05 with score             54.65
2019-02-12 19:30:12,786 : Evaluating...
2019-02-12 19:30:33,582 : 
Dev acc : 54.6 Test acc : 53.6 for BIGRAMSHIFT classification

2019-02-12 19:30:33,589 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 19:30:34,147 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 19:30:34,212 : loading BERT mode bert-base-uncased
2019-02-12 19:30:34,212 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:30:34,242 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:30:34,242 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp97d0pya6
2019-02-12 19:30:36,664 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:30:38,134 : Computing embeddings for train/dev/test
2019-02-12 19:33:15,096 : Computed embeddings
2019-02-12 19:33:15,096 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:34:10,573 : [('reg:1e-05', 82.43), ('reg:0.0001', 82.62), ('reg:0.001', 82.3), ('reg:0.01', 81.13)]
2019-02-12 19:34:10,573 : Validation : best param found is reg = 0.0001 with score             82.62
2019-02-12 19:34:10,573 : Evaluating...
2019-02-12 19:34:29,310 : 
Dev acc : 82.6 Test acc : 81.7 for TENSE classification

2019-02-12 19:34:29,319 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 19:34:29,895 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 19:34:29,959 : loading BERT mode bert-base-uncased
2019-02-12 19:34:29,959 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:34:29,985 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:34:29,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3bm2ckqn
2019-02-12 19:34:32,400 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:34:33,882 : Computing embeddings for train/dev/test
2019-02-12 19:37:11,303 : Computed embeddings
2019-02-12 19:37:11,303 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:38:24,171 : [('reg:1e-05', 76.33), ('reg:0.0001', 76.61), ('reg:0.001', 76.55), ('reg:0.01', 72.23)]
2019-02-12 19:38:24,171 : Validation : best param found is reg = 0.0001 with score             76.61
2019-02-12 19:38:24,171 : Evaluating...
2019-02-12 19:38:45,160 : 
Dev acc : 76.6 Test acc : 75.1 for SUBJNUMBER classification

2019-02-12 19:38:45,169 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 19:38:45,598 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 19:38:45,664 : loading BERT mode bert-base-uncased
2019-02-12 19:38:45,664 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:38:45,691 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:38:45,691 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq1wmw3bf
2019-02-12 19:38:48,113 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:38:49,562 : Computing embeddings for train/dev/test
2019-02-12 19:41:13,405 : Computed embeddings
2019-02-12 19:41:13,405 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:42:24,837 : [('reg:1e-05', 74.08), ('reg:0.0001', 74.14), ('reg:0.001', 74.22), ('reg:0.01', 70.35)]
2019-02-12 19:42:24,837 : Validation : best param found is reg = 0.001 with score             74.22
2019-02-12 19:42:24,837 : Evaluating...
2019-02-12 19:42:48,821 : 
Dev acc : 74.2 Test acc : 74.8 for OBJNUMBER classification

2019-02-12 19:42:48,830 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 19:42:49,221 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 19:42:49,289 : loading BERT mode bert-base-uncased
2019-02-12 19:42:49,289 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:42:49,411 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:42:49,411 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsubqd5se
2019-02-12 19:42:51,829 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:42:53,255 : Computing embeddings for train/dev/test
2019-02-12 19:45:19,612 : Computed embeddings
2019-02-12 19:45:19,612 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:46:10,180 : [('reg:1e-05', 51.96), ('reg:0.0001', 51.8), ('reg:0.001', 51.63), ('reg:0.01', 51.64)]
2019-02-12 19:46:10,180 : Validation : best param found is reg = 1e-05 with score             51.96
2019-02-12 19:46:10,180 : Evaluating...
2019-02-12 19:46:21,018 : 
Dev acc : 52.0 Test acc : 51.4 for ODDMANOUT classification

2019-02-12 19:46:21,028 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 19:46:21,632 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 19:46:21,706 : loading BERT mode bert-base-uncased
2019-02-12 19:46:21,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:46:21,737 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:46:21,737 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqidkdo76
2019-02-12 19:46:24,165 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:46:25,589 : Computing embeddings for train/dev/test
2019-02-12 19:48:47,316 : Computed embeddings
2019-02-12 19:48:47,316 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:49:48,168 : [('reg:1e-05', 50.38), ('reg:0.0001', 50.36), ('reg:0.001', 53.59), ('reg:0.01', 50.0)]
2019-02-12 19:49:48,168 : Validation : best param found is reg = 0.001 with score             53.59
2019-02-12 19:49:48,168 : Evaluating...
2019-02-12 19:50:10,636 : 
Dev acc : 53.6 Test acc : 52.8 for COORDINATIONINVERSION classification

2019-02-12 19:50:10,646 : {'STS12': {'MSRpar': {'pearson': (0.38069638861979616, 2.8157054993071588e-27), 'spearman': SpearmanrResult(correlation=0.4125865398681763, pvalue=3.445092319161489e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7642346648213648, 1.2611967356420944e-144), 'spearman': SpearmanrResult(correlation=0.7660416614520523, pvalue=1.0383598356083297e-145), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5009663626201865, 1.558060622826133e-30), 'spearman': SpearmanrResult(correlation=0.5960898148899993, pvalue=1.6959359791444293e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6266758965382246, 4.504759056839864e-83), 'spearman': SpearmanrResult(correlation=0.6507550207364325, pvalue=1.5654109013263755e-91), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5401865335976657, 1.3270698620685984e-31), 'spearman': SpearmanrResult(correlation=0.4552595274414354, pvalue=8.265472671887956e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5625519692394475, 'wmean': 0.570844015390178}, 'spearman': {'mean': 0.5761465128776192, 'wmean': 0.5879315292876893}}}, 'STS13': {'FNWN': {'pearson': (0.22992137199438617, 0.001459209753732531), 'spearman': SpearmanrResult(correlation=0.23804877457335558, pvalue=0.0009721479618058084), 'nsamples': 189}, 'headlines': {'pearson': (0.6854022846599871, 3.766749933132417e-105), 'spearman': SpearmanrResult(correlation=0.6782259536883557, pvalue=3.5557642413927836e-102), 'nsamples': 750}, 'OnWN': {'pearson': (0.6428401750383972, 1.0065426137124007e-66), 'spearman': SpearmanrResult(correlation=0.6409609338456004, pvalue=3.1781216734650553e-66), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5193879438975901, 'wmean': 0.6120934606656467}, 'spearman': {'mean': 0.5190785540357705, 'wmean': 0.6088265116986752}}}, 'STS14': {'deft-forum': {'pearson': (0.3957486400249637, 2.530200224604935e-18), 'spearman': SpearmanrResult(correlation=0.406576800256275, pvalue=2.423468939694758e-19), 'nsamples': 450}, 'deft-news': {'pearson': (0.713501632358904, 5.884127498758998e-48), 'spearman': SpearmanrResult(correlation=0.6762143818073906, pvalue=1.932688842306899e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.6616185437275139, 1.319061244646489e-95), 'spearman': SpearmanrResult(correlation=0.6394692718378472, pvalue=1.7932194128148609e-87), 'nsamples': 750}, 'images': {'pearson': (0.7105853051793857, 2.615109456359279e-116), 'spearman': SpearmanrResult(correlation=0.6973818391152311, pvalue=2.5884604195410694e-110), 'nsamples': 750}, 'OnWN': {'pearson': (0.7054077063175149, 6.432395192699002e-114), 'spearman': SpearmanrResult(correlation=0.7296906069490945, pvalue=1.31055551381845e-125), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6203133997860553, 5.839799721799749e-81), 'spearman': SpearmanrResult(correlation=0.5994941621901851, pvalue=2.2452144435558347e-74), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6345292045657229, 'wmean': 0.6441549583938019}, 'spearman': {'mean': 0.6248045103593373, 'wmean': 0.6360935425938158}}}, 'STS15': {'answers-forums': {'pearson': (0.5281476328208886, 2.5221490673201502e-28), 'spearman': SpearmanrResult(correlation=0.49788047188875023, pvalue=6.954086517580838e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.7205325141256118, 4.704096924251792e-121), 'spearman': SpearmanrResult(correlation=0.7332885936881175, pvalue=1.8894523240862917e-127), 'nsamples': 750}, 'belief': {'pearson': (0.5512099979070266, 3.4512852879396676e-31), 'spearman': SpearmanrResult(correlation=0.6033820307662148, pvalue=1.4758934809909849e-38), 'nsamples': 375}, 'headlines': {'pearson': (0.6861978223006284, 1.7412493094083542e-105), 'spearman': SpearmanrResult(correlation=0.6878476717414747, pvalue=3.486943326710338e-106), 'nsamples': 750}, 'images': {'pearson': (0.7864628950553668, 1.090432133284442e-158), 'spearman': SpearmanrResult(correlation=0.7969144616866566, pvalue=6.734336175793431e-166), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6545101724419045, 'wmean': 0.6832180117113912}, 'spearman': {'mean': 0.6638626459542427, 'wmean': 0.6921704946109328}}}, 'STS16': {'answer-answer': {'pearson': (0.4181422049357414, 3.5923090241615974e-12), 'spearman': SpearmanrResult(correlation=0.4364707758759629, pvalue=3.0813110749723756e-13), 'nsamples': 254}, 'headlines': {'pearson': (0.691238633964886, 1.036041045975562e-36), 'spearman': SpearmanrResult(correlation=0.7036303351133384, pvalue=1.5935730668111904e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.713662104896694, 4.157780116519914e-37), 'spearman': SpearmanrResult(correlation=0.7265403156630837, pvalue=5.0529779703772695e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.791515094590184, 1.142287595967173e-53), 'spearman': SpearmanrResult(correlation=0.8409924699101883, pvalue=1.686134294775216e-66), 'nsamples': 244}, 'question-question': {'pearson': (0.5769991377697243, 6.026447714152873e-20), 'spearman': SpearmanrResult(correlation=0.576105708908422, pvalue=7.080789690193989e-20), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.638311435231446, 'wmean': 0.6375980833988633}, 'spearman': {'mean': 0.656747921094199, 'wmean': 0.6566442318197059}}}, 'MR': {'devacc': 57.91, 'acc': 56.86, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 69.99, 'acc': 64.93, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.92, 'acc': 84.98, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 85.1, 'acc': 85.1, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 75.0, 'acc': 76.06, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 32.24, 'acc': 28.33, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 61.07, 'acc': 72.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.2, 'acc': 72.0, 'f1': 81.4, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 74.6, 'acc': 72.8, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7992206545120942, 'pearson': 0.8061241353457513, 'spearman': 0.7395668721473344, 'mse': 0.35651854256819004, 'yhat': array([3.34722076, 4.19541263, 1.40789467, ..., 3.03857946, 4.20813792,
       4.43069232]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7017670416428679, 'pearson': 0.6680418259700962, 'spearman': 0.6642506073298712, 'mse': 1.3527185259098462, 'yhat': array([1.35163078, 1.6973677 , 2.56751219, ..., 3.71423694, 4.3902917 ,
       3.49068192]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 77.76, 'acc': 78.4, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 33.49, 'acc': 34.07, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.54, 'acc': 27.6, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 49.58, 'acc': 49.92, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 54.65, 'acc': 53.62, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 82.62, 'acc': 81.68, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 76.61, 'acc': 75.11, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 74.22, 'acc': 74.83, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 51.96, 'acc': 51.4, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.59, 'acc': 52.82, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 19:50:10,647 : ********************************************************************************
2019-02-12 19:50:10,647 : ********************************************************************************
2019-02-12 19:50:10,647 : ********************************************************************************
2019-02-12 19:50:10,647 : layer 2
2019-02-12 19:50:10,647 : ********************************************************************************
2019-02-12 19:50:10,647 : ********************************************************************************
2019-02-12 19:50:10,647 : ********************************************************************************
2019-02-12 19:50:10,733 : ***** Transfer task : STS12 *****


2019-02-12 19:50:10,746 : loading BERT mode bert-base-uncased
2019-02-12 19:50:10,746 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:50:10,762 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:50:10,763 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl6ii__ht
2019-02-12 19:50:13,216 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:50:17,726 : MSRpar : pearson = 0.3635, spearman = 0.3998
2019-02-12 19:50:19,934 : MSRvid : pearson = 0.7558, spearman = 0.7582
2019-02-12 19:50:21,571 : SMTeuroparl : pearson = 0.4896, spearman = 0.5895
2019-02-12 19:50:24,233 : surprise.OnWN : pearson = 0.6216, spearman = 0.6451
2019-02-12 19:50:25,649 : surprise.SMTnews : pearson = 0.5225, spearman = 0.4373
2019-02-12 19:50:25,649 : ALL (weighted average) : Pearson = 0.5595,             Spearman = 0.5783
2019-02-12 19:50:25,649 : ALL (average) : Pearson = 0.5506,             Spearman = 0.5660

2019-02-12 19:50:25,649 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 19:50:25,656 : loading BERT mode bert-base-uncased
2019-02-12 19:50:25,657 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:50:25,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:50:25,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcr1fptby
2019-02-12 19:50:28,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:50:30,477 : FNWN : pearson = 0.2181, spearman = 0.2092
2019-02-12 19:50:32,508 : headlines : pearson = 0.6957, spearman = 0.6874
2019-02-12 19:50:34,128 : OnWN : pearson = 0.6474, spearman = 0.6457
2019-02-12 19:50:34,128 : ALL (weighted average) : Pearson = 0.6175,             Spearman = 0.6115
2019-02-12 19:50:34,128 : ALL (average) : Pearson = 0.5204,             Spearman = 0.5141

2019-02-12 19:50:34,128 : ***** Transfer task : STS14 *****


2019-02-12 19:50:34,146 : loading BERT mode bert-base-uncased
2019-02-12 19:50:34,146 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:50:34,194 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:50:34,194 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpavbvnbg2
2019-02-12 19:50:36,611 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:50:39,150 : deft-forum : pearson = 0.4194, spearman = 0.4359
2019-02-12 19:50:40,156 : deft-news : pearson = 0.7179, spearman = 0.6858
2019-02-12 19:50:42,094 : headlines : pearson = 0.6601, spearman = 0.6348
2019-02-12 19:50:44,121 : images : pearson = 0.6962, spearman = 0.6862
2019-02-12 19:50:46,338 : OnWN : pearson = 0.7154, spearman = 0.7384
2019-02-12 19:50:48,995 : tweet-news : pearson = 0.6095, spearman = 0.5925
2019-02-12 19:50:48,995 : ALL (weighted average) : Pearson = 0.6440,             Spearman = 0.6375
2019-02-12 19:50:48,995 : ALL (average) : Pearson = 0.6364,             Spearman = 0.6289

2019-02-12 19:50:48,995 : ***** Transfer task : STS15 *****


2019-02-12 19:50:49,026 : loading BERT mode bert-base-uncased
2019-02-12 19:50:49,026 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:50:49,043 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:50:49,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9mm53yaj
2019-02-12 19:50:51,460 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:50:53,953 : answers-forums : pearson = 0.5480, spearman = 0.5250
2019-02-12 19:50:55,333 : answers-students : pearson = 0.7161, spearman = 0.7245
2019-02-12 19:50:56,421 : belief : pearson = 0.5699, spearman = 0.6127
2019-02-12 19:50:57,771 : headlines : pearson = 0.6977, spearman = 0.7011
2019-02-12 19:50:58,936 : images : pearson = 0.7854, spearman = 0.7933
2019-02-12 19:50:58,937 : ALL (weighted average) : Pearson = 0.6895,             Spearman = 0.6969
2019-02-12 19:50:58,937 : ALL (average) : Pearson = 0.6634,             Spearman = 0.6713

2019-02-12 19:50:58,937 : ***** Transfer task : STS16 *****


2019-02-12 19:50:59,006 : loading BERT mode bert-base-uncased
2019-02-12 19:50:59,006 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:50:59,023 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:50:59,024 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplyg8jfme
2019-02-12 19:51:01,459 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:51:03,534 : answer-answer : pearson = 0.4391, spearman = 0.4474
2019-02-12 19:51:04,033 : headlines : pearson = 0.7000, spearman = 0.7123
2019-02-12 19:51:04,649 : plagiarism : pearson = 0.7244, spearman = 0.7326
2019-02-12 19:51:05,536 : postediting : pearson = 0.7973, spearman = 0.8435
2019-02-12 19:51:06,011 : question-question : pearson = 0.5814, spearman = 0.5890
2019-02-12 19:51:06,011 : ALL (weighted average) : Pearson = 0.6480,             Spearman = 0.6648
2019-02-12 19:51:06,011 : ALL (average) : Pearson = 0.6484,             Spearman = 0.6650

2019-02-12 19:51:06,011 : ***** Transfer task : MR *****


2019-02-12 19:51:06,028 : loading BERT mode bert-base-uncased
2019-02-12 19:51:06,028 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:51:06,048 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:51:06,048 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvj3cehlz
2019-02-12 19:51:08,463 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:51:09,971 : Generating sentence embeddings
2019-02-12 19:51:29,241 : Generated sentence embeddings
2019-02-12 19:51:29,241 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:51:59,525 : Best param found at split 1: l2reg = 1e-05                 with score 60.89
2019-02-12 19:52:26,018 : Best param found at split 2: l2reg = 1e-05                 with score 60.05
2019-02-12 19:52:55,804 : Best param found at split 3: l2reg = 0.0001                 with score 58.59
2019-02-12 19:53:27,177 : Best param found at split 4: l2reg = 1e-05                 with score 57.7
2019-02-12 19:54:12,295 : Best param found at split 5: l2reg = 0.0001                 with score 61.66
2019-02-12 19:54:14,472 : Dev acc : 59.78 Test acc : 62.16

2019-02-12 19:54:14,473 : ***** Transfer task : CR *****


2019-02-12 19:54:14,481 : loading BERT mode bert-base-uncased
2019-02-12 19:54:14,481 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:54:14,500 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:54:14,500 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_6fspokm
2019-02-12 19:54:16,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:54:18,406 : Generating sentence embeddings
2019-02-12 19:54:24,654 : Generated sentence embeddings
2019-02-12 19:54:24,654 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:54:32,977 : Best param found at split 1: l2reg = 0.01                 with score 69.09
2019-02-12 19:54:44,515 : Best param found at split 2: l2reg = 0.0001                 with score 67.54
2019-02-12 19:54:51,922 : Best param found at split 3: l2reg = 0.0001                 with score 68.41
2019-02-12 19:54:56,850 : Best param found at split 4: l2reg = 0.001                 with score 73.12
2019-02-12 19:55:01,145 : Best param found at split 5: l2reg = 1e-05                 with score 72.16
2019-02-12 19:55:01,382 : Dev acc : 70.06 Test acc : 67.23

2019-02-12 19:55:01,382 : ***** Transfer task : MPQA *****


2019-02-12 19:55:01,387 : loading BERT mode bert-base-uncased
2019-02-12 19:55:01,387 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:55:01,407 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:55:01,407 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0vn13ye6
2019-02-12 19:55:03,835 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:55:05,341 : Generating sentence embeddings
2019-02-12 19:55:15,710 : Generated sentence embeddings
2019-02-12 19:55:15,710 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:55:40,637 : Best param found at split 1: l2reg = 0.0001                 with score 87.01
2019-02-12 19:56:01,272 : Best param found at split 2: l2reg = 0.01                 with score 86.95
2019-02-12 19:56:23,193 : Best param found at split 3: l2reg = 1e-05                 with score 86.62
2019-02-12 19:56:50,348 : Best param found at split 4: l2reg = 0.001                 with score 86.06
2019-02-12 19:57:30,335 : Best param found at split 5: l2reg = 0.0001                 with score 86.74
2019-02-12 19:57:32,123 : Dev acc : 86.68 Test acc : 86.58

2019-02-12 19:57:32,124 : ***** Transfer task : SUBJ *****


2019-02-12 19:57:32,139 : loading BERT mode bert-base-uncased
2019-02-12 19:57:32,139 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:57:32,160 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:57:32,160 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwk5loq2e
2019-02-12 19:57:34,595 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:57:36,064 : Generating sentence embeddings
2019-02-12 19:57:56,484 : Generated sentence embeddings
2019-02-12 19:57:56,485 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:58:22,658 : Best param found at split 1: l2reg = 0.0001                 with score 88.26
2019-02-12 19:58:47,299 : Best param found at split 2: l2reg = 1e-05                 with score 87.61
2019-02-12 19:59:21,088 : Best param found at split 3: l2reg = 1e-05                 with score 88.6
2019-02-12 20:00:01,428 : Best param found at split 4: l2reg = 1e-05                 with score 89.76
2019-02-12 20:00:48,130 : Best param found at split 5: l2reg = 0.0001                 with score 88.25
2019-02-12 20:00:49,571 : Dev acc : 88.5 Test acc : 89.54

2019-02-12 20:00:49,572 : ***** Transfer task : SST Binary classification *****


2019-02-12 20:00:49,696 : loading BERT mode bert-base-uncased
2019-02-12 20:00:49,696 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:00:49,720 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:00:49,720 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4u67dhml
2019-02-12 20:00:52,102 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:00:53,562 : Computing embedding for train
2019-02-12 20:02:20,200 : Computed train embeddings
2019-02-12 20:02:20,200 : Computing embedding for dev
2019-02-12 20:02:21,390 : Computed dev embeddings
2019-02-12 20:02:21,391 : Computing embedding for test
2019-02-12 20:02:23,727 : Computed test embeddings
2019-02-12 20:02:23,727 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:03:00,998 : [('reg:1e-05', 76.26), ('reg:0.0001', 76.38), ('reg:0.001', 76.49), ('reg:0.01', 72.71)]
2019-02-12 20:03:00,998 : Validation : best param found is reg = 0.001 with score             76.49
2019-02-12 20:03:00,998 : Evaluating...
2019-02-12 20:03:11,506 : 
Dev acc : 76.49 Test acc : 77.38 for             SST Binary classification

2019-02-12 20:03:11,507 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 20:03:11,560 : loading BERT mode bert-base-uncased
2019-02-12 20:03:11,560 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:03:11,581 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:03:11,581 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptvf6f2f3
2019-02-12 20:03:14,006 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:03:15,496 : Computing embedding for train
2019-02-12 20:03:30,384 : Computed train embeddings
2019-02-12 20:03:30,384 : Computing embedding for dev
2019-02-12 20:03:32,050 : Computed dev embeddings
2019-02-12 20:03:32,050 : Computing embedding for test
2019-02-12 20:03:35,435 : Computed test embeddings
2019-02-12 20:03:35,435 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:03:46,216 : [('reg:1e-05', 34.33), ('reg:0.0001', 30.52), ('reg:0.001', 33.51), ('reg:0.01', 37.51)]
2019-02-12 20:03:46,216 : Validation : best param found is reg = 0.01 with score             37.51
2019-02-12 20:03:46,216 : Evaluating...
2019-02-12 20:03:48,299 : 
Dev acc : 37.51 Test acc : 36.61 for             SST Fine-Grained classification

2019-02-12 20:03:48,300 : ***** Transfer task : TREC *****


2019-02-12 20:03:48,313 : loading BERT mode bert-base-uncased
2019-02-12 20:03:48,313 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:03:48,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:03:48,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp750a8bj0
2019-02-12 20:03:50,715 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:04:01,746 : Computed train embeddings
2019-02-12 20:04:02,527 : Computed test embeddings
2019-02-12 20:04:02,527 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 20:04:30,141 : [('reg:1e-05', 67.34), ('reg:0.0001', 57.26), ('reg:0.001', 63.83), ('reg:0.01', 53.77)]
2019-02-12 20:04:30,141 : Cross-validation : best param found is reg = 1e-05             with score 67.34
2019-02-12 20:04:30,141 : Evaluating...
2019-02-12 20:04:31,634 : 
Dev acc : 67.34 Test acc : 70.6             for TREC

2019-02-12 20:04:31,635 : ***** Transfer task : MRPC *****


2019-02-12 20:04:31,656 : loading BERT mode bert-base-uncased
2019-02-12 20:04:31,657 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:04:31,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:04:31,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqkatx989
2019-02-12 20:04:34,059 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:04:35,649 : Computing embedding for train
2019-02-12 20:04:52,046 : Computed train embeddings
2019-02-12 20:04:52,046 : Computing embedding for test
2019-02-12 20:04:58,408 : Computed test embeddings
2019-02-12 20:04:58,424 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 20:05:11,307 : [('reg:1e-05', 71.44), ('reg:0.0001', 70.85), ('reg:0.001', 70.66), ('reg:0.01', 71.83)]
2019-02-12 20:05:11,307 : Cross-validation : best param found is reg = 0.01             with score 71.83
2019-02-12 20:05:11,307 : Evaluating...
2019-02-12 20:05:12,014 : Dev acc : 71.83 Test acc 71.36; Test F1 80.24 for MRPC.

2019-02-12 20:05:12,015 : ***** Transfer task : SICK-Entailment*****


2019-02-12 20:05:12,077 : loading BERT mode bert-base-uncased
2019-02-12 20:05:12,077 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:05:12,096 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:05:12,096 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdvr03jop
2019-02-12 20:05:14,509 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:05:16,002 : Computing embedding for train
2019-02-12 20:05:29,052 : Computed train embeddings
2019-02-12 20:05:29,052 : Computing embedding for dev
2019-02-12 20:05:30,658 : Computed dev embeddings
2019-02-12 20:05:30,658 : Computing embedding for test
2019-02-12 20:05:45,324 : Computed test embeddings
2019-02-12 20:05:45,352 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:05:48,631 : [('reg:1e-05', 75.0), ('reg:0.0001', 77.8), ('reg:0.001', 76.6), ('reg:0.01', 76.2)]
2019-02-12 20:05:48,631 : Validation : best param found is reg = 0.0001 with score             77.8
2019-02-12 20:05:48,631 : Evaluating...
2019-02-12 20:05:49,337 : 
Dev acc : 77.8 Test acc : 75.4 for                        SICK entailment

2019-02-12 20:05:49,337 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 20:05:49,364 : loading BERT mode bert-base-uncased
2019-02-12 20:05:49,364 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:05:49,383 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:05:49,383 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmqd1yp_6
2019-02-12 20:05:51,809 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:05:53,269 : Computing embedding for train
2019-02-12 20:06:04,390 : Computed train embeddings
2019-02-12 20:06:04,390 : Computing embedding for dev
2019-02-12 20:06:06,088 : Computed dev embeddings
2019-02-12 20:06:06,088 : Computing embedding for test
2019-02-12 20:06:18,691 : Computed test embeddings
2019-02-12 20:06:49,837 : Dev : Pearson 0.7978968172552805
2019-02-12 20:06:49,838 : Test : Pearson 0.801687707607353 Spearman 0.7333321394894639 MSE 0.36452502670947                        for SICK Relatedness

2019-02-12 20:06:49,838 : 

***** Transfer task : STSBenchmark*****


2019-02-12 20:06:49,877 : loading BERT mode bert-base-uncased
2019-02-12 20:06:49,877 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:06:49,905 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:06:49,905 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp503xd84z
2019-02-12 20:06:52,325 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:06:53,807 : Computing embedding for train
2019-02-12 20:07:08,161 : Computed train embeddings
2019-02-12 20:07:08,161 : Computing embedding for dev
2019-02-12 20:07:12,304 : Computed dev embeddings
2019-02-12 20:07:12,304 : Computing embedding for test
2019-02-12 20:07:15,902 : Computed test embeddings
2019-02-12 20:08:18,716 : Dev : Pearson 0.6805422197170821
2019-02-12 20:08:18,716 : Test : Pearson 0.6539986199305371 Spearman 0.648530565583907 MSE 1.4058035217290834                        for SICK Relatedness

2019-02-12 20:08:18,717 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 20:08:19,032 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 20:08:19,041 : loading BERT mode bert-base-uncased
2019-02-12 20:08:19,042 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:08:19,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:08:19,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp13yey5ys
2019-02-12 20:08:21,448 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:08:22,976 : Computing embeddings for train/dev/test
2019-02-12 20:11:23,316 : Computed embeddings
2019-02-12 20:11:23,316 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:12:29,917 : [('reg:1e-05', 70.8), ('reg:0.0001', 59.68), ('reg:0.001', 65.67), ('reg:0.01', 55.2)]
2019-02-12 20:12:29,917 : Validation : best param found is reg = 1e-05 with score             70.8
2019-02-12 20:12:29,917 : Evaluating...
2019-02-12 20:12:48,177 : 
Dev acc : 70.8 Test acc : 71.6 for LENGTH classification

2019-02-12 20:12:48,185 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 20:12:48,525 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 20:12:48,570 : loading BERT mode bert-base-uncased
2019-02-12 20:12:48,570 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:12:48,597 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:12:48,597 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp35nmnng
2019-02-12 20:12:50,989 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:12:52,479 : Computing embeddings for train/dev/test
2019-02-12 20:15:33,894 : Computed embeddings
2019-02-12 20:15:33,894 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:16:47,473 : [('reg:1e-05', 36.57), ('reg:0.0001', 27.15), ('reg:0.001', 0.77), ('reg:0.01', 0.2)]
2019-02-12 20:16:47,473 : Validation : best param found is reg = 1e-05 with score             36.57
2019-02-12 20:16:47,473 : Evaluating...
2019-02-12 20:17:17,999 : 
Dev acc : 36.6 Test acc : 36.4 for WORDCONTENT classification

2019-02-12 20:17:18,007 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 20:17:18,350 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 20:17:18,413 : loading BERT mode bert-base-uncased
2019-02-12 20:17:18,413 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:17:18,506 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:17:18,506 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2063ieit
2019-02-12 20:17:20,894 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:17:22,410 : Computing embeddings for train/dev/test
2019-02-12 20:20:06,560 : Computed embeddings
2019-02-12 20:20:06,560 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:21:32,013 : [('reg:1e-05', 25.76), ('reg:0.0001', 27.13), ('reg:0.001', 22.85), ('reg:0.01', 25.28)]
2019-02-12 20:21:32,013 : Validation : best param found is reg = 0.0001 with score             27.13
2019-02-12 20:21:32,013 : Evaluating...
2019-02-12 20:21:58,800 : 
Dev acc : 27.1 Test acc : 26.9 for DEPTH classification

2019-02-12 20:21:58,808 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 20:21:59,174 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 20:21:59,235 : loading BERT mode bert-base-uncased
2019-02-12 20:21:59,235 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:21:59,357 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:21:59,357 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpywjunioe
2019-02-12 20:22:01,778 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:22:03,243 : Computing embeddings for train/dev/test
2019-02-12 20:24:35,493 : Computed embeddings
2019-02-12 20:24:35,494 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:26:05,213 : [('reg:1e-05', 53.37), ('reg:0.0001', 46.28), ('reg:0.001', 48.14), ('reg:0.01', 37.65)]
2019-02-12 20:26:05,213 : Validation : best param found is reg = 1e-05 with score             53.37
2019-02-12 20:26:05,213 : Evaluating...
2019-02-12 20:26:39,822 : 
Dev acc : 53.4 Test acc : 53.8 for TOPCONSTITUENTS classification

2019-02-12 20:26:39,830 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 20:26:40,341 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 20:26:40,406 : loading BERT mode bert-base-uncased
2019-02-12 20:26:40,407 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:26:40,437 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:26:40,437 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0acaf_r_
2019-02-12 20:26:42,855 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:26:44,312 : Computing embeddings for train/dev/test
2019-02-12 20:29:16,808 : Computed embeddings
2019-02-12 20:29:16,808 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:31:16,834 : [('reg:1e-05', 61.4), ('reg:0.0001', 61.37), ('reg:0.001', 61.27), ('reg:0.01', 56.91)]
2019-02-12 20:31:16,834 : Validation : best param found is reg = 1e-05 with score             61.4
2019-02-12 20:31:16,834 : Evaluating...
2019-02-12 20:31:32,433 : 
Dev acc : 61.4 Test acc : 61.5 for BIGRAMSHIFT classification

2019-02-12 20:31:32,441 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 20:31:32,836 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 20:31:32,900 : loading BERT mode bert-base-uncased
2019-02-12 20:31:32,900 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:31:32,931 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:31:32,931 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2ky0ds0v
2019-02-12 20:31:35,357 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:31:36,757 : Computing embeddings for train/dev/test
2019-02-12 20:33:39,833 : Computed embeddings
2019-02-12 20:33:39,834 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:34:39,558 : [('reg:1e-05', 85.13), ('reg:0.0001', 85.08), ('reg:0.001', 85.12), ('reg:0.01', 85.07)]
2019-02-12 20:34:39,558 : Validation : best param found is reg = 1e-05 with score             85.13
2019-02-12 20:34:39,558 : Evaluating...
2019-02-12 20:34:53,785 : 
Dev acc : 85.1 Test acc : 83.6 for TENSE classification

2019-02-12 20:34:53,795 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 20:34:54,206 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 20:34:54,268 : loading BERT mode bert-base-uncased
2019-02-12 20:34:54,269 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:34:54,294 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:34:54,294 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp69wqtmdo
2019-02-12 20:34:56,716 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:34:58,190 : Computing embeddings for train/dev/test
2019-02-12 20:37:10,648 : Computed embeddings
2019-02-12 20:37:10,648 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:38:36,918 : [('reg:1e-05', 75.24), ('reg:0.0001', 75.25), ('reg:0.001', 75.16), ('reg:0.01', 75.59)]
2019-02-12 20:38:36,918 : Validation : best param found is reg = 0.01 with score             75.59
2019-02-12 20:38:36,918 : Evaluating...
2019-02-12 20:38:58,191 : 
Dev acc : 75.6 Test acc : 74.5 for SUBJNUMBER classification

2019-02-12 20:38:58,200 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 20:38:58,603 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 20:38:58,669 : loading BERT mode bert-base-uncased
2019-02-12 20:38:58,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:38:58,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:38:58,782 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprzwtx_ov
2019-02-12 20:39:01,206 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:39:02,632 : Computing embeddings for train/dev/test
2019-02-12 20:41:21,642 : Computed embeddings
2019-02-12 20:41:21,642 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:42:28,848 : [('reg:1e-05', 73.87), ('reg:0.0001', 73.83), ('reg:0.001', 74.13), ('reg:0.01', 71.64)]
2019-02-12 20:42:28,848 : Validation : best param found is reg = 0.001 with score             74.13
2019-02-12 20:42:28,848 : Evaluating...
2019-02-12 20:42:44,107 : 
Dev acc : 74.1 Test acc : 74.3 for OBJNUMBER classification

2019-02-12 20:42:44,115 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 20:42:44,492 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 20:42:44,560 : loading BERT mode bert-base-uncased
2019-02-12 20:42:44,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:42:44,682 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:42:44,682 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzy1900pe
2019-02-12 20:42:47,117 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:42:48,528 : Computing embeddings for train/dev/test
2019-02-12 20:45:13,843 : Computed embeddings
2019-02-12 20:45:13,844 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:46:00,865 : [('reg:1e-05', 53.19), ('reg:0.0001', 53.21), ('reg:0.001', 53.2), ('reg:0.01', 53.43)]
2019-02-12 20:46:00,865 : Validation : best param found is reg = 0.01 with score             53.43
2019-02-12 20:46:00,865 : Evaluating...
2019-02-12 20:46:10,549 : 
Dev acc : 53.4 Test acc : 52.3 for ODDMANOUT classification

2019-02-12 20:46:10,559 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 20:46:11,144 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 20:46:11,218 : loading BERT mode bert-base-uncased
2019-02-12 20:46:11,218 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:46:11,249 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:46:11,249 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps_7kdi9q
2019-02-12 20:46:13,675 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:46:15,085 : Computing embeddings for train/dev/test
2019-02-12 20:48:59,657 : Computed embeddings
2019-02-12 20:48:59,657 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:49:58,609 : [('reg:1e-05', 50.16), ('reg:0.0001', 50.15), ('reg:0.001', 50.09), ('reg:0.01', 50.12)]
2019-02-12 20:49:58,609 : Validation : best param found is reg = 1e-05 with score             50.16
2019-02-12 20:49:58,609 : Evaluating...
2019-02-12 20:50:10,131 : 
Dev acc : 50.2 Test acc : 50.2 for COORDINATIONINVERSION classification

2019-02-12 20:50:10,140 : {'STS12': {'MSRpar': {'pearson': (0.3634798165699589, 7.673853849198575e-25), 'spearman': SpearmanrResult(correlation=0.3997694419428671, pvalue=3.768303683766432e-30), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7557501835215009, 1.1617117874998997e-139), 'spearman': SpearmanrResult(correlation=0.7581860004545258, pvalue=4.5808026483292326e-141), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48961696941393046, 4.7948323518174923e-29), 'spearman': SpearmanrResult(correlation=0.5894611547256319, pvalue=2.7326498951014033e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6215585200458827, 2.27392023941898e-81), 'spearman': SpearmanrResult(correlation=0.6451086182352678, pvalue=1.765525820392722e-89), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5225457373554884, 2.451699030501157e-29), 'spearman': SpearmanrResult(correlation=0.43725411380960116, pvalue=4.618036235212196e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5505902453813523, 'wmean': 0.5594663218368213}, 'spearman': {'mean': 0.5659558658335787, 'wmean': 0.5782899314361619}}}, 'STS13': {'FNWN': {'pearson': (0.21807781148377023, 0.0025735827001728434), 'spearman': SpearmanrResult(correlation=0.20919975019973863, pvalue=0.003864963970377738), 'nsamples': 189}, 'headlines': {'pearson': (0.695725960220636, 1.3863848432541341e-109), 'spearman': SpearmanrResult(correlation=0.6873607457725012, pvalue=5.611211509716322e-106), 'nsamples': 750}, 'OnWN': {'pearson': (0.6474096221519898, 5.942014461117843e-68), 'spearman': SpearmanrResult(correlation=0.6456511757253226, pvalue=1.7755784341377614e-67), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5204044646187987, 'wmean': 0.6174719830421173}, 'spearman': {'mean': 0.5140705572325208, 'wmean': 0.6115130811326882}}}, 'STS14': {'deft-forum': {'pearson': (0.41942977486781613, 1.3365097935154013e-20), 'spearman': SpearmanrResult(correlation=0.43591941927240585, pvalue=2.692676977963053e-22), 'nsamples': 450}, 'deft-news': {'pearson': (0.7178704520501891, 8.65922235191604e-49), 'spearman': SpearmanrResult(correlation=0.6857823498319265, pvalue=5.098535573857949e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.6601415931441176, 4.832788244917321e-95), 'spearman': SpearmanrResult(correlation=0.6347780481115839, pvalue=7.784471956664213e-86), 'nsamples': 750}, 'images': {'pearson': (0.6962274893278043, 8.349489595758187e-110), 'spearman': SpearmanrResult(correlation=0.6861508005433177, pvalue=1.8226277867111263e-105), 'nsamples': 750}, 'OnWN': {'pearson': (0.7154079514698037, 1.3882487803546206e-118), 'spearman': SpearmanrResult(correlation=0.738362218281524, pvalue=4.251350780170232e-130), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6095389619574102, 1.7190514323920226e-77), 'spearman': SpearmanrResult(correlation=0.5925470533527082, pvalue=2.770376411813423e-72), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6364360371361902, 'wmean': 0.6440244083279802}, 'spearman': {'mean': 0.6289233148989111, 'wmean': 0.6375405423570696}}}, 'STS15': {'answers-forums': {'pearson': (0.5479755689987685, 8.975859061246794e-31), 'spearman': SpearmanrResult(correlation=0.5250348925920255, pvalue=5.911555088648052e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.7160583558855695, 6.792276605625499e-119), 'spearman': SpearmanrResult(correlation=0.7245155203484552, pvalue=5.1772729938540527e-123), 'nsamples': 750}, 'belief': {'pearson': (0.5698822147925212, 1.1243267897527341e-33), 'spearman': SpearmanrResult(correlation=0.6126539366003817, pvalue=5.171230340304649e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6977468190119592, 1.7853416015797153e-110), 'spearman': SpearmanrResult(correlation=0.7010552535594983, pvalue=6.000538979801117e-112), 'nsamples': 750}, 'images': {'pearson': (0.7854117957038306, 5.497123701788706e-158), 'spearman': SpearmanrResult(correlation=0.7932605149017111, pvalue=2.489299104462609e-163), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6634149508785299, 'wmean': 0.6895364656242511}, 'spearman': {'mean': 0.6713040236004143, 'wmean': 0.6969189258514671}}}, 'STS16': {'answer-answer': {'pearson': (0.43911276605393923, 2.1360648175300349e-13), 'spearman': SpearmanrResult(correlation=0.4473942529805447, pvalue=6.634625212330925e-14), 'nsamples': 254}, 'headlines': {'pearson': (0.6999669069894656, 5.598427941943264e-38), 'spearman': SpearmanrResult(correlation=0.712333943836619, pvalue=7.442370559591558e-40), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7243775499613481, 1.0784243533502777e-38), 'spearman': SpearmanrResult(correlation=0.7326007048417494, pvalue=5.806453070698871e-40), 'nsamples': 230}, 'postediting': {'pearson': (0.7973381347057792, 5.4519019037435585e-55), 'spearman': SpearmanrResult(correlation=0.843466590764093, pvalue=2.965630311314217e-67), 'nsamples': 244}, 'question-question': {'pearson': (0.5813783850113474, 2.714799073798569e-20), 'spearman': SpearmanrResult(correlation=0.5890341462779458, pvalue=6.541021236966973e-21), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.648434748544376, 'wmean': 0.6479694993632119}, 'spearman': {'mean': 0.6649659277401904, 'wmean': 0.6647735574237}}}, 'MR': {'devacc': 59.78, 'acc': 62.16, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.06, 'acc': 67.23, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.68, 'acc': 86.58, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 88.5, 'acc': 89.54, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.49, 'acc': 77.38, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.51, 'acc': 36.61, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.34, 'acc': 70.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.83, 'acc': 71.36, 'f1': 80.24, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.8, 'acc': 75.4, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7978968172552805, 'pearson': 0.801687707607353, 'spearman': 0.7333321394894639, 'mse': 0.36452502670947, 'yhat': array([3.26779042, 4.06687002, 1.40914109, ..., 3.07057839, 4.77973782,
       4.71874024]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6805422197170821, 'pearson': 0.6539986199305371, 'spearman': 0.648530565583907, 'mse': 1.4058035217290834, 'yhat': array([1.35363002, 1.90169274, 2.79531034, ..., 4.12400527, 4.40375328,
       3.40761603]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 70.8, 'acc': 71.61, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 36.57, 'acc': 36.38, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.13, 'acc': 26.94, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 53.37, 'acc': 53.75, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 61.4, 'acc': 61.55, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.13, 'acc': 83.57, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 75.59, 'acc': 74.5, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 74.13, 'acc': 74.3, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 53.43, 'acc': 52.3, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.16, 'acc': 50.23, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 20:50:10,140 : ********************************************************************************
2019-02-12 20:50:10,140 : ********************************************************************************
2019-02-12 20:50:10,140 : ********************************************************************************
2019-02-12 20:50:10,140 : layer 3
2019-02-12 20:50:10,140 : ********************************************************************************
2019-02-12 20:50:10,140 : ********************************************************************************
2019-02-12 20:50:10,140 : ********************************************************************************
2019-02-12 20:50:10,224 : ***** Transfer task : STS12 *****


2019-02-12 20:50:10,236 : loading BERT mode bert-base-uncased
2019-02-12 20:50:10,236 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:50:10,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:50:10,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1rzxrpb4
2019-02-12 20:50:12,722 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:50:16,321 : MSRpar : pearson = 0.3371, spearman = 0.3784
2019-02-12 20:50:17,491 : MSRvid : pearson = 0.7220, spearman = 0.7276
2019-02-12 20:50:18,378 : SMTeuroparl : pearson = 0.5059, spearman = 0.6118
2019-02-12 20:50:19,956 : surprise.OnWN : pearson = 0.5872, spearman = 0.6119
2019-02-12 20:50:20,824 : surprise.SMTnews : pearson = 0.4989, spearman = 0.4198
2019-02-12 20:50:20,824 : ALL (weighted average) : Pearson = 0.5360,             Spearman = 0.5588
2019-02-12 20:50:20,824 : ALL (average) : Pearson = 0.5302,             Spearman = 0.5499

2019-02-12 20:50:20,824 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 20:50:20,832 : loading BERT mode bert-base-uncased
2019-02-12 20:50:20,833 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:50:20,849 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:50:20,850 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpchpuovb4
2019-02-12 20:50:23,277 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:50:25,441 : FNWN : pearson = 0.1986, spearman = 0.1935
2019-02-12 20:50:26,728 : headlines : pearson = 0.6651, spearman = 0.6574
2019-02-12 20:50:27,696 : OnWN : pearson = 0.5921, spearman = 0.5917
2019-02-12 20:50:27,696 : ALL (weighted average) : Pearson = 0.5790,             Spearman = 0.5744
2019-02-12 20:50:27,696 : ALL (average) : Pearson = 0.4853,             Spearman = 0.4809

2019-02-12 20:50:27,696 : ***** Transfer task : STS14 *****


2019-02-12 20:50:27,714 : loading BERT mode bert-base-uncased
2019-02-12 20:50:27,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:50:27,732 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:50:27,732 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbeuf00kk
2019-02-12 20:50:30,165 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:50:32,485 : deft-forum : pearson = 0.4096, spearman = 0.4298
2019-02-12 20:50:33,366 : deft-news : pearson = 0.7105, spearman = 0.6741
2019-02-12 20:50:34,769 : headlines : pearson = 0.6249, spearman = 0.6011
2019-02-12 20:50:36,134 : images : pearson = 0.6389, spearman = 0.6320
2019-02-12 20:50:37,517 : OnWN : pearson = 0.6723, spearman = 0.7020
2019-02-12 20:50:39,198 : tweet-news : pearson = 0.5777, spearman = 0.5568
2019-02-12 20:50:39,198 : ALL (weighted average) : Pearson = 0.6087,             Spearman = 0.6039
2019-02-12 20:50:39,198 : ALL (average) : Pearson = 0.6056,             Spearman = 0.5993

2019-02-12 20:50:39,198 : ***** Transfer task : STS15 *****


2019-02-12 20:50:39,233 : loading BERT mode bert-base-uncased
2019-02-12 20:50:39,234 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:50:39,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:50:39,251 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdy_bzhhi
2019-02-12 20:50:41,688 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:50:44,084 : answers-forums : pearson = 0.5039, spearman = 0.4646
2019-02-12 20:50:45,184 : answers-students : pearson = 0.6941, spearman = 0.7016
2019-02-12 20:50:46,194 : belief : pearson = 0.5406, spearman = 0.5836
2019-02-12 20:50:47,457 : headlines : pearson = 0.6665, spearman = 0.6701
2019-02-12 20:50:48,687 : images : pearson = 0.7591, spearman = 0.7681
2019-02-12 20:50:48,687 : ALL (weighted average) : Pearson = 0.6605,             Spearman = 0.6659
2019-02-12 20:50:48,687 : ALL (average) : Pearson = 0.6328,             Spearman = 0.6376

2019-02-12 20:50:48,687 : ***** Transfer task : STS16 *****


2019-02-12 20:50:48,760 : loading BERT mode bert-base-uncased
2019-02-12 20:50:48,760 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:50:48,779 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:50:48,779 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpioryt7yi
2019-02-12 20:50:51,195 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:50:53,398 : answer-answer : pearson = 0.3998, spearman = 0.4188
2019-02-12 20:50:54,079 : headlines : pearson = 0.6680, spearman = 0.6783
2019-02-12 20:50:54,773 : plagiarism : pearson = 0.6982, spearman = 0.7059
2019-02-12 20:50:55,781 : postediting : pearson = 0.7731, spearman = 0.8313
2019-02-12 20:50:56,368 : question-question : pearson = 0.4767, spearman = 0.4860
2019-02-12 20:50:56,368 : ALL (weighted average) : Pearson = 0.6043,             Spearman = 0.6257
2019-02-12 20:50:56,368 : ALL (average) : Pearson = 0.6032,             Spearman = 0.6241

2019-02-12 20:50:56,368 : ***** Transfer task : MR *****


2019-02-12 20:50:56,384 : loading BERT mode bert-base-uncased
2019-02-12 20:50:56,384 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:50:56,404 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:50:56,404 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0qa_no2w
2019-02-12 20:50:58,787 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:51:00,328 : Generating sentence embeddings
2019-02-12 20:51:22,128 : Generated sentence embeddings
2019-02-12 20:51:22,128 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:51:58,154 : Best param found at split 1: l2reg = 1e-05                 with score 59.09
2019-02-12 20:52:35,552 : Best param found at split 2: l2reg = 0.01                 with score 58.99
2019-02-12 20:53:10,314 : Best param found at split 3: l2reg = 0.01                 with score 54.22
2019-02-12 20:53:38,338 : Best param found at split 4: l2reg = 0.01                 with score 56.27
2019-02-12 20:54:00,475 : Best param found at split 5: l2reg = 1e-05                 with score 59.44
2019-02-12 20:54:01,552 : Dev acc : 57.6 Test acc : 59.16

2019-02-12 20:54:01,553 : ***** Transfer task : CR *****


2019-02-12 20:54:01,560 : loading BERT mode bert-base-uncased
2019-02-12 20:54:01,560 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:54:01,579 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:54:01,580 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiz25c2t0
2019-02-12 20:54:04,010 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:54:05,512 : Generating sentence embeddings
2019-02-12 20:54:13,146 : Generated sentence embeddings
2019-02-12 20:54:13,147 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:54:23,740 : Best param found at split 1: l2reg = 0.01                 with score 71.58
2019-02-12 20:54:29,865 : Best param found at split 2: l2reg = 0.001                 with score 69.23
2019-02-12 20:54:35,076 : Best param found at split 3: l2reg = 0.001                 with score 68.91
2019-02-12 20:54:43,309 : Best param found at split 4: l2reg = 0.001                 with score 71.8
2019-02-12 20:54:51,873 : Best param found at split 5: l2reg = 1e-05                 with score 72.36
2019-02-12 20:54:52,333 : Dev acc : 70.78 Test acc : 70.18

2019-02-12 20:54:52,334 : ***** Transfer task : MPQA *****


2019-02-12 20:54:52,339 : loading BERT mode bert-base-uncased
2019-02-12 20:54:52,339 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:54:52,358 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:54:52,358 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzcogfed8
2019-02-12 20:54:54,761 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:54:56,253 : Generating sentence embeddings
2019-02-12 20:55:06,784 : Generated sentence embeddings
2019-02-12 20:55:06,784 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:55:40,757 : Best param found at split 1: l2reg = 0.0001                 with score 85.91
2019-02-12 20:56:10,061 : Best param found at split 2: l2reg = 0.001                 with score 86.87
2019-02-12 20:56:41,092 : Best param found at split 3: l2reg = 0.01                 with score 85.9
2019-02-12 20:57:04,833 : Best param found at split 4: l2reg = 0.001                 with score 86.74
2019-02-12 20:57:28,067 : Best param found at split 5: l2reg = 0.01                 with score 86.47
2019-02-12 20:57:29,116 : Dev acc : 86.38 Test acc : 85.08

2019-02-12 20:57:29,116 : ***** Transfer task : SUBJ *****


2019-02-12 20:57:29,130 : loading BERT mode bert-base-uncased
2019-02-12 20:57:29,131 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:57:29,152 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:57:29,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp31do4un_
2019-02-12 20:57:31,598 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:57:33,025 : Generating sentence embeddings
2019-02-12 20:57:49,053 : Generated sentence embeddings
2019-02-12 20:57:49,053 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:58:11,946 : Best param found at split 1: l2reg = 0.0001                 with score 85.0
2019-02-12 20:58:55,585 : Best param found at split 2: l2reg = 0.01                 with score 88.34
2019-02-12 20:59:46,527 : Best param found at split 3: l2reg = 1e-05                 with score 87.39
2019-02-12 21:00:38,185 : Best param found at split 4: l2reg = 0.01                 with score 88.56
2019-02-12 21:01:02,354 : Best param found at split 5: l2reg = 1e-05                 with score 83.94
2019-02-12 21:01:03,297 : Dev acc : 86.65 Test acc : 89.43

2019-02-12 21:01:03,298 : ***** Transfer task : SST Binary classification *****


2019-02-12 21:01:03,423 : loading BERT mode bert-base-uncased
2019-02-12 21:01:03,424 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:01:03,445 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:01:03,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9blxfixq
2019-02-12 21:01:05,925 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:01:07,349 : Computing embedding for train
2019-02-12 21:02:11,714 : Computed train embeddings
2019-02-12 21:02:11,715 : Computing embedding for dev
2019-02-12 21:02:13,235 : Computed dev embeddings
2019-02-12 21:02:13,236 : Computing embedding for test
2019-02-12 21:02:16,532 : Computed test embeddings
2019-02-12 21:02:16,532 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:03:23,835 : [('reg:1e-05', 75.8), ('reg:0.0001', 75.23), ('reg:0.001', 74.66), ('reg:0.01', 60.89)]
2019-02-12 21:03:23,835 : Validation : best param found is reg = 1e-05 with score             75.8
2019-02-12 21:03:23,835 : Evaluating...
2019-02-12 21:03:43,537 : 
Dev acc : 75.8 Test acc : 73.92 for             SST Binary classification

2019-02-12 21:03:43,538 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 21:03:43,587 : loading BERT mode bert-base-uncased
2019-02-12 21:03:43,587 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:03:43,607 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:03:43,608 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd42nd1y_
2019-02-12 21:03:46,034 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:03:47,649 : Computing embedding for train
2019-02-12 21:04:06,759 : Computed train embeddings
2019-02-12 21:04:06,759 : Computing embedding for dev
2019-02-12 21:04:09,365 : Computed dev embeddings
2019-02-12 21:04:09,365 : Computing embedding for test
2019-02-12 21:04:15,003 : Computed test embeddings
2019-02-12 21:04:15,003 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:04:25,975 : [('reg:1e-05', 36.78), ('reg:0.0001', 37.69), ('reg:0.001', 29.61), ('reg:0.01', 37.6)]
2019-02-12 21:04:25,975 : Validation : best param found is reg = 0.0001 with score             37.69
2019-02-12 21:04:25,975 : Evaluating...
2019-02-12 21:04:28,654 : 
Dev acc : 37.69 Test acc : 36.88 for             SST Fine-Grained classification

2019-02-12 21:04:28,655 : ***** Transfer task : TREC *****


2019-02-12 21:04:28,668 : loading BERT mode bert-base-uncased
2019-02-12 21:04:28,668 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:04:28,686 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:04:28,686 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9ra4qmmd
2019-02-12 21:04:31,116 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:04:40,067 : Computed train embeddings
2019-02-12 21:04:40,620 : Computed test embeddings
2019-02-12 21:04:40,621 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 21:04:59,372 : [('reg:1e-05', 59.61), ('reg:0.0001', 61.32), ('reg:0.001', 62.09), ('reg:0.01', 61.47)]
2019-02-12 21:04:59,373 : Cross-validation : best param found is reg = 0.001             with score 62.09
2019-02-12 21:04:59,373 : Evaluating...
2019-02-12 21:05:00,221 : 
Dev acc : 62.09 Test acc : 75.2             for TREC

2019-02-12 21:05:00,222 : ***** Transfer task : MRPC *****


2019-02-12 21:05:00,243 : loading BERT mode bert-base-uncased
2019-02-12 21:05:00,243 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:05:00,263 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:05:00,263 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcs1z6eey
2019-02-12 21:05:02,695 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:05:04,183 : Computing embedding for train
2019-02-12 21:05:16,850 : Computed train embeddings
2019-02-12 21:05:16,850 : Computing embedding for test
2019-02-12 21:05:22,271 : Computed test embeddings
2019-02-12 21:05:22,287 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 21:05:31,341 : [('reg:1e-05', 70.58), ('reg:0.0001', 70.51), ('reg:0.001', 70.29), ('reg:0.01', 70.46)]
2019-02-12 21:05:31,341 : Cross-validation : best param found is reg = 1e-05             with score 70.58
2019-02-12 21:05:31,341 : Evaluating...
2019-02-12 21:05:31,813 : Dev acc : 70.58 Test acc 67.71; Test F1 73.26 for MRPC.

2019-02-12 21:05:31,813 : ***** Transfer task : SICK-Entailment*****


2019-02-12 21:05:31,877 : loading BERT mode bert-base-uncased
2019-02-12 21:05:31,877 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:05:31,896 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:05:31,896 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1i8od_tj
2019-02-12 21:05:34,324 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:05:35,773 : Computing embedding for train
2019-02-12 21:05:43,908 : Computed train embeddings
2019-02-12 21:05:43,908 : Computing embedding for dev
2019-02-12 21:05:44,929 : Computed dev embeddings
2019-02-12 21:05:44,929 : Computing embedding for test
2019-02-12 21:05:53,748 : Computed test embeddings
2019-02-12 21:05:53,777 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:05:55,976 : [('reg:1e-05', 79.6), ('reg:0.0001', 77.4), ('reg:0.001', 76.8), ('reg:0.01', 77.6)]
2019-02-12 21:05:55,976 : Validation : best param found is reg = 1e-05 with score             79.6
2019-02-12 21:05:55,976 : Evaluating...
2019-02-12 21:05:56,465 : 
Dev acc : 79.6 Test acc : 78.53 for                        SICK entailment

2019-02-12 21:05:56,466 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 21:05:56,494 : loading BERT mode bert-base-uncased
2019-02-12 21:05:56,494 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:05:56,552 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:05:56,553 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8xy4r7i0
2019-02-12 21:05:59,011 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:06:00,470 : Computing embedding for train
2019-02-12 21:06:09,668 : Computed train embeddings
2019-02-12 21:06:09,668 : Computing embedding for dev
2019-02-12 21:06:10,861 : Computed dev embeddings
2019-02-12 21:06:10,861 : Computing embedding for test
2019-02-12 21:06:19,982 : Computed test embeddings
2019-02-12 21:07:27,326 : Dev : Pearson 0.791443666955994
2019-02-12 21:07:27,326 : Test : Pearson 0.7852568825300805 Spearman 0.7194472533832414 MSE 0.394842839137381                        for SICK Relatedness

2019-02-12 21:07:27,326 : 

***** Transfer task : STSBenchmark*****


2019-02-12 21:07:27,366 : loading BERT mode bert-base-uncased
2019-02-12 21:07:27,366 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:07:27,393 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:07:27,393 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2nh6ox15
2019-02-12 21:07:29,776 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:07:31,389 : Computing embedding for train
2019-02-12 21:07:53,948 : Computed train embeddings
2019-02-12 21:07:53,949 : Computing embedding for dev
2019-02-12 21:08:00,388 : Computed dev embeddings
2019-02-12 21:08:00,388 : Computing embedding for test
2019-02-12 21:08:06,343 : Computed test embeddings
2019-02-12 21:09:11,386 : Dev : Pearson 0.6448378815406637
2019-02-12 21:09:11,386 : Test : Pearson 0.6322046135664458 Spearman 0.6266734611653674 MSE 1.4739610306651059                        for SICK Relatedness

2019-02-12 21:09:11,387 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 21:09:11,704 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 21:09:11,714 : loading BERT mode bert-base-uncased
2019-02-12 21:09:11,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:09:11,737 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:09:11,737 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3sva5lru
2019-02-12 21:09:14,167 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:09:15,591 : Computing embeddings for train/dev/test
2019-02-12 21:11:54,540 : Computed embeddings
2019-02-12 21:11:54,541 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:13:20,649 : [('reg:1e-05', 66.57), ('reg:0.0001', 64.04), ('reg:0.001', 63.54), ('reg:0.01', 56.95)]
2019-02-12 21:13:20,649 : Validation : best param found is reg = 1e-05 with score             66.57
2019-02-12 21:13:20,649 : Evaluating...
2019-02-12 21:13:28,269 : 
Dev acc : 66.6 Test acc : 66.6 for LENGTH classification

2019-02-12 21:13:28,277 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 21:13:28,616 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 21:13:28,660 : loading BERT mode bert-base-uncased
2019-02-12 21:13:28,660 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:13:28,688 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:13:28,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoigjcv96
2019-02-12 21:13:31,114 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:13:32,559 : Computing embeddings for train/dev/test
2019-02-12 21:15:55,325 : Computed embeddings
2019-02-12 21:15:55,325 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:17:26,954 : [('reg:1e-05', 48.54), ('reg:0.0001', 19.05), ('reg:0.001', 0.41), ('reg:0.01', 0.1)]
2019-02-12 21:17:26,954 : Validation : best param found is reg = 1e-05 with score             48.54
2019-02-12 21:17:26,954 : Evaluating...
2019-02-12 21:17:50,270 : 
Dev acc : 48.5 Test acc : 48.8 for WORDCONTENT classification

2019-02-12 21:17:50,278 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 21:17:50,624 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 21:17:50,688 : loading BERT mode bert-base-uncased
2019-02-12 21:17:50,688 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:17:50,712 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:17:50,713 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnpbf6g9s
2019-02-12 21:17:53,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:17:54,602 : Computing embeddings for train/dev/test
2019-02-12 21:20:18,370 : Computed embeddings
2019-02-12 21:20:18,371 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:21:57,982 : [('reg:1e-05', 27.91), ('reg:0.0001', 27.52), ('reg:0.001', 24.97), ('reg:0.01', 26.69)]
2019-02-12 21:21:57,982 : Validation : best param found is reg = 1e-05 with score             27.91
2019-02-12 21:21:57,983 : Evaluating...
2019-02-12 21:22:24,979 : 
Dev acc : 27.9 Test acc : 28.1 for DEPTH classification

2019-02-12 21:22:24,987 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 21:22:25,356 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 21:22:25,418 : loading BERT mode bert-base-uncased
2019-02-12 21:22:25,418 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:22:25,524 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:22:25,524 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpceudc_08
2019-02-12 21:22:27,948 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:22:29,412 : Computing embeddings for train/dev/test
2019-02-12 21:24:56,483 : Computed embeddings
2019-02-12 21:24:56,483 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:26:22,503 : [('reg:1e-05', 52.22), ('reg:0.0001', 49.9), ('reg:0.001', 46.2), ('reg:0.01', 33.85)]
2019-02-12 21:26:22,503 : Validation : best param found is reg = 1e-05 with score             52.22
2019-02-12 21:26:22,504 : Evaluating...
2019-02-12 21:26:44,859 : 
Dev acc : 52.2 Test acc : 52.9 for TOPCONSTITUENTS classification

2019-02-12 21:26:44,867 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 21:26:45,204 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 21:26:45,269 : loading BERT mode bert-base-uncased
2019-02-12 21:26:45,269 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:26:45,389 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:26:45,389 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvwswg3e_
2019-02-12 21:26:47,817 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:26:49,293 : Computing embeddings for train/dev/test
2019-02-12 21:29:09,900 : Computed embeddings
2019-02-12 21:29:09,901 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:30:10,896 : [('reg:1e-05', 70.77), ('reg:0.0001', 70.62), ('reg:0.001', 75.07), ('reg:0.01', 73.32)]
2019-02-12 21:30:10,896 : Validation : best param found is reg = 0.001 with score             75.07
2019-02-12 21:30:10,896 : Evaluating...
2019-02-12 21:30:33,019 : 
Dev acc : 75.1 Test acc : 74.9 for BIGRAMSHIFT classification

2019-02-12 21:30:33,028 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 21:30:33,584 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 21:30:33,649 : loading BERT mode bert-base-uncased
2019-02-12 21:30:33,649 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:30:33,679 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:30:33,679 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe9s36bq3
2019-02-12 21:30:36,100 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:30:37,563 : Computing embeddings for train/dev/test
2019-02-12 21:32:48,779 : Computed embeddings
2019-02-12 21:32:48,780 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:33:42,853 : [('reg:1e-05', 84.56), ('reg:0.0001', 84.55), ('reg:0.001', 84.72), ('reg:0.01', 84.24)]
2019-02-12 21:33:42,853 : Validation : best param found is reg = 0.001 with score             84.72
2019-02-12 21:33:42,853 : Evaluating...
2019-02-12 21:33:58,127 : 
Dev acc : 84.7 Test acc : 83.2 for TENSE classification

2019-02-12 21:33:58,136 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 21:33:58,697 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 21:33:58,759 : loading BERT mode bert-base-uncased
2019-02-12 21:33:58,759 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:33:58,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:33:58,784 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpns05d691
2019-02-12 21:34:01,210 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:34:02,661 : Computing embeddings for train/dev/test
2019-02-12 21:36:41,362 : Computed embeddings
2019-02-12 21:36:41,362 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:38:09,352 : [('reg:1e-05', 76.77), ('reg:0.0001', 76.86), ('reg:0.001', 76.86), ('reg:0.01', 74.77)]
2019-02-12 21:38:09,352 : Validation : best param found is reg = 0.0001 with score             76.86
2019-02-12 21:38:09,352 : Evaluating...
2019-02-12 21:38:44,306 : 
Dev acc : 76.9 Test acc : 75.8 for SUBJNUMBER classification

2019-02-12 21:38:44,314 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 21:38:44,747 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 21:38:44,815 : loading BERT mode bert-base-uncased
2019-02-12 21:38:44,815 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:38:44,842 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:38:44,842 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkutv7iw6
2019-02-12 21:38:47,256 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:38:48,845 : Computing embeddings for train/dev/test
2019-02-12 21:41:13,067 : Computed embeddings
2019-02-12 21:41:13,067 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:42:26,347 : [('reg:1e-05', 75.44), ('reg:0.0001', 75.12), ('reg:0.001', 75.28), ('reg:0.01', 73.9)]
2019-02-12 21:42:26,347 : Validation : best param found is reg = 1e-05 with score             75.44
2019-02-12 21:42:26,347 : Evaluating...
2019-02-12 21:42:45,462 : 
Dev acc : 75.4 Test acc : 76.0 for OBJNUMBER classification

2019-02-12 21:42:45,469 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 21:42:45,856 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 21:42:45,924 : loading BERT mode bert-base-uncased
2019-02-12 21:42:45,925 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:42:46,046 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:42:46,046 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9en08r1h
2019-02-12 21:42:48,482 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:42:49,900 : Computing embeddings for train/dev/test
2019-02-12 21:45:03,808 : Computed embeddings
2019-02-12 21:45:03,808 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:46:08,903 : [('reg:1e-05', 54.04), ('reg:0.0001', 54.05), ('reg:0.001', 54.07), ('reg:0.01', 54.16)]
2019-02-12 21:46:08,904 : Validation : best param found is reg = 0.01 with score             54.16
2019-02-12 21:46:08,904 : Evaluating...
2019-02-12 21:46:24,505 : 
Dev acc : 54.2 Test acc : 53.7 for ODDMANOUT classification

2019-02-12 21:46:24,516 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 21:46:25,114 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 21:46:25,188 : loading BERT mode bert-base-uncased
2019-02-12 21:46:25,189 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:46:25,218 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:46:25,219 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo4e8qkqw
2019-02-12 21:46:27,624 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:46:29,115 : Computing embeddings for train/dev/test
2019-02-12 21:49:03,940 : Computed embeddings
2019-02-12 21:49:03,940 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:50:19,097 : [('reg:1e-05', 55.39), ('reg:0.0001', 55.4), ('reg:0.001', 55.13), ('reg:0.01', 50.05)]
2019-02-12 21:50:19,098 : Validation : best param found is reg = 0.0001 with score             55.4
2019-02-12 21:50:19,098 : Evaluating...
2019-02-12 21:50:45,472 : 
Dev acc : 55.4 Test acc : 55.1 for COORDINATIONINVERSION classification

2019-02-12 21:50:45,482 : {'STS12': {'MSRpar': {'pearson': (0.3371374024601905, 2.16204792579965e-21), 'spearman': SpearmanrResult(correlation=0.37835821963228683, pvalue=6.151770146981816e-27), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7219649848677115, 9.377832797477276e-122), 'spearman': SpearmanrResult(correlation=0.7276171998366326, pvalue=1.462179004227035e-124), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5058900780829677, 3.383988020553368e-31), 'spearman': SpearmanrResult(correlation=0.6117555394468404, pvalue=1.8281546245001072e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5872227928562502, 1.0261105542691276e-70), 'spearman': SpearmanrResult(correlation=0.6119219008015674, pvalue=3.017149545160747e-78), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4988595141311005, 1.7043958373204728e-26), 'spearman': SpearmanrResult(correlation=0.41983308799244123, pvalue=1.813896697358388e-18), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5302149544796441, 'wmean': 0.5360335833708191}, 'spearman': {'mean': 0.5498971895419537, 'wmean': 0.5587941392914894}}}, 'STS13': {'FNWN': {'pearson': (0.1986497202199223, 0.0061396490765262815), 'spearman': SpearmanrResult(correlation=0.19348562488236565, pvalue=0.007639578361229931), 'nsamples': 189}, 'headlines': {'pearson': (0.6650815190106989, 6.100249749442635e-97), 'spearman': SpearmanrResult(correlation=0.6573867443227979, pvalue=5.340059954002557e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.592092600274722, 2.2716269158635486e-54), 'spearman': SpearmanrResult(correlation=0.5916855444993537, pvalue=2.7968244346607414e-54), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4852746131684477, 'wmean': 0.5790132567558056}, 'spearman': {'mean': 0.48085263790150573, 'wmean': 0.5743629545393353}}}, 'STS14': {'deft-forum': {'pearson': (0.4096139598692221, 1.2357953810810065e-19), 'spearman': SpearmanrResult(correlation=0.4298129096199732, pvalue=1.1723479388408378e-21), 'nsamples': 450}, 'deft-news': {'pearson': (0.7104545186683172, 2.1925134192304404e-47), 'spearman': SpearmanrResult(correlation=0.674121527910147, pvalue=4.2030409883844686e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.6248657234122955, 1.8186050433501845e-82), 'spearman': SpearmanrResult(correlation=0.6010865555559064, pvalue=7.322383852057969e-75), 'nsamples': 750}, 'images': {'pearson': (0.6388911458350115, 2.86408387793588e-87), 'spearman': SpearmanrResult(correlation=0.6320273267933535, pvalue=6.891627944462661e-85), 'nsamples': 750}, 'OnWN': {'pearson': (0.672274403482019, 9.005225979504906e-100), 'spearman': SpearmanrResult(correlation=0.7020136221038634, pvalue=2.226057190011703e-112), 'nsamples': 750}, 'tweet-news': {'pearson': (0.577708241823742, 5.53458718551228e-68), 'spearman': SpearmanrResult(correlation=0.5567625004451875, pvalue=2.8255787917592806e-62), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6056346655151011, 'wmean': 0.6087379395883855}, 'spearman': {'mean': 0.5993040737380718, 'wmean': 0.6038852723668707}}}, 'STS15': {'answers-forums': {'pearson': (0.5038542601423485, 1.5493531705608256e-25), 'spearman': SpearmanrResult(correlation=0.464552481839159, pvalue=1.7960695466679514e-21), 'nsamples': 375}, 'answers-students': {'pearson': (0.6940773207970337, 7.2880116576492085e-109), 'spearman': SpearmanrResult(correlation=0.7015552937530586, pvalue=3.578546946112438e-112), 'nsamples': 750}, 'belief': {'pearson': (0.5406420820926097, 7.545740619714642e-30), 'spearman': SpearmanrResult(correlation=0.5836365567656353, pvalue=1.3026592271155328e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.6665141215397001, 1.6898349447002245e-97), 'spearman': SpearmanrResult(correlation=0.670072746785518, pvalue=6.752327207492537e-99), 'nsamples': 750}, 'images': {'pearson': (0.7591257727046778, 1.3023512416012964e-141), 'spearman': SpearmanrResult(correlation=0.7680530145980171, pvalue=6.27585746036151e-147), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.632842711455274, 'wmean': 0.6604913465397226}, 'spearman': {'mean': 0.6375740187482777, 'wmean': 0.6659438936097477}}}, 'STS16': {'answer-answer': {'pearson': (0.39984267252105193, 3.611949400289921e-11), 'spearman': SpearmanrResult(correlation=0.4188248445219572, pvalue=3.2869414744136386e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.6680059402682259, 1.5108790561694613e-33), 'spearman': SpearmanrResult(correlation=0.6783460208795807, pvalue=6.413220306437679e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6981691985608248, 6.129810389786437e-35), 'spearman': SpearmanrResult(correlation=0.7058627507329153, pvalue=5.348396991179477e-36), 'nsamples': 230}, 'postediting': {'pearson': (0.7730944925913059, 9.418205517230193e-50), 'spearman': SpearmanrResult(correlation=0.8313356067330445, pvalue=1.1317969704684177e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.4767286954950688, 2.945899457793772e-13), 'spearman': SpearmanrResult(correlation=0.4860404290964914, pvalue=8.64163838701246e-14), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6031681998872955, 'wmean': 0.6043370886735862}, 'spearman': {'mean': 0.6240819303927978, 'wmean': 0.6256883980608708}}}, 'MR': {'devacc': 57.6, 'acc': 59.16, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.78, 'acc': 70.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.38, 'acc': 85.08, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 86.65, 'acc': 89.43, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 75.8, 'acc': 73.92, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.69, 'acc': 36.88, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 62.09, 'acc': 75.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.58, 'acc': 67.71, 'f1': 73.26, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.6, 'acc': 78.53, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.791443666955994, 'pearson': 0.7852568825300805, 'spearman': 0.7194472533832414, 'mse': 0.394842839137381, 'yhat': array([3.01226072, 4.15230138, 1.1619529 , ..., 3.12687879, 4.55433739,
       4.85673748]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6448378815406637, 'pearson': 0.6322046135664458, 'spearman': 0.6266734611653674, 'mse': 1.4739610306651059, 'yhat': array([1.04024258, 1.74381548, 1.91207808, ..., 4.08654689, 4.20948703,
       3.64535713]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 66.57, 'acc': 66.64, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 48.54, 'acc': 48.8, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.91, 'acc': 28.07, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 52.22, 'acc': 52.92, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 75.07, 'acc': 74.89, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 84.72, 'acc': 83.22, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 76.86, 'acc': 75.76, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.44, 'acc': 76.0, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.16, 'acc': 53.73, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.4, 'acc': 55.07, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 21:50:45,482 : ********************************************************************************
2019-02-12 21:50:45,482 : ********************************************************************************
2019-02-12 21:50:45,482 : ********************************************************************************
2019-02-12 21:50:45,482 : layer 4
2019-02-12 21:50:45,482 : ********************************************************************************
2019-02-12 21:50:45,482 : ********************************************************************************
2019-02-12 21:50:45,482 : ********************************************************************************
2019-02-12 21:50:45,570 : ***** Transfer task : STS12 *****


2019-02-12 21:50:45,582 : loading BERT mode bert-base-uncased
2019-02-12 21:50:45,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:50:45,599 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:50:45,599 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr1tqdbrx
2019-02-12 21:50:48,001 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:50:52,665 : MSRpar : pearson = 0.3122, spearman = 0.3482
2019-02-12 21:50:55,112 : MSRvid : pearson = 0.6635, spearman = 0.6741
2019-02-12 21:50:56,951 : SMTeuroparl : pearson = 0.5058, spearman = 0.6124
2019-02-12 21:50:59,348 : surprise.OnWN : pearson = 0.5652, spearman = 0.5960
2019-02-12 21:51:00,615 : surprise.SMTnews : pearson = 0.5155, spearman = 0.4414
2019-02-12 21:51:00,615 : ALL (weighted average) : Pearson = 0.5127,             Spearman = 0.5376
2019-02-12 21:51:00,615 : ALL (average) : Pearson = 0.5124,             Spearman = 0.5344

2019-02-12 21:51:00,616 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 21:51:00,623 : loading BERT mode bert-base-uncased
2019-02-12 21:51:00,623 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:51:00,641 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:51:00,641 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyg1pxvmx
2019-02-12 21:51:03,027 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:51:05,665 : FNWN : pearson = 0.1599, spearman = 0.1611
2019-02-12 21:51:08,296 : headlines : pearson = 0.6460, spearman = 0.6406
2019-02-12 21:51:10,349 : OnWN : pearson = 0.5458, spearman = 0.5456
2019-02-12 21:51:10,349 : ALL (weighted average) : Pearson = 0.5473,             Spearman = 0.5446
2019-02-12 21:51:10,349 : ALL (average) : Pearson = 0.4506,             Spearman = 0.4491

2019-02-12 21:51:10,349 : ***** Transfer task : STS14 *****


2019-02-12 21:51:10,372 : loading BERT mode bert-base-uncased
2019-02-12 21:51:10,372 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:51:10,419 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:51:10,419 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpliexk14x
2019-02-12 21:51:12,850 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:51:15,598 : deft-forum : pearson = 0.3898, spearman = 0.4053
2019-02-12 21:51:16,708 : deft-news : pearson = 0.7204, spearman = 0.6878
2019-02-12 21:51:18,855 : headlines : pearson = 0.6031, spearman = 0.5777
2019-02-12 21:51:20,944 : images : pearson = 0.5948, spearman = 0.5932
2019-02-12 21:51:23,092 : OnWN : pearson = 0.6363, spearman = 0.6681
2019-02-12 21:51:25,589 : tweet-news : pearson = 0.5656, spearman = 0.5427
2019-02-12 21:51:25,590 : ALL (weighted average) : Pearson = 0.5844,             Spearman = 0.5800
2019-02-12 21:51:25,590 : ALL (average) : Pearson = 0.5850,             Spearman = 0.5791

2019-02-12 21:51:25,590 : ***** Transfer task : STS15 *****


2019-02-12 21:51:25,621 : loading BERT mode bert-base-uncased
2019-02-12 21:51:25,621 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:51:25,638 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:51:25,638 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg923wsd8
2019-02-12 21:51:28,052 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:51:30,992 : answers-forums : pearson = 0.5056, spearman = 0.4758
2019-02-12 21:51:33,307 : answers-students : pearson = 0.6828, spearman = 0.6882
2019-02-12 21:51:34,854 : belief : pearson = 0.5274, spearman = 0.5690
2019-02-12 21:51:37,305 : headlines : pearson = 0.6427, spearman = 0.6496
2019-02-12 21:51:39,736 : images : pearson = 0.7302, spearman = 0.7400
2019-02-12 21:51:39,736 : ALL (weighted average) : Pearson = 0.6430,             Spearman = 0.6500
2019-02-12 21:51:39,736 : ALL (average) : Pearson = 0.6177,             Spearman = 0.6245

2019-02-12 21:51:39,736 : ***** Transfer task : STS16 *****


2019-02-12 21:51:39,806 : loading BERT mode bert-base-uncased
2019-02-12 21:51:39,807 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:51:39,825 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:51:39,825 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb5ejmb6_
2019-02-12 21:51:42,252 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:51:44,621 : answer-answer : pearson = 0.4276, spearman = 0.4392
2019-02-12 21:51:45,503 : headlines : pearson = 0.6432, spearman = 0.6527
2019-02-12 21:51:46,477 : plagiarism : pearson = 0.6693, spearman = 0.6792
2019-02-12 21:51:47,719 : postediting : pearson = 0.7621, spearman = 0.8150
2019-02-12 21:51:48,457 : question-question : pearson = 0.4029, spearman = 0.4173
2019-02-12 21:51:48,458 : ALL (weighted average) : Pearson = 0.5842,             Spearman = 0.6040
2019-02-12 21:51:48,458 : ALL (average) : Pearson = 0.5810,             Spearman = 0.6007

2019-02-12 21:51:48,458 : ***** Transfer task : MR *****


2019-02-12 21:51:48,475 : loading BERT mode bert-base-uncased
2019-02-12 21:51:48,475 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:51:48,495 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:51:48,496 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf8lsxv28
2019-02-12 21:51:50,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:51:52,497 : Generating sentence embeddings
2019-02-12 21:52:12,906 : Generated sentence embeddings
2019-02-12 21:52:12,906 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:52:28,723 : Best param found at split 1: l2reg = 0.01                 with score 60.75
2019-02-12 21:52:47,019 : Best param found at split 2: l2reg = 0.001                 with score 58.98
2019-02-12 21:53:21,126 : Best param found at split 3: l2reg = 0.001                 with score 56.0
2019-02-12 21:53:49,507 : Best param found at split 4: l2reg = 1e-05                 with score 56.05
2019-02-12 21:54:30,226 : Best param found at split 5: l2reg = 1e-05                 with score 60.62
2019-02-12 21:54:33,261 : Dev acc : 58.48 Test acc : 58.67

2019-02-12 21:54:33,262 : ***** Transfer task : CR *****


2019-02-12 21:54:33,270 : loading BERT mode bert-base-uncased
2019-02-12 21:54:33,270 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:54:33,289 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:54:33,289 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsc74etam
2019-02-12 21:54:35,668 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:54:37,199 : Generating sentence embeddings
2019-02-12 21:54:45,000 : Generated sentence embeddings
2019-02-12 21:54:45,000 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:54:55,669 : Best param found at split 1: l2reg = 0.001                 with score 68.46
2019-02-12 21:55:03,952 : Best param found at split 2: l2reg = 0.01                 with score 67.04
2019-02-12 21:55:13,079 : Best param found at split 3: l2reg = 0.0001                 with score 69.2
2019-02-12 21:55:21,157 : Best param found at split 4: l2reg = 0.001                 with score 73.65
2019-02-12 21:55:26,215 : Best param found at split 5: l2reg = 1e-05                 with score 73.62
2019-02-12 21:55:26,417 : Dev acc : 70.39 Test acc : 66.38

2019-02-12 21:55:26,418 : ***** Transfer task : MPQA *****


2019-02-12 21:55:26,423 : loading BERT mode bert-base-uncased
2019-02-12 21:55:26,423 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:55:26,442 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:55:26,442 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9cx8m5eo
2019-02-12 21:55:28,872 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:55:30,310 : Generating sentence embeddings
2019-02-12 21:55:36,838 : Generated sentence embeddings
2019-02-12 21:55:36,839 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:55:53,680 : Best param found at split 1: l2reg = 1e-05                 with score 86.09
2019-02-12 21:56:12,314 : Best param found at split 2: l2reg = 0.0001                 with score 87.0
2019-02-12 21:56:45,381 : Best param found at split 3: l2reg = 0.001                 with score 86.6
2019-02-12 21:57:12,150 : Best param found at split 4: l2reg = 0.0001                 with score 86.28
2019-02-12 21:57:48,863 : Best param found at split 5: l2reg = 0.01                 with score 86.52
2019-02-12 21:57:50,284 : Dev acc : 86.5 Test acc : 85.2

2019-02-12 21:57:50,285 : ***** Transfer task : SUBJ *****


2019-02-12 21:57:50,299 : loading BERT mode bert-base-uncased
2019-02-12 21:57:50,299 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:57:50,319 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:57:50,319 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8aafgb90
2019-02-12 21:57:52,711 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:57:54,256 : Generating sentence embeddings
2019-02-12 21:58:16,552 : Generated sentence embeddings
2019-02-12 21:58:16,552 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:58:45,096 : Best param found at split 1: l2reg = 1e-05                 with score 88.4
2019-02-12 21:59:17,849 : Best param found at split 2: l2reg = 0.0001                 with score 86.64
2019-02-12 21:59:51,701 : Best param found at split 3: l2reg = 0.01                 with score 87.7
2019-02-12 22:00:28,740 : Best param found at split 4: l2reg = 1e-05                 with score 89.39
2019-02-12 22:01:00,742 : Best param found at split 5: l2reg = 1e-05                 with score 87.49
2019-02-12 22:01:02,541 : Dev acc : 87.92 Test acc : 89.93

2019-02-12 22:01:02,542 : ***** Transfer task : SST Binary classification *****


2019-02-12 22:01:02,667 : loading BERT mode bert-base-uncased
2019-02-12 22:01:02,668 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:01:02,691 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:01:02,691 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp445jj7dr
2019-02-12 22:01:05,110 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:01:06,602 : Computing embedding for train
2019-02-12 22:02:26,918 : Computed train embeddings
2019-02-12 22:02:26,918 : Computing embedding for dev
2019-02-12 22:02:28,223 : Computed dev embeddings
2019-02-12 22:02:28,223 : Computing embedding for test
2019-02-12 22:02:31,107 : Computed test embeddings
2019-02-12 22:02:31,107 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:03:42,056 : [('reg:1e-05', 76.95), ('reg:0.0001', 76.83), ('reg:0.001', 76.83), ('reg:0.01', 62.73)]
2019-02-12 22:03:42,056 : Validation : best param found is reg = 1e-05 with score             76.95
2019-02-12 22:03:42,056 : Evaluating...
2019-02-12 22:04:01,738 : 
Dev acc : 76.95 Test acc : 76.22 for             SST Binary classification

2019-02-12 22:04:01,738 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 22:04:01,791 : loading BERT mode bert-base-uncased
2019-02-12 22:04:01,791 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:04:01,812 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:04:01,812 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbyuanft8
2019-02-12 22:04:04,232 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:04:05,739 : Computing embedding for train
2019-02-12 22:04:22,599 : Computed train embeddings
2019-02-12 22:04:22,599 : Computing embedding for dev
2019-02-12 22:04:24,454 : Computed dev embeddings
2019-02-12 22:04:24,454 : Computing embedding for test
2019-02-12 22:04:28,189 : Computed test embeddings
2019-02-12 22:04:28,190 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:04:33,692 : [('reg:1e-05', 31.88), ('reg:0.0001', 32.33), ('reg:0.001', 29.25), ('reg:0.01', 30.15)]
2019-02-12 22:04:33,692 : Validation : best param found is reg = 0.0001 with score             32.33
2019-02-12 22:04:33,692 : Evaluating...
2019-02-12 22:04:35,143 : 
Dev acc : 32.33 Test acc : 30.27 for             SST Fine-Grained classification

2019-02-12 22:04:35,143 : ***** Transfer task : TREC *****


2019-02-12 22:04:35,157 : loading BERT mode bert-base-uncased
2019-02-12 22:04:35,157 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:04:35,175 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:04:35,175 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8je6jvy9
2019-02-12 22:04:37,598 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:04:46,102 : Computed train embeddings
2019-02-12 22:04:46,715 : Computed test embeddings
2019-02-12 22:04:46,716 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 22:05:05,561 : [('reg:1e-05', 66.27), ('reg:0.0001', 62.52), ('reg:0.001', 63.85), ('reg:0.01', 59.75)]
2019-02-12 22:05:05,562 : Cross-validation : best param found is reg = 1e-05             with score 66.27
2019-02-12 22:05:05,562 : Evaluating...
2019-02-12 22:05:06,479 : 
Dev acc : 66.27 Test acc : 80.2             for TREC

2019-02-12 22:05:06,480 : ***** Transfer task : MRPC *****


2019-02-12 22:05:06,502 : loading BERT mode bert-base-uncased
2019-02-12 22:05:06,502 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:05:06,521 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:05:06,521 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpycytuzh3
2019-02-12 22:05:08,971 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:05:10,515 : Computing embedding for train
2019-02-12 22:05:25,641 : Computed train embeddings
2019-02-12 22:05:25,642 : Computing embedding for test
2019-02-12 22:05:32,454 : Computed test embeddings
2019-02-12 22:05:32,470 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 22:05:45,349 : [('reg:1e-05', 71.64), ('reg:0.0001', 70.95), ('reg:0.001', 71.02), ('reg:0.01', 70.58)]
2019-02-12 22:05:45,349 : Cross-validation : best param found is reg = 1e-05             with score 71.64
2019-02-12 22:05:45,349 : Evaluating...
2019-02-12 22:05:46,161 : Dev acc : 71.64 Test acc 72.93; Test F1 81.65 for MRPC.

2019-02-12 22:05:46,162 : ***** Transfer task : SICK-Entailment*****


2019-02-12 22:05:46,225 : loading BERT mode bert-base-uncased
2019-02-12 22:05:46,226 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:05:46,244 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:05:46,244 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbghdwkme
2019-02-12 22:05:48,669 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:05:50,152 : Computing embedding for train
2019-02-12 22:06:03,729 : Computed train embeddings
2019-02-12 22:06:03,729 : Computing embedding for dev
2019-02-12 22:06:05,544 : Computed dev embeddings
2019-02-12 22:06:05,544 : Computing embedding for test
2019-02-12 22:06:18,042 : Computed test embeddings
2019-02-12 22:06:18,070 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:06:21,142 : [('reg:1e-05', 75.2), ('reg:0.0001', 71.4), ('reg:0.001', 76.2), ('reg:0.01', 71.6)]
2019-02-12 22:06:21,143 : Validation : best param found is reg = 0.001 with score             76.2
2019-02-12 22:06:21,143 : Evaluating...
2019-02-12 22:06:21,992 : 
Dev acc : 76.2 Test acc : 76.74 for                        SICK entailment

2019-02-12 22:06:21,993 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 22:06:22,020 : loading BERT mode bert-base-uncased
2019-02-12 22:06:22,020 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:06:22,039 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:06:22,039 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpimc7d10i
2019-02-12 22:06:24,468 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:06:26,016 : Computing embedding for train
2019-02-12 22:06:35,339 : Computed train embeddings
2019-02-12 22:06:35,339 : Computing embedding for dev
2019-02-12 22:06:36,379 : Computed dev embeddings
2019-02-12 22:06:36,379 : Computing embedding for test
2019-02-12 22:06:44,009 : Computed test embeddings
2019-02-12 22:07:31,138 : Dev : Pearson 0.783497781215318
2019-02-12 22:07:31,138 : Test : Pearson 0.7914938097001635 Spearman 0.730879495418651 MSE 0.3827408204461735                        for SICK Relatedness

2019-02-12 22:07:31,139 : 

***** Transfer task : STSBenchmark*****


2019-02-12 22:07:31,178 : loading BERT mode bert-base-uncased
2019-02-12 22:07:31,178 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:07:31,206 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:07:31,206 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmple4nkoyi
2019-02-12 22:07:33,656 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:07:35,142 : Computing embedding for train
2019-02-12 22:07:51,750 : Computed train embeddings
2019-02-12 22:07:51,750 : Computing embedding for dev
2019-02-12 22:07:56,906 : Computed dev embeddings
2019-02-12 22:07:56,906 : Computing embedding for test
2019-02-12 22:08:01,577 : Computed test embeddings
2019-02-12 22:09:20,622 : Dev : Pearson 0.6325067065473199
2019-02-12 22:09:20,622 : Test : Pearson 0.6318460324632185 Spearman 0.6285202863684332 MSE 1.465290329959281                        for SICK Relatedness

2019-02-12 22:09:20,622 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 22:09:20,945 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 22:09:20,954 : loading BERT mode bert-base-uncased
2019-02-12 22:09:20,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:09:20,979 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:09:20,980 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp075jdv1h
2019-02-12 22:09:23,395 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:09:24,864 : Computing embeddings for train/dev/test
2019-02-12 22:12:03,352 : Computed embeddings
2019-02-12 22:12:03,352 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:13:30,702 : [('reg:1e-05', 68.98), ('reg:0.0001', 67.34), ('reg:0.001', 60.38), ('reg:0.01', 67.69)]
2019-02-12 22:13:30,702 : Validation : best param found is reg = 1e-05 with score             68.98
2019-02-12 22:13:30,702 : Evaluating...
2019-02-12 22:13:50,164 : 
Dev acc : 69.0 Test acc : 67.7 for LENGTH classification

2019-02-12 22:13:50,173 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 22:13:50,517 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 22:13:50,562 : loading BERT mode bert-base-uncased
2019-02-12 22:13:50,562 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:13:50,589 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:13:50,589 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4kuule6x
2019-02-12 22:13:53,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:13:54,444 : Computing embeddings for train/dev/test
2019-02-12 22:16:15,819 : Computed embeddings
2019-02-12 22:16:15,820 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:18:38,245 : [('reg:1e-05', 31.55), ('reg:0.0001', 7.57), ('reg:0.001', 0.35), ('reg:0.01', 0.16)]
2019-02-12 22:18:38,245 : Validation : best param found is reg = 1e-05 with score             31.55
2019-02-12 22:18:38,245 : Evaluating...
2019-02-12 22:19:08,883 : 
Dev acc : 31.6 Test acc : 32.5 for WORDCONTENT classification

2019-02-12 22:19:08,891 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 22:19:09,237 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 22:19:09,301 : loading BERT mode bert-base-uncased
2019-02-12 22:19:09,301 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:19:09,397 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:19:09,397 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp35iwpwc5
2019-02-12 22:19:11,825 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:19:13,253 : Computing embeddings for train/dev/test
2019-02-12 22:22:07,218 : Computed embeddings
2019-02-12 22:22:07,218 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:23:17,470 : [('reg:1e-05', 30.73), ('reg:0.0001', 32.37), ('reg:0.001', 27.3), ('reg:0.01', 27.4)]
2019-02-12 22:23:17,470 : Validation : best param found is reg = 0.0001 with score             32.37
2019-02-12 22:23:17,470 : Evaluating...
2019-02-12 22:23:29,100 : 
Dev acc : 32.4 Test acc : 32.2 for DEPTH classification

2019-02-12 22:23:29,109 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 22:23:29,483 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 22:23:29,546 : loading BERT mode bert-base-uncased
2019-02-12 22:23:29,546 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:23:29,655 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:23:29,655 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4tqtp9tl
2019-02-12 22:23:32,085 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:23:33,521 : Computing embeddings for train/dev/test
2019-02-12 22:26:13,730 : Computed embeddings
2019-02-12 22:26:13,730 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:26:57,156 : [('reg:1e-05', 54.32), ('reg:0.0001', 51.41), ('reg:0.001', 51.41), ('reg:0.01', 45.3)]
2019-02-12 22:26:57,157 : Validation : best param found is reg = 1e-05 with score             54.32
2019-02-12 22:26:57,157 : Evaluating...
2019-02-12 22:27:05,932 : 
Dev acc : 54.3 Test acc : 53.9 for TOPCONSTITUENTS classification

2019-02-12 22:27:05,942 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 22:27:06,459 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 22:27:06,525 : loading BERT mode bert-base-uncased
2019-02-12 22:27:06,525 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:27:06,555 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:27:06,556 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxh__z_xs
2019-02-12 22:27:09,006 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:27:10,434 : Computing embeddings for train/dev/test
2019-02-12 22:29:45,067 : Computed embeddings
2019-02-12 22:29:45,067 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:30:46,107 : [('reg:1e-05', 80.61), ('reg:0.0001', 80.76), ('reg:0.001', 80.54), ('reg:0.01', 80.17)]
2019-02-12 22:30:46,107 : Validation : best param found is reg = 0.0001 with score             80.76
2019-02-12 22:30:46,107 : Evaluating...
2019-02-12 22:31:01,654 : 
Dev acc : 80.8 Test acc : 80.0 for BIGRAMSHIFT classification

2019-02-12 22:31:01,665 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 22:31:02,087 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 22:31:02,156 : loading BERT mode bert-base-uncased
2019-02-12 22:31:02,157 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:31:02,189 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:31:02,189 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp785b0pqe
2019-02-12 22:31:04,631 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:31:06,142 : Computing embeddings for train/dev/test
2019-02-12 22:33:59,049 : Computed embeddings
2019-02-12 22:33:59,049 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:34:48,889 : [('reg:1e-05', 85.44), ('reg:0.0001', 85.13), ('reg:0.001', 85.66), ('reg:0.01', 85.37)]
2019-02-12 22:34:48,889 : Validation : best param found is reg = 0.001 with score             85.66
2019-02-12 22:34:48,889 : Evaluating...
2019-02-12 22:35:10,086 : 
Dev acc : 85.7 Test acc : 84.5 for TENSE classification

2019-02-12 22:35:10,093 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 22:35:10,501 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 22:35:10,564 : loading BERT mode bert-base-uncased
2019-02-12 22:35:10,564 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:35:10,589 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:35:10,589 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcgrkact2
2019-02-12 22:35:12,981 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:35:14,518 : Computing embeddings for train/dev/test
2019-02-12 22:38:22,280 : Computed embeddings
2019-02-12 22:38:22,281 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:39:36,451 : [('reg:1e-05', 76.99), ('reg:0.0001', 76.94), ('reg:0.001', 76.64), ('reg:0.01', 77.54)]
2019-02-12 22:39:36,451 : Validation : best param found is reg = 0.01 with score             77.54
2019-02-12 22:39:36,451 : Evaluating...
2019-02-12 22:39:55,129 : 
Dev acc : 77.5 Test acc : 76.9 for SUBJNUMBER classification

2019-02-12 22:39:55,136 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 22:39:55,540 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 22:39:55,606 : loading BERT mode bert-base-uncased
2019-02-12 22:39:55,606 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:39:55,720 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:39:55,720 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3xev4tlk
2019-02-12 22:39:58,155 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:39:59,650 : Computing embeddings for train/dev/test
2019-02-12 22:42:09,891 : Computed embeddings
2019-02-12 22:42:09,891 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:44:04,651 : [('reg:1e-05', 77.24), ('reg:0.0001', 77.28), ('reg:0.001', 77.96), ('reg:0.01', 73.03)]
2019-02-12 22:44:04,651 : Validation : best param found is reg = 0.001 with score             77.96
2019-02-12 22:44:04,651 : Evaluating...
2019-02-12 22:44:29,067 : 
Dev acc : 78.0 Test acc : 78.3 for OBJNUMBER classification

2019-02-12 22:44:29,078 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 22:44:29,457 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 22:44:29,527 : loading BERT mode bert-base-uncased
2019-02-12 22:44:29,527 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:44:29,650 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:44:29,650 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptrt5uy25
2019-02-12 22:44:32,092 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:44:33,512 : Computing embeddings for train/dev/test
2019-02-12 22:46:59,517 : Computed embeddings
2019-02-12 22:46:59,517 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:48:05,955 : [('reg:1e-05', 54.54), ('reg:0.0001', 54.56), ('reg:0.001', 54.65), ('reg:0.01', 54.5)]
2019-02-12 22:48:05,955 : Validation : best param found is reg = 0.001 with score             54.65
2019-02-12 22:48:05,955 : Evaluating...
2019-02-12 22:48:19,612 : 
Dev acc : 54.6 Test acc : 54.7 for ODDMANOUT classification

2019-02-12 22:48:19,621 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 22:48:20,209 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 22:48:20,284 : loading BERT mode bert-base-uncased
2019-02-12 22:48:20,284 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:48:20,316 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:48:20,316 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsx9l563d
2019-02-12 22:48:22,776 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:48:24,200 : Computing embeddings for train/dev/test
2019-02-12 22:51:10,725 : Computed embeddings
2019-02-12 22:51:10,725 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:52:29,612 : [('reg:1e-05', 50.31), ('reg:0.0001', 50.31), ('reg:0.001', 50.23), ('reg:0.01', 50.0)]
2019-02-12 22:52:29,613 : Validation : best param found is reg = 1e-05 with score             50.31
2019-02-12 22:52:29,613 : Evaluating...
2019-02-12 22:52:46,241 : 
Dev acc : 50.3 Test acc : 50.4 for COORDINATIONINVERSION classification

2019-02-12 22:52:46,251 : {'STS12': {'MSRpar': {'pearson': (0.3122166820186254, 2.0238226623146045e-18), 'spearman': SpearmanrResult(correlation=0.34822678766034754, pvalue=8.362187684453667e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6634805037477882, 2.5394182192443723e-96), 'spearman': SpearmanrResult(correlation=0.6740925700318483, pvalue=1.683477177106444e-100), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5058453097875151, 3.4316831861868068e-31), 'spearman': SpearmanrResult(correlation=0.6123676401491357, pvalue=1.388946941942825e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5651728410543488, 1.6149931781347085e-64), 'spearman': SpearmanrResult(correlation=0.5959767288109727, pvalue=2.6090811714756803e-73), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5154976526897959, 1.8135985642270956e-28), 'spearman': SpearmanrResult(correlation=0.4413779210959883, pvalue=1.8768122245103063e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5124425978596147, 'wmean': 0.5127152769405631}, 'spearman': {'mean': 0.5344083295496584, 'wmean': 0.5376153803806721}}}, 'STS13': {'FNWN': {'pearson': (0.15988281869751747, 0.02797760807566723), 'spearman': SpearmanrResult(correlation=0.16114595854290978, pvalue=0.02674690588898557), 'nsamples': 189}, 'headlines': {'pearson': (0.6460458682312775, 8.11394298301438e-90), 'spearman': SpearmanrResult(correlation=0.6405527343152857, pvalue=7.436229646980297e-88), 'nsamples': 750}, 'OnWN': {'pearson': (0.5458352928735449, 7.102851545873179e-45), 'spearman': SpearmanrResult(correlation=0.5455861681609885, pvalue=7.918273546071402e-45), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4505879932674466, 'wmean': 0.5473105688062317}, 'spearman': {'mean': 0.44909495367306135, 'wmean': 0.5446299848262592}}}, 'STS14': {'deft-forum': {'pearson': (0.3898134305088139, 8.827991067139376e-18), 'spearman': SpearmanrResult(correlation=0.4053232422083518, pvalue=3.193671208542357e-19), 'nsamples': 450}, 'deft-news': {'pearson': (0.7203682996529029, 2.8480558859252036e-49), 'spearman': SpearmanrResult(correlation=0.6877514531424567, pvalue=2.3711898243761078e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.6031202936536042, 1.7345420665940543e-75), 'spearman': SpearmanrResult(correlation=0.5777406582022563, pvalue=5.419129335295342e-68), 'nsamples': 750}, 'images': {'pearson': (0.5947537491015102, 6.078294742748783e-73), 'spearman': SpearmanrResult(correlation=0.593220474481269, pvalue=1.7460016956454181e-72), 'nsamples': 750}, 'OnWN': {'pearson': (0.6362910606654603, 2.323681603474666e-86), 'spearman': SpearmanrResult(correlation=0.6681263929618395, pvalue=3.950887143982658e-98), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5655836193557078, 1.2501592945608691e-64), 'spearman': SpearmanrResult(correlation=0.5426678739059951, pvalue=1.1735026709460315e-58), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5849884088229999, 'wmean': 0.5843568201885464}, 'spearman': {'mean': 0.5791383491503614, 'wmean': 0.5800099852266707}}}, 'STS15': {'answers-forums': {'pearson': (0.5055628783243161, 1.0029242159781212e-25), 'spearman': SpearmanrResult(correlation=0.4757557206135593, pvalue=1.4085511798381865e-22), 'nsamples': 375}, 'answers-students': {'pearson': (0.6827747374948985, 4.733345776252249e-104), 'spearman': SpearmanrResult(correlation=0.6882034536503452, pvalue=2.4616242886889924e-106), 'nsamples': 750}, 'belief': {'pearson': (0.5273635517176553, 3.128346655719293e-28), 'spearman': SpearmanrResult(correlation=0.5689677859292421, pvalue=1.5010688684839677e-33), 'nsamples': 375}, 'headlines': {'pearson': (0.6426974847837373, 1.2883963035817603e-88), 'spearman': SpearmanrResult(correlation=0.6495925707135694, pvalue=4.1755382892681335e-91), 'nsamples': 750}, 'images': {'pearson': (0.7302124630732332, 7.116439870351464e-126), 'spearman': SpearmanrResult(correlation=0.7399826647120971, pvalue=5.88582670203036e-131), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6177222230787681, 'wmean': 0.6430369750932137}, 'spearman': {'mean': 0.6245004391237625, 'wmean': 0.650035110586853}}}, 'STS16': {'answer-answer': {'pearson': (0.4276446413957827, 1.024268305111074e-12), 'spearman': SpearmanrResult(correlation=0.4391795305931522, pvalue=2.1162913606095836e-13), 'nsamples': 254}, 'headlines': {'pearson': (0.6431868144792594, 1.8120267649410172e-30), 'spearman': SpearmanrResult(correlation=0.6526859005943635, pvalue=1.299724413444842e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6692523896012285, 3.053116324524378e-31), 'spearman': SpearmanrResult(correlation=0.6791628713184334, pvalue=1.8423526084946455e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.762109653940884, 1.373632377080932e-47), 'spearman': SpearmanrResult(correlation=0.8150003766996614, pvalue=2.8137526921894145e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.4029055695000011, 1.4655067700632883e-09), 'spearman': SpearmanrResult(correlation=0.41734693468607326, pvalue=3.2456599578159416e-10), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5810198137834311, 'wmean': 0.5842034780060901}, 'spearman': {'mean': 0.6006751227783368, 'wmean': 0.60401640108432}}}, 'MR': {'devacc': 58.48, 'acc': 58.67, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.39, 'acc': 66.38, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.5, 'acc': 85.2, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 87.92, 'acc': 89.93, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.95, 'acc': 76.22, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 32.33, 'acc': 30.27, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 66.27, 'acc': 80.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.64, 'acc': 72.93, 'f1': 81.65, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.2, 'acc': 76.74, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.783497781215318, 'pearson': 0.7914938097001635, 'spearman': 0.730879495418651, 'mse': 0.3827408204461735, 'yhat': array([3.17077788, 4.14392142, 1.74857454, ..., 3.21604462, 4.31403944,
       4.87393995]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6325067065473199, 'pearson': 0.6318460324632185, 'spearman': 0.6285202863684332, 'mse': 1.465290329959281, 'yhat': array([1.16553882, 1.52775248, 2.11774838, ..., 3.81075196, 4.92069276,
       4.0493054 ]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 68.98, 'acc': 67.73, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 31.55, 'acc': 32.49, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.37, 'acc': 32.25, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.32, 'acc': 53.86, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 80.76, 'acc': 80.01, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.66, 'acc': 84.54, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 77.54, 'acc': 76.94, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.96, 'acc': 78.27, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.65, 'acc': 54.74, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.31, 'acc': 50.37, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 22:52:46,251 : ********************************************************************************
2019-02-12 22:52:46,251 : ********************************************************************************
2019-02-12 22:52:46,251 : ********************************************************************************
2019-02-12 22:52:46,251 : layer 5
2019-02-12 22:52:46,251 : ********************************************************************************
2019-02-12 22:52:46,251 : ********************************************************************************
2019-02-12 22:52:46,252 : ********************************************************************************
2019-02-12 22:52:46,334 : ***** Transfer task : STS12 *****


2019-02-12 22:52:46,346 : loading BERT mode bert-base-uncased
2019-02-12 22:52:46,346 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:52:46,364 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:52:46,364 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1c0hnjsu
2019-02-12 22:52:48,791 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:52:52,407 : MSRpar : pearson = 0.3152, spearman = 0.3534
2019-02-12 22:52:53,431 : MSRvid : pearson = 0.6660, spearman = 0.6758
2019-02-12 22:52:54,203 : SMTeuroparl : pearson = 0.4891, spearman = 0.5965
2019-02-12 22:52:55,572 : surprise.OnWN : pearson = 0.5582, spearman = 0.5846
2019-02-12 22:52:56,327 : surprise.SMTnews : pearson = 0.5278, spearman = 0.4567
2019-02-12 22:52:56,327 : ALL (weighted average) : Pearson = 0.5115,             Spearman = 0.5361
2019-02-12 22:52:56,327 : ALL (average) : Pearson = 0.5113,             Spearman = 0.5334

2019-02-12 22:52:56,327 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 22:52:56,336 : loading BERT mode bert-base-uncased
2019-02-12 22:52:56,336 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:52:56,354 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:52:56,354 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyd7pqnng
2019-02-12 22:52:58,795 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:53:01,367 : FNWN : pearson = 0.1763, spearman = 0.1839
2019-02-12 22:53:03,747 : headlines : pearson = 0.6502, spearman = 0.6435
2019-02-12 22:53:05,615 : OnWN : pearson = 0.5237, spearman = 0.5226
2019-02-12 22:53:05,615 : ALL (weighted average) : Pearson = 0.5432,             Spearman = 0.5404
2019-02-12 22:53:05,615 : ALL (average) : Pearson = 0.4501,             Spearman = 0.4500

2019-02-12 22:53:05,615 : ***** Transfer task : STS14 *****


2019-02-12 22:53:05,633 : loading BERT mode bert-base-uncased
2019-02-12 22:53:05,633 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:53:05,651 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:53:05,651 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcidjiv8_
2019-02-12 22:53:08,057 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:53:11,172 : deft-forum : pearson = 0.3905, spearman = 0.4032
2019-02-12 22:53:12,518 : deft-news : pearson = 0.7182, spearman = 0.6926
2019-02-12 22:53:14,847 : headlines : pearson = 0.5975, spearman = 0.5698
2019-02-12 22:53:16,228 : images : pearson = 0.5945, spearman = 0.5882
2019-02-12 22:53:17,630 : OnWN : pearson = 0.6280, spearman = 0.6583
2019-02-12 22:53:19,329 : tweet-news : pearson = 0.5752, spearman = 0.5472
2019-02-12 22:53:19,329 : ALL (weighted average) : Pearson = 0.5834,             Spearman = 0.5765
2019-02-12 22:53:19,329 : ALL (average) : Pearson = 0.5840,             Spearman = 0.5765

2019-02-12 22:53:19,330 : ***** Transfer task : STS15 *****


2019-02-12 22:53:19,366 : loading BERT mode bert-base-uncased
2019-02-12 22:53:19,366 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:53:19,384 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:53:19,384 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwzhrlx5z
2019-02-12 22:53:21,816 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:53:24,343 : answers-forums : pearson = 0.5000, spearman = 0.4693
2019-02-12 22:53:25,591 : answers-students : pearson = 0.6711, spearman = 0.6776
2019-02-12 22:53:26,552 : belief : pearson = 0.5408, spearman = 0.5792
2019-02-12 22:53:27,767 : headlines : pearson = 0.6493, spearman = 0.6552
2019-02-12 22:53:28,944 : images : pearson = 0.7323, spearman = 0.7411
2019-02-12 22:53:28,944 : ALL (weighted average) : Pearson = 0.6433,             Spearman = 0.6495
2019-02-12 22:53:28,944 : ALL (average) : Pearson = 0.6187,             Spearman = 0.6245

2019-02-12 22:53:28,945 : ***** Transfer task : STS16 *****


2019-02-12 22:53:29,011 : loading BERT mode bert-base-uncased
2019-02-12 22:53:29,012 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:53:29,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:53:29,030 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpijpmq7aw
2019-02-12 22:53:31,488 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:53:33,650 : answer-answer : pearson = 0.4629, spearman = 0.4826
2019-02-12 22:53:34,248 : headlines : pearson = 0.6367, spearman = 0.6429
2019-02-12 22:53:34,919 : plagiarism : pearson = 0.6862, spearman = 0.6994
2019-02-12 22:53:35,880 : postediting : pearson = 0.7654, spearman = 0.8168
2019-02-12 22:53:36,526 : question-question : pearson = 0.3398, spearman = 0.3648
2019-02-12 22:53:36,526 : ALL (weighted average) : Pearson = 0.5832,             Spearman = 0.6063
2019-02-12 22:53:36,526 : ALL (average) : Pearson = 0.5782,             Spearman = 0.6013

2019-02-12 22:53:36,526 : ***** Transfer task : MR *****


2019-02-12 22:53:36,542 : loading BERT mode bert-base-uncased
2019-02-12 22:53:36,543 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:53:36,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:53:36,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplun48r2l
2019-02-12 22:53:38,987 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:53:40,484 : Generating sentence embeddings
2019-02-12 22:53:57,655 : Generated sentence embeddings
2019-02-12 22:53:57,656 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:54:30,717 : Best param found at split 1: l2reg = 0.001                 with score 65.75
2019-02-12 22:55:02,998 : Best param found at split 2: l2reg = 1e-05                 with score 59.68
2019-02-12 22:55:43,216 : Best param found at split 3: l2reg = 0.001                 with score 59.5
2019-02-12 22:56:05,329 : Best param found at split 4: l2reg = 1e-05                 with score 59.31
2019-02-12 22:56:23,156 : Best param found at split 5: l2reg = 1e-05                 with score 62.71
2019-02-12 22:56:24,047 : Dev acc : 61.39 Test acc : 65.13

2019-02-12 22:56:24,048 : ***** Transfer task : CR *****


2019-02-12 22:56:24,056 : loading BERT mode bert-base-uncased
2019-02-12 22:56:24,056 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:56:24,075 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:56:24,075 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3tz7pk13
2019-02-12 22:56:26,520 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:56:27,932 : Generating sentence embeddings
2019-02-12 22:56:32,605 : Generated sentence embeddings
2019-02-12 22:56:32,605 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:56:38,270 : Best param found at split 1: l2reg = 1e-05                 with score 68.4
2019-02-12 22:56:44,636 : Best param found at split 2: l2reg = 0.0001                 with score 71.25
2019-02-12 22:56:54,033 : Best param found at split 3: l2reg = 0.01                 with score 69.7
2019-02-12 22:57:06,213 : Best param found at split 4: l2reg = 0.0001                 with score 72.72
2019-02-12 22:57:17,771 : Best param found at split 5: l2reg = 0.001                 with score 72.92
2019-02-12 22:57:18,357 : Dev acc : 71.0 Test acc : 66.68

2019-02-12 22:57:18,358 : ***** Transfer task : MPQA *****


2019-02-12 22:57:18,363 : loading BERT mode bert-base-uncased
2019-02-12 22:57:18,363 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:57:18,382 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:57:18,383 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprmn3iq5c
2019-02-12 22:57:20,771 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:57:22,373 : Generating sentence embeddings
2019-02-12 22:57:37,278 : Generated sentence embeddings
2019-02-12 22:57:37,278 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:58:24,673 : Best param found at split 1: l2reg = 0.0001                 with score 87.0
2019-02-12 22:59:10,234 : Best param found at split 2: l2reg = 1e-05                 with score 86.82
2019-02-12 22:59:36,592 : Best param found at split 3: l2reg = 0.0001                 with score 87.34
2019-02-12 22:59:52,646 : Best param found at split 4: l2reg = 0.0001                 with score 86.91
2019-02-12 23:00:09,273 : Best param found at split 5: l2reg = 1e-05                 with score 86.39
2019-02-12 23:00:10,336 : Dev acc : 86.89 Test acc : 85.9

2019-02-12 23:00:10,337 : ***** Transfer task : SUBJ *****


2019-02-12 23:00:10,351 : loading BERT mode bert-base-uncased
2019-02-12 23:00:10,351 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:00:10,372 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:00:10,373 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnict5gdd
2019-02-12 23:00:12,807 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:00:14,214 : Generating sentence embeddings
2019-02-12 23:00:31,700 : Generated sentence embeddings
2019-02-12 23:00:31,700 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:00:59,480 : Best param found at split 1: l2reg = 1e-05                 with score 90.85
2019-02-12 23:01:41,415 : Best param found at split 2: l2reg = 0.001                 with score 90.18
2019-02-12 23:02:24,656 : Best param found at split 3: l2reg = 0.01                 with score 90.55
2019-02-12 23:03:03,819 : Best param found at split 4: l2reg = 0.0001                 with score 90.78
2019-02-12 23:03:31,600 : Best param found at split 5: l2reg = 0.001                 with score 90.62
2019-02-12 23:03:33,034 : Dev acc : 90.6 Test acc : 90.55

2019-02-12 23:03:33,035 : ***** Transfer task : SST Binary classification *****


2019-02-12 23:03:33,160 : loading BERT mode bert-base-uncased
2019-02-12 23:03:33,160 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:03:33,181 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:03:33,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprxk8v2t8
2019-02-12 23:03:35,675 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:03:37,137 : Computing embedding for train
2019-02-12 23:04:45,129 : Computed train embeddings
2019-02-12 23:04:45,129 : Computing embedding for dev
2019-02-12 23:04:46,469 : Computed dev embeddings
2019-02-12 23:04:46,469 : Computing embedding for test
2019-02-12 23:04:49,387 : Computed test embeddings
2019-02-12 23:04:49,387 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:05:42,199 : [('reg:1e-05', 79.7), ('reg:0.0001', 79.59), ('reg:0.001', 79.13), ('reg:0.01', 77.06)]
2019-02-12 23:05:42,199 : Validation : best param found is reg = 1e-05 with score             79.7
2019-02-12 23:05:42,199 : Evaluating...
2019-02-12 23:06:00,587 : 
Dev acc : 79.7 Test acc : 77.38 for             SST Binary classification

2019-02-12 23:06:00,593 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 23:06:00,642 : loading BERT mode bert-base-uncased
2019-02-12 23:06:00,642 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:06:00,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:06:00,663 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaqxpb5rf
2019-02-12 23:06:03,054 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:06:04,619 : Computing embedding for train
2019-02-12 23:06:23,803 : Computed train embeddings
2019-02-12 23:06:23,803 : Computing embedding for dev
2019-02-12 23:06:25,980 : Computed dev embeddings
2019-02-12 23:06:25,980 : Computing embedding for test
2019-02-12 23:06:30,386 : Computed test embeddings
2019-02-12 23:06:30,386 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:06:40,429 : [('reg:1e-05', 31.24), ('reg:0.0001', 32.97), ('reg:0.001', 35.15), ('reg:0.01', 31.34)]
2019-02-12 23:06:40,429 : Validation : best param found is reg = 0.001 with score             35.15
2019-02-12 23:06:40,429 : Evaluating...
2019-02-12 23:06:42,797 : 
Dev acc : 35.15 Test acc : 38.55 for             SST Fine-Grained classification

2019-02-12 23:06:42,797 : ***** Transfer task : TREC *****


2019-02-12 23:06:42,810 : loading BERT mode bert-base-uncased
2019-02-12 23:06:42,810 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:06:42,828 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:06:42,828 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2cpp12ur
2019-02-12 23:06:45,207 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:06:56,448 : Computed train embeddings
2019-02-12 23:06:57,153 : Computed test embeddings
2019-02-12 23:06:57,154 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 23:07:16,764 : [('reg:1e-05', 64.52), ('reg:0.0001', 68.78), ('reg:0.001', 66.82), ('reg:0.01', 63.03)]
2019-02-12 23:07:16,764 : Cross-validation : best param found is reg = 0.0001             with score 68.78
2019-02-12 23:07:16,764 : Evaluating...
2019-02-12 23:07:18,098 : 
Dev acc : 68.78 Test acc : 76.8             for TREC

2019-02-12 23:07:18,098 : ***** Transfer task : MRPC *****


2019-02-12 23:07:18,120 : loading BERT mode bert-base-uncased
2019-02-12 23:07:18,120 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:07:18,140 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:07:18,141 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvzn1o4_5
2019-02-12 23:07:20,555 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:07:22,079 : Computing embedding for train
2019-02-12 23:07:37,090 : Computed train embeddings
2019-02-12 23:07:37,090 : Computing embedding for test
2019-02-12 23:07:43,632 : Computed test embeddings
2019-02-12 23:07:43,649 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 23:07:56,414 : [('reg:1e-05', 71.66), ('reg:0.0001', 71.84), ('reg:0.001', 71.71), ('reg:0.01', 70.88)]
2019-02-12 23:07:56,415 : Cross-validation : best param found is reg = 0.0001             with score 71.84
2019-02-12 23:07:56,415 : Evaluating...
2019-02-12 23:07:56,906 : Dev acc : 71.84 Test acc 72.52; Test F1 81.67 for MRPC.

2019-02-12 23:07:56,906 : ***** Transfer task : SICK-Entailment*****


2019-02-12 23:07:56,969 : loading BERT mode bert-base-uncased
2019-02-12 23:07:56,969 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:07:56,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:07:56,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj6z0slnb
2019-02-12 23:07:59,415 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:08:00,827 : Computing embedding for train
2019-02-12 23:08:08,453 : Computed train embeddings
2019-02-12 23:08:08,453 : Computing embedding for dev
2019-02-12 23:08:09,415 : Computed dev embeddings
2019-02-12 23:08:09,415 : Computing embedding for test
2019-02-12 23:08:17,667 : Computed test embeddings
2019-02-12 23:08:17,695 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:08:19,775 : [('reg:1e-05', 74.6), ('reg:0.0001', 80.4), ('reg:0.001', 76.4), ('reg:0.01', 77.2)]
2019-02-12 23:08:19,775 : Validation : best param found is reg = 0.0001 with score             80.4
2019-02-12 23:08:19,775 : Evaluating...
2019-02-12 23:08:20,349 : 
Dev acc : 80.4 Test acc : 78.16 for                        SICK entailment

2019-02-12 23:08:20,350 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 23:08:20,377 : loading BERT mode bert-base-uncased
2019-02-12 23:08:20,377 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:08:20,432 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:08:20,432 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpabt3kof2
2019-02-12 23:08:22,880 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:08:24,310 : Computing embedding for train
2019-02-12 23:08:31,028 : Computed train embeddings
2019-02-12 23:08:31,028 : Computing embedding for dev
2019-02-12 23:08:32,123 : Computed dev embeddings
2019-02-12 23:08:32,123 : Computing embedding for test
2019-02-12 23:08:41,721 : Computed test embeddings
2019-02-12 23:09:36,101 : Dev : Pearson 0.8044908751785099
2019-02-12 23:09:36,101 : Test : Pearson 0.7908200536926443 Spearman 0.7219444348371298 MSE 0.3820328401020975                        for SICK Relatedness

2019-02-12 23:09:36,102 : 

***** Transfer task : STSBenchmark*****


2019-02-12 23:09:36,142 : loading BERT mode bert-base-uncased
2019-02-12 23:09:36,142 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:09:36,169 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:09:36,169 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphl59m2gw
2019-02-12 23:09:38,552 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:09:40,155 : Computing embedding for train
2019-02-12 23:10:00,749 : Computed train embeddings
2019-02-12 23:10:00,749 : Computing embedding for dev
2019-02-12 23:10:06,746 : Computed dev embeddings
2019-02-12 23:10:06,746 : Computing embedding for test
2019-02-12 23:10:12,159 : Computed test embeddings
2019-02-12 23:11:27,987 : Dev : Pearson 0.6478805515062751
2019-02-12 23:11:27,987 : Test : Pearson 0.6420220664289261 Spearman 0.6402174377560115 MSE 1.5042491503707691                        for SICK Relatedness

2019-02-12 23:11:27,988 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 23:11:28,305 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 23:11:28,315 : loading BERT mode bert-base-uncased
2019-02-12 23:11:28,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:11:28,338 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:11:28,338 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_mo6sy4o
2019-02-12 23:11:30,794 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:11:32,254 : Computing embeddings for train/dev/test
2019-02-12 23:13:54,446 : Computed embeddings
2019-02-12 23:13:54,446 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:15:37,117 : [('reg:1e-05', 66.11), ('reg:0.0001', 65.38), ('reg:0.001', 67.24), ('reg:0.01', 49.99)]
2019-02-12 23:15:37,117 : Validation : best param found is reg = 0.001 with score             67.24
2019-02-12 23:15:37,117 : Evaluating...
2019-02-12 23:15:58,950 : 
Dev acc : 67.2 Test acc : 66.5 for LENGTH classification

2019-02-12 23:15:58,958 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 23:15:59,298 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 23:15:59,343 : loading BERT mode bert-base-uncased
2019-02-12 23:15:59,343 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:15:59,371 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:15:59,371 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3rbknwfa
2019-02-12 23:16:01,800 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:16:03,287 : Computing embeddings for train/dev/test
2019-02-12 23:18:17,809 : Computed embeddings
2019-02-12 23:18:17,809 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:20:23,574 : [('reg:1e-05', 33.8), ('reg:0.0001', 6.66), ('reg:0.001', 0.52), ('reg:0.01', 0.2)]
2019-02-12 23:20:23,574 : Validation : best param found is reg = 1e-05 with score             33.8
2019-02-12 23:20:23,574 : Evaluating...
2019-02-12 23:20:58,702 : 
Dev acc : 33.8 Test acc : 34.3 for WORDCONTENT classification

2019-02-12 23:20:58,711 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 23:20:59,064 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 23:20:59,128 : loading BERT mode bert-base-uncased
2019-02-12 23:20:59,128 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:20:59,154 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:20:59,154 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjlzhdw0x
2019-02-12 23:21:01,593 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:21:03,049 : Computing embeddings for train/dev/test
2019-02-12 23:23:23,856 : Computed embeddings
2019-02-12 23:23:23,857 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:24:41,458 : [('reg:1e-05', 28.95), ('reg:0.0001', 27.8), ('reg:0.001', 30.22), ('reg:0.01', 25.79)]
2019-02-12 23:24:41,458 : Validation : best param found is reg = 0.001 with score             30.22
2019-02-12 23:24:41,458 : Evaluating...
2019-02-12 23:24:50,893 : 
Dev acc : 30.2 Test acc : 30.0 for DEPTH classification

2019-02-12 23:24:50,900 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 23:24:51,279 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 23:24:51,341 : loading BERT mode bert-base-uncased
2019-02-12 23:24:51,341 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:24:51,449 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:24:51,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwciep_jc
2019-02-12 23:24:53,874 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:24:55,282 : Computing embeddings for train/dev/test
2019-02-12 23:27:02,922 : Computed embeddings
2019-02-12 23:27:02,922 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:28:12,794 : [('reg:1e-05', 57.94), ('reg:0.0001', 55.66), ('reg:0.001', 43.62), ('reg:0.01', 36.78)]
2019-02-12 23:28:12,794 : Validation : best param found is reg = 1e-05 with score             57.94
2019-02-12 23:28:12,794 : Evaluating...
2019-02-12 23:28:27,708 : 
Dev acc : 57.9 Test acc : 57.9 for TOPCONSTITUENTS classification

2019-02-12 23:28:27,715 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 23:28:28,060 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 23:28:28,127 : loading BERT mode bert-base-uncased
2019-02-12 23:28:28,127 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:28:28,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:28:28,251 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3auw8r1a
2019-02-12 23:28:30,698 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:28:32,101 : Computing embeddings for train/dev/test
2019-02-12 23:30:52,887 : Computed embeddings
2019-02-12 23:30:52,887 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:32:17,789 : [('reg:1e-05', 82.94), ('reg:0.0001', 82.9), ('reg:0.001', 83.25), ('reg:0.01', 80.8)]
2019-02-12 23:32:17,789 : Validation : best param found is reg = 0.001 with score             83.25
2019-02-12 23:32:17,789 : Evaluating...
2019-02-12 23:32:32,230 : 
Dev acc : 83.2 Test acc : 82.4 for BIGRAMSHIFT classification

2019-02-12 23:32:32,240 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 23:32:32,810 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 23:32:32,876 : loading BERT mode bert-base-uncased
2019-02-12 23:32:32,876 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:32:32,907 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:32:32,907 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz78jt18x
2019-02-12 23:32:35,340 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:32:36,811 : Computing embeddings for train/dev/test
2019-02-12 23:35:17,452 : Computed embeddings
2019-02-12 23:35:17,452 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:36:43,983 : [('reg:1e-05', 87.43), ('reg:0.0001', 87.37), ('reg:0.001', 87.41), ('reg:0.01', 87.37)]
2019-02-12 23:36:43,984 : Validation : best param found is reg = 1e-05 with score             87.43
2019-02-12 23:36:43,984 : Evaluating...
2019-02-12 23:36:54,135 : 
Dev acc : 87.4 Test acc : 86.2 for TENSE classification

2019-02-12 23:36:54,145 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 23:36:54,722 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 23:36:54,787 : loading BERT mode bert-base-uncased
2019-02-12 23:36:54,787 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:36:54,816 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:36:54,817 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6ptycd8c
2019-02-12 23:36:57,318 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:36:58,821 : Computing embeddings for train/dev/test
2019-02-12 23:39:30,389 : Computed embeddings
2019-02-12 23:39:30,389 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:40:24,393 : [('reg:1e-05', 79.97), ('reg:0.0001', 79.98), ('reg:0.001', 80.02), ('reg:0.01', 79.27)]
2019-02-12 23:40:24,393 : Validation : best param found is reg = 0.001 with score             80.02
2019-02-12 23:40:24,393 : Evaluating...
2019-02-12 23:40:42,946 : 
Dev acc : 80.0 Test acc : 79.4 for SUBJNUMBER classification

2019-02-12 23:40:42,954 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 23:40:43,390 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 23:40:43,457 : loading BERT mode bert-base-uncased
2019-02-12 23:40:43,457 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:40:43,485 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:40:43,485 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuyzckp00
2019-02-12 23:40:45,943 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:40:47,417 : Computing embeddings for train/dev/test
2019-02-12 23:43:28,093 : Computed embeddings
2019-02-12 23:43:28,093 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:44:18,566 : [('reg:1e-05', 76.58), ('reg:0.0001', 76.56), ('reg:0.001', 76.36), ('reg:0.01', 74.99)]
2019-02-12 23:44:18,567 : Validation : best param found is reg = 1e-05 with score             76.58
2019-02-12 23:44:18,567 : Evaluating...
2019-02-12 23:44:35,929 : 
Dev acc : 76.6 Test acc : 77.3 for OBJNUMBER classification

2019-02-12 23:44:35,937 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 23:44:36,333 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 23:44:36,403 : loading BERT mode bert-base-uncased
2019-02-12 23:44:36,403 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:44:36,527 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:44:36,527 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwwts28gr
2019-02-12 23:44:38,960 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:44:40,440 : Computing embeddings for train/dev/test
2019-02-12 23:47:39,385 : Computed embeddings
2019-02-12 23:47:39,386 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:48:38,234 : [('reg:1e-05', 56.47), ('reg:0.0001', 56.48), ('reg:0.001', 56.37), ('reg:0.01', 56.44)]
2019-02-12 23:48:38,234 : Validation : best param found is reg = 0.0001 with score             56.48
2019-02-12 23:48:38,234 : Evaluating...
2019-02-12 23:49:01,946 : 
Dev acc : 56.5 Test acc : 56.7 for ODDMANOUT classification

2019-02-12 23:49:01,954 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 23:49:02,568 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 23:49:02,644 : loading BERT mode bert-base-uncased
2019-02-12 23:49:02,645 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:49:02,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:49:02,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3hq91v8f
2019-02-12 23:49:05,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:49:06,643 : Computing embeddings for train/dev/test
2019-02-12 23:52:34,169 : Computed embeddings
2019-02-12 23:52:34,169 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:54:42,787 : [('reg:1e-05', 58.4), ('reg:0.0001', 58.29), ('reg:0.001', 57.41), ('reg:0.01', 55.08)]
2019-02-12 23:54:42,787 : Validation : best param found is reg = 1e-05 with score             58.4
2019-02-12 23:54:42,787 : Evaluating...
2019-02-12 23:55:03,732 : 
Dev acc : 58.4 Test acc : 57.4 for COORDINATIONINVERSION classification

2019-02-12 23:55:03,740 : {'STS12': {'MSRpar': {'pearson': (0.31517962043380005, 9.274245985630103e-19), 'spearman': SpearmanrResult(correlation=0.35335482006504165, pvalue=1.7771813092395498e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.666022682741378, 2.6268758548865865e-97), 'spearman': SpearmanrResult(correlation=0.675760943288564, pvalue=3.5752723552564956e-101), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48905904327298066, 5.655951867635274e-29), 'spearman': SpearmanrResult(correlation=0.596543180327218, pvalue=1.398999641079036e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5581871937714659, 1.190310186904133e-62), 'spearman': SpearmanrResult(correlation=0.58460323320039, pvalue=5.920516996823856e-70), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5278132445700195, 5.327884022339016e-30), 'spearman': SpearmanrResult(correlation=0.4567146830166179, pvalue=5.908528657942639e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5112523569579288, 'wmean': 0.5114600090591116}, 'spearman': {'mean': 0.5333953719795663, 'wmean': 0.5361427688897427}}}, 'STS13': {'FNWN': {'pearson': (0.17628146696661798, 0.015248407716798664), 'spearman': SpearmanrResult(correlation=0.18385473165859867, pvalue=0.011327110494333856), 'nsamples': 189}, 'headlines': {'pearson': (0.6502119846052229, 2.4768845706869196e-91), 'spearman': SpearmanrResult(correlation=0.6435148626523652, pvalue=6.581182261830661e-89), 'nsamples': 750}, 'OnWN': {'pearson': (0.5236854073124746, 7.898132167816414e-41), 'spearman': SpearmanrResult(correlation=0.5225876340722075, pvalue=1.2313329200352398e-40), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4500596196281051, 'wmean': 0.5431757994752708}, 'spearman': {'mean': 0.44998574279439046, 'wmean': 0.5403709026581717}}}, 'STS14': {'deft-forum': {'pearson': (0.39049257781677976, 7.661544465881755e-18), 'spearman': SpearmanrResult(correlation=0.4032201042040241, pvalue=5.0609955789961e-19), 'nsamples': 450}, 'deft-news': {'pearson': (0.718190490714152, 7.514422746929178e-49), 'spearman': SpearmanrResult(correlation=0.6926240212078038, pvalue=3.4733388372130707e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.5974912505536703, 9.108397311231084e-74), 'spearman': SpearmanrResult(correlation=0.5697585278060356, pvalue=9.074702675085512e-66), 'nsamples': 750}, 'images': {'pearson': (0.594532470989574, 7.080587539077131e-73), 'spearman': SpearmanrResult(correlation=0.5881876545655778, pvalue=5.3589630419650775e-71), 'nsamples': 750}, 'OnWN': {'pearson': (0.627995217099557, 1.619643800656171e-83), 'spearman': SpearmanrResult(correlation=0.6582527861811847, pvalue=2.5161465966249743e-94), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5752404490761874, 2.7358645574348726e-67), 'spearman': SpearmanrResult(correlation=0.5471522079198559, pvalue=8.647526391818863e-60), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5839904093749867, 'wmean': 0.5833662261389435}, 'spearman': {'mean': 0.5765325503140803, 'wmean': 0.576466569495638}}}, 'STS15': {'answers-forums': {'pearson': (0.5000142411763683, 4.0812405923192728e-25), 'spearman': SpearmanrResult(correlation=0.46925023893679624, pvalue=6.24727918348043e-22), 'nsamples': 375}, 'answers-students': {'pearson': (0.671125651924859, 2.5820351951778227e-99), 'spearman': SpearmanrResult(correlation=0.6775773055627093, pvalue=6.541119019823477e-102), 'nsamples': 750}, 'belief': {'pearson': (0.5407949207950868, 7.222081814308047e-30), 'spearman': SpearmanrResult(correlation=0.5791611987437087, pvalue=5.686663981276436e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.6492550986991714, 5.547013176252168e-91), 'spearman': SpearmanrResult(correlation=0.6551934739891613, pvalue=3.551196322065464e-93), 'nsamples': 750}, 'images': {'pearson': (0.7322802894807683, 6.241807260873921e-127), 'spearman': SpearmanrResult(correlation=0.7411027875984493, pvalue=1.4875081756685134e-131), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6186940404152509, 'wmean': 0.6432664052726316}, 'spearman': {'mean': 0.624457000966165, 'wmean': 0.6495198214976431}}}, 'STS16': {'answer-answer': {'pearson': (0.4628567438561703, 6.856204659572955e-15), 'spearman': SpearmanrResult(correlation=0.4826169844622393, pvalue=3.174893646993659e-16), 'nsamples': 254}, 'headlines': {'pearson': (0.6366892607275633, 1.0420635168267824e-29), 'spearman': SpearmanrResult(correlation=0.6429002045507601, pvalue=1.9591321244232668e-30), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6862352028140658, 2.3199413209641038e-33), 'spearman': SpearmanrResult(correlation=0.6994336381984178, pvalue=4.127458448838043e-35), 'nsamples': 230}, 'postediting': {'pearson': (0.7654234813380592, 3.1446940402690925e-48), 'spearman': SpearmanrResult(correlation=0.8168159608142607, pvalue=9.607331237620113e-60), 'nsamples': 244}, 'question-question': {'pearson': (0.33982046087234086, 4.806703303128953e-07), 'spearman': SpearmanrResult(correlation=0.3648131502171608, pvalue=5.6120053696216537e-08), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5782050299216399, 'wmean': 0.5832387363209707}, 'spearman': {'mean': 0.6013159876485676, 'wmean': 0.6063116733610879}}}, 'MR': {'devacc': 61.39, 'acc': 65.13, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 71.0, 'acc': 66.68, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.89, 'acc': 85.9, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.6, 'acc': 90.55, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.7, 'acc': 77.38, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.15, 'acc': 38.55, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 68.78, 'acc': 76.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.84, 'acc': 72.52, 'f1': 81.67, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.4, 'acc': 78.16, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8044908751785099, 'pearson': 0.7908200536926443, 'spearman': 0.7219444348371298, 'mse': 0.3820328401020975, 'yhat': array([2.55963588, 4.11010255, 1.26987492, ..., 3.32369615, 4.11696814,
       4.02663421]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6478805515062751, 'pearson': 0.6420220664289261, 'spearman': 0.6402174377560115, 'mse': 1.5042491503707691, 'yhat': array([1.44988881, 1.5051401 , 2.25148232, ..., 3.980443  , 4.38074323,
       3.91794949]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 67.24, 'acc': 66.52, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 33.8, 'acc': 34.31, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.22, 'acc': 29.96, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 57.94, 'acc': 57.93, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 83.25, 'acc': 82.38, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.43, 'acc': 86.21, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.02, 'acc': 79.4, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.58, 'acc': 77.34, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 56.48, 'acc': 56.69, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.4, 'acc': 57.44, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 23:55:03,741 : ********************************************************************************
2019-02-12 23:55:03,741 : ********************************************************************************
2019-02-12 23:55:03,741 : ********************************************************************************
2019-02-12 23:55:03,741 : layer 6
2019-02-12 23:55:03,741 : ********************************************************************************
2019-02-12 23:55:03,741 : ********************************************************************************
2019-02-12 23:55:03,741 : ********************************************************************************
2019-02-12 23:55:03,828 : ***** Transfer task : STS12 *****


2019-02-12 23:55:03,840 : loading BERT mode bert-base-uncased
2019-02-12 23:55:03,841 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:55:03,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:55:03,858 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx5r4l2lz
2019-02-12 23:55:06,301 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:55:10,724 : MSRpar : pearson = 0.2980, spearman = 0.3358
2019-02-12 23:55:12,942 : MSRvid : pearson = 0.6319, spearman = 0.6432
2019-02-12 23:55:14,218 : SMTeuroparl : pearson = 0.4782, spearman = 0.5977
2019-02-12 23:55:16,480 : surprise.OnWN : pearson = 0.5490, spearman = 0.5763
2019-02-12 23:55:17,844 : surprise.SMTnews : pearson = 0.5509, spearman = 0.4631
2019-02-12 23:55:17,844 : ALL (weighted average) : Pearson = 0.4982,             Spearman = 0.5230
2019-02-12 23:55:17,844 : ALL (average) : Pearson = 0.5016,             Spearman = 0.5232

2019-02-12 23:55:17,844 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 23:55:17,852 : loading BERT mode bert-base-uncased
2019-02-12 23:55:17,852 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:55:17,870 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:55:17,870 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxpmst9mp
2019-02-12 23:55:20,301 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:55:22,823 : FNWN : pearson = 0.1588, spearman = 0.1697
2019-02-12 23:55:25,138 : headlines : pearson = 0.6383, spearman = 0.6341
2019-02-12 23:55:26,783 : OnWN : pearson = 0.4911, spearman = 0.4920
2019-02-12 23:55:26,783 : ALL (weighted average) : Pearson = 0.5228,             Spearman = 0.5224
2019-02-12 23:55:26,783 : ALL (average) : Pearson = 0.4294,             Spearman = 0.4319

2019-02-12 23:55:26,783 : ***** Transfer task : STS14 *****


2019-02-12 23:55:26,804 : loading BERT mode bert-base-uncased
2019-02-12 23:55:26,804 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:55:26,851 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:55:26,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphdmugo3b
2019-02-12 23:55:29,298 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:55:31,710 : deft-forum : pearson = 0.3768, spearman = 0.3839
2019-02-12 23:55:32,676 : deft-news : pearson = 0.7247, spearman = 0.7009
2019-02-12 23:55:34,257 : headlines : pearson = 0.5853, spearman = 0.5592
2019-02-12 23:55:35,793 : images : pearson = 0.5667, spearman = 0.5654
2019-02-12 23:55:37,345 : OnWN : pearson = 0.6138, spearman = 0.6459
2019-02-12 23:55:39,216 : tweet-news : pearson = 0.5713, spearman = 0.5449
2019-02-12 23:55:39,216 : ALL (weighted average) : Pearson = 0.5706,             Spearman = 0.5652
2019-02-12 23:55:39,216 : ALL (average) : Pearson = 0.5731,             Spearman = 0.5667

2019-02-12 23:55:39,216 : ***** Transfer task : STS15 *****


2019-02-12 23:55:39,247 : loading BERT mode bert-base-uncased
2019-02-12 23:55:39,247 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:55:39,265 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:55:39,265 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpynfkssp0
2019-02-12 23:55:41,706 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:55:44,319 : answers-forums : pearson = 0.4950, spearman = 0.4625
2019-02-12 23:55:45,864 : answers-students : pearson = 0.6638, spearman = 0.6721
2019-02-12 23:55:46,926 : belief : pearson = 0.5286, spearman = 0.5663
2019-02-12 23:55:48,312 : headlines : pearson = 0.6309, spearman = 0.6355
2019-02-12 23:55:49,647 : images : pearson = 0.7009, spearman = 0.7111
2019-02-12 23:55:49,647 : ALL (weighted average) : Pearson = 0.6268,             Spearman = 0.6333
2019-02-12 23:55:49,647 : ALL (average) : Pearson = 0.6038,             Spearman = 0.6095

2019-02-12 23:55:49,647 : ***** Transfer task : STS16 *****


2019-02-12 23:55:49,719 : loading BERT mode bert-base-uncased
2019-02-12 23:55:49,719 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:55:49,738 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:55:49,738 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqww1t4sd
2019-02-12 23:55:52,166 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:55:54,294 : answer-answer : pearson = 0.4941, spearman = 0.5171
2019-02-12 23:55:54,865 : headlines : pearson = 0.6267, spearman = 0.6334
2019-02-12 23:55:55,494 : plagiarism : pearson = 0.6797, spearman = 0.6808
2019-02-12 23:55:56,435 : postediting : pearson = 0.7637, spearman = 0.8170
2019-02-12 23:55:56,957 : question-question : pearson = 0.2585, spearman = 0.2875
2019-02-12 23:55:56,958 : ALL (weighted average) : Pearson = 0.5719,             Spearman = 0.5945
2019-02-12 23:55:56,958 : ALL (average) : Pearson = 0.5645,             Spearman = 0.5872

2019-02-12 23:55:56,958 : ***** Transfer task : MR *****


2019-02-12 23:55:56,975 : loading BERT mode bert-base-uncased
2019-02-12 23:55:56,975 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:55:56,995 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:55:56,995 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj4h8kfqz
2019-02-12 23:55:59,445 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:56:00,939 : Generating sentence embeddings
2019-02-12 23:56:22,104 : Generated sentence embeddings
2019-02-12 23:56:22,104 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:57:00,393 : Best param found at split 1: l2reg = 0.0001                 with score 66.87
2019-02-12 23:57:51,097 : Best param found at split 2: l2reg = 1e-05                 with score 62.81
2019-02-12 23:58:40,162 : Best param found at split 3: l2reg = 0.0001                 with score 62.17
2019-02-12 23:59:05,447 : Best param found at split 4: l2reg = 1e-05                 with score 60.09
2019-02-12 23:59:27,659 : Best param found at split 5: l2reg = 0.0001                 with score 61.78
2019-02-12 23:59:28,554 : Dev acc : 62.74 Test acc : 65.62

2019-02-12 23:59:28,555 : ***** Transfer task : CR *****


2019-02-12 23:59:28,563 : loading BERT mode bert-base-uncased
2019-02-12 23:59:28,563 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:59:28,583 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:59:28,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk3gvtl7h
2019-02-12 23:59:31,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:59:32,427 : Generating sentence embeddings
2019-02-12 23:59:37,249 : Generated sentence embeddings
2019-02-12 23:59:37,249 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:59:40,583 : Best param found at split 1: l2reg = 0.0001                 with score 70.82
2019-02-12 23:59:44,651 : Best param found at split 2: l2reg = 0.0001                 with score 72.24
2019-02-12 23:59:49,397 : Best param found at split 3: l2reg = 0.0001                 with score 68.11
2019-02-12 23:59:54,133 : Best param found at split 4: l2reg = 0.01                 with score 70.74
2019-02-13 00:00:01,838 : Best param found at split 5: l2reg = 0.001                 with score 72.72
2019-02-13 00:00:02,254 : Dev acc : 70.93 Test acc : 68.0

2019-02-13 00:00:02,255 : ***** Transfer task : MPQA *****


2019-02-13 00:00:02,260 : loading BERT mode bert-base-uncased
2019-02-13 00:00:02,260 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:00:02,280 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:00:02,280 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4xc_koeg
2019-02-13 00:00:04,754 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:00:06,235 : Generating sentence embeddings
2019-02-13 00:00:16,117 : Generated sentence embeddings
2019-02-13 00:00:16,118 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:00:47,202 : Best param found at split 1: l2reg = 0.001                 with score 87.8
2019-02-13 00:01:32,389 : Best param found at split 2: l2reg = 0.001                 with score 87.33
2019-02-13 00:02:08,682 : Best param found at split 3: l2reg = 0.001                 with score 87.58
2019-02-13 00:02:40,067 : Best param found at split 4: l2reg = 1e-05                 with score 87.11
2019-02-13 00:03:11,913 : Best param found at split 5: l2reg = 0.0001                 with score 86.94
2019-02-13 00:03:13,291 : Dev acc : 87.35 Test acc : 87.33

2019-02-13 00:03:13,292 : ***** Transfer task : SUBJ *****


2019-02-13 00:03:13,307 : loading BERT mode bert-base-uncased
2019-02-13 00:03:13,307 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:03:13,328 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:03:13,329 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3xqj17bp
2019-02-13 00:03:15,756 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:03:17,179 : Generating sentence embeddings
2019-02-13 00:03:34,594 : Generated sentence embeddings
2019-02-13 00:03:34,594 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:04:00,940 : Best param found at split 1: l2reg = 1e-05                 with score 91.4
2019-02-13 00:04:30,783 : Best param found at split 2: l2reg = 0.01                 with score 91.35
2019-02-13 00:05:02,814 : Best param found at split 3: l2reg = 0.01                 with score 91.2
2019-02-13 00:05:39,778 : Best param found at split 4: l2reg = 0.01                 with score 91.4
2019-02-13 00:06:11,662 : Best param found at split 5: l2reg = 0.0001                 with score 91.59
2019-02-13 00:06:13,499 : Dev acc : 91.39 Test acc : 90.06

2019-02-13 00:06:13,500 : ***** Transfer task : SST Binary classification *****


2019-02-13 00:06:13,626 : loading BERT mode bert-base-uncased
2019-02-13 00:06:13,627 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:06:13,651 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:06:13,651 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphrk3bkgw
2019-02-13 00:06:16,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:06:17,569 : Computing embedding for train
2019-02-13 00:07:51,928 : Computed train embeddings
2019-02-13 00:07:51,928 : Computing embedding for dev
2019-02-13 00:07:53,279 : Computed dev embeddings
2019-02-13 00:07:53,279 : Computing embedding for test
2019-02-13 00:07:56,254 : Computed test embeddings
2019-02-13 00:07:56,254 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:08:30,928 : [('reg:1e-05', 80.05), ('reg:0.0001', 80.16), ('reg:0.001', 79.47), ('reg:0.01', 79.59)]
2019-02-13 00:08:30,928 : Validation : best param found is reg = 0.0001 with score             80.16
2019-02-13 00:08:30,928 : Evaluating...
2019-02-13 00:08:39,872 : 
Dev acc : 80.16 Test acc : 77.81 for             SST Binary classification

2019-02-13 00:08:39,877 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 00:08:39,930 : loading BERT mode bert-base-uncased
2019-02-13 00:08:39,930 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:08:39,950 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:08:39,951 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkudq3p6l
2019-02-13 00:08:42,404 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:08:43,874 : Computing embedding for train
2019-02-13 00:08:59,828 : Computed train embeddings
2019-02-13 00:08:59,828 : Computing embedding for dev
2019-02-13 00:09:02,006 : Computed dev embeddings
2019-02-13 00:09:02,006 : Computing embedding for test
2019-02-13 00:09:06,366 : Computed test embeddings
2019-02-13 00:09:06,366 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:09:12,782 : [('reg:1e-05', 39.33), ('reg:0.0001', 29.79), ('reg:0.001', 35.51), ('reg:0.01', 25.34)]
2019-02-13 00:09:12,782 : Validation : best param found is reg = 1e-05 with score             39.33
2019-02-13 00:09:12,782 : Evaluating...
2019-02-13 00:09:14,716 : 
Dev acc : 39.33 Test acc : 36.7 for             SST Fine-Grained classification

2019-02-13 00:09:14,716 : ***** Transfer task : TREC *****


2019-02-13 00:09:14,730 : loading BERT mode bert-base-uncased
2019-02-13 00:09:14,730 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:09:14,748 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:09:14,749 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn1i03zma
2019-02-13 00:09:17,186 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:09:27,311 : Computed train embeddings
2019-02-13 00:09:27,919 : Computed test embeddings
2019-02-13 00:09:27,920 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 00:09:53,020 : [('reg:1e-05', 62.73), ('reg:0.0001', 69.43), ('reg:0.001', 70.16), ('reg:0.01', 63.57)]
2019-02-13 00:09:53,020 : Cross-validation : best param found is reg = 0.001             with score 70.16
2019-02-13 00:09:53,020 : Evaluating...
2019-02-13 00:09:53,935 : 
Dev acc : 70.16 Test acc : 81.0             for TREC

2019-02-13 00:09:53,936 : ***** Transfer task : MRPC *****


2019-02-13 00:09:53,958 : loading BERT mode bert-base-uncased
2019-02-13 00:09:53,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:09:53,978 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:09:53,978 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8gqudqql
2019-02-13 00:09:56,410 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:09:57,940 : Computing embedding for train
2019-02-13 00:10:11,326 : Computed train embeddings
2019-02-13 00:10:11,326 : Computing embedding for test
2019-02-13 00:10:17,304 : Computed test embeddings
2019-02-13 00:10:17,320 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 00:10:30,388 : [('reg:1e-05', 70.49), ('reg:0.0001', 72.65), ('reg:0.001', 71.47), ('reg:0.01', 70.85)]
2019-02-13 00:10:30,388 : Cross-validation : best param found is reg = 0.0001             with score 72.65
2019-02-13 00:10:30,388 : Evaluating...
2019-02-13 00:10:31,404 : Dev acc : 72.65 Test acc 71.48; Test F1 81.78 for MRPC.

2019-02-13 00:10:31,405 : ***** Transfer task : SICK-Entailment*****


2019-02-13 00:10:31,469 : loading BERT mode bert-base-uncased
2019-02-13 00:10:31,469 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:10:31,488 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:10:31,488 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp354m9ibk
2019-02-13 00:10:33,916 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:10:35,373 : Computing embedding for train
2019-02-13 00:10:47,017 : Computed train embeddings
2019-02-13 00:10:47,017 : Computing embedding for dev
2019-02-13 00:10:48,486 : Computed dev embeddings
2019-02-13 00:10:48,486 : Computing embedding for test
2019-02-13 00:11:02,048 : Computed test embeddings
2019-02-13 00:11:02,076 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:11:05,159 : [('reg:1e-05', 75.4), ('reg:0.0001', 78.2), ('reg:0.001', 74.8), ('reg:0.01', 76.8)]
2019-02-13 00:11:05,159 : Validation : best param found is reg = 0.0001 with score             78.2
2019-02-13 00:11:05,159 : Evaluating...
2019-02-13 00:11:06,096 : 
Dev acc : 78.2 Test acc : 74.91 for                        SICK entailment

2019-02-13 00:11:06,097 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 00:11:06,124 : loading BERT mode bert-base-uncased
2019-02-13 00:11:06,124 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:11:06,144 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:11:06,144 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvfe5bz34
2019-02-13 00:11:08,577 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:11:10,131 : Computing embedding for train
2019-02-13 00:11:22,451 : Computed train embeddings
2019-02-13 00:11:22,451 : Computing embedding for dev
2019-02-13 00:11:23,711 : Computed dev embeddings
2019-02-13 00:11:23,711 : Computing embedding for test
2019-02-13 00:11:35,690 : Computed test embeddings
2019-02-13 00:12:12,306 : Dev : Pearson 0.8016525987092108
2019-02-13 00:12:12,306 : Test : Pearson 0.776170577689326 Spearman 0.7123677296437668 MSE 0.405752075275095                        for SICK Relatedness

2019-02-13 00:12:12,307 : 

***** Transfer task : STSBenchmark*****


2019-02-13 00:12:12,346 : loading BERT mode bert-base-uncased
2019-02-13 00:12:12,346 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:12:12,375 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:12:12,375 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph6sarqvt
2019-02-13 00:12:14,825 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:12:16,311 : Computing embedding for train
2019-02-13 00:12:32,053 : Computed train embeddings
2019-02-13 00:12:32,054 : Computing embedding for dev
2019-02-13 00:12:36,613 : Computed dev embeddings
2019-02-13 00:12:36,613 : Computing embedding for test
2019-02-13 00:12:40,566 : Computed test embeddings
2019-02-13 00:14:04,497 : Dev : Pearson 0.6419749514192992
2019-02-13 00:14:04,497 : Test : Pearson 0.6270375233457395 Spearman 0.6247146932789917 MSE 1.4881659884311775                        for SICK Relatedness

2019-02-13 00:14:04,497 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 00:14:04,819 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 00:14:04,829 : loading BERT mode bert-base-uncased
2019-02-13 00:14:04,829 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:14:04,855 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:14:04,855 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9zxqpbe3
2019-02-13 00:14:07,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:14:08,855 : Computing embeddings for train/dev/test
2019-02-13 00:17:04,807 : Computed embeddings
2019-02-13 00:17:04,807 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:18:29,918 : [('reg:1e-05', 67.08), ('reg:0.0001', 69.98), ('reg:0.001', 70.11), ('reg:0.01', 51.64)]
2019-02-13 00:18:29,918 : Validation : best param found is reg = 0.001 with score             70.11
2019-02-13 00:18:29,918 : Evaluating...
2019-02-13 00:18:52,247 : 
Dev acc : 70.1 Test acc : 70.8 for LENGTH classification

2019-02-13 00:18:52,254 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 00:18:52,602 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 00:18:52,647 : loading BERT mode bert-base-uncased
2019-02-13 00:18:52,647 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:18:52,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:18:52,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqqsm8d5d
2019-02-13 00:18:55,101 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:18:56,651 : Computing embeddings for train/dev/test
2019-02-13 00:21:38,363 : Computed embeddings
2019-02-13 00:21:38,363 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:22:47,956 : [('reg:1e-05', 17.58), ('reg:0.0001', 6.67), ('reg:0.001', 0.49), ('reg:0.01', 0.13)]
2019-02-13 00:22:47,956 : Validation : best param found is reg = 1e-05 with score             17.58
2019-02-13 00:22:47,956 : Evaluating...
2019-02-13 00:23:18,359 : 
Dev acc : 17.6 Test acc : 17.7 for WORDCONTENT classification

2019-02-13 00:23:18,366 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 00:23:18,718 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 00:23:18,782 : loading BERT mode bert-base-uncased
2019-02-13 00:23:18,782 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:23:18,879 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:23:18,879 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_4shzqjo
2019-02-13 00:23:21,314 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:23:22,821 : Computing embeddings for train/dev/test
2019-02-13 00:25:50,070 : Computed embeddings
2019-02-13 00:25:50,071 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:26:44,874 : [('reg:1e-05', 27.57), ('reg:0.0001', 26.61), ('reg:0.001', 29.3), ('reg:0.01', 25.41)]
2019-02-13 00:26:44,874 : Validation : best param found is reg = 0.001 with score             29.3
2019-02-13 00:26:44,874 : Evaluating...
2019-02-13 00:27:03,094 : 
Dev acc : 29.3 Test acc : 29.8 for DEPTH classification

2019-02-13 00:27:03,101 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 00:27:03,476 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 00:27:03,538 : loading BERT mode bert-base-uncased
2019-02-13 00:27:03,539 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:27:03,649 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:27:03,649 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpua0bx883
2019-02-13 00:27:06,080 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:27:07,557 : Computing embeddings for train/dev/test
2019-02-13 00:29:32,013 : Computed embeddings
2019-02-13 00:29:32,013 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:30:22,620 : [('reg:1e-05', 59.11), ('reg:0.0001', 50.53), ('reg:0.001', 46.03), ('reg:0.01', 39.62)]
2019-02-13 00:30:22,620 : Validation : best param found is reg = 1e-05 with score             59.11
2019-02-13 00:30:22,620 : Evaluating...
2019-02-13 00:30:44,703 : 
Dev acc : 59.1 Test acc : 59.1 for TOPCONSTITUENTS classification

2019-02-13 00:30:44,711 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 00:30:45,236 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 00:30:45,302 : loading BERT mode bert-base-uncased
2019-02-13 00:30:45,302 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:30:45,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:30:45,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp04urhd6d
2019-02-13 00:30:47,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:30:49,232 : Computing embeddings for train/dev/test
2019-02-13 00:33:46,520 : Computed embeddings
2019-02-13 00:33:46,520 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:35:31,204 : [('reg:1e-05', 84.31), ('reg:0.0001', 84.3), ('reg:0.001', 83.88), ('reg:0.01', 75.1)]
2019-02-13 00:35:31,204 : Validation : best param found is reg = 1e-05 with score             84.31
2019-02-13 00:35:31,204 : Evaluating...
2019-02-13 00:36:05,073 : 
Dev acc : 84.3 Test acc : 83.6 for BIGRAMSHIFT classification

2019-02-13 00:36:05,080 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 00:36:05,480 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 00:36:05,545 : loading BERT mode bert-base-uncased
2019-02-13 00:36:05,545 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:36:05,575 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:36:05,575 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmneag_wx
2019-02-13 00:36:08,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:36:09,565 : Computing embeddings for train/dev/test
2019-02-13 00:38:24,812 : Computed embeddings
2019-02-13 00:38:24,812 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:39:08,207 : [('reg:1e-05', 87.33), ('reg:0.0001', 87.3), ('reg:0.001', 87.43), ('reg:0.01', 87.08)]
2019-02-13 00:39:08,208 : Validation : best param found is reg = 0.001 with score             87.43
2019-02-13 00:39:08,208 : Evaluating...
2019-02-13 00:39:31,548 : 
Dev acc : 87.4 Test acc : 86.0 for TENSE classification

2019-02-13 00:39:31,555 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 00:39:31,979 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 00:39:32,042 : loading BERT mode bert-base-uncased
2019-02-13 00:39:32,042 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:39:32,070 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:39:32,070 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsic6_pix
2019-02-13 00:39:34,606 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:39:36,108 : Computing embeddings for train/dev/test
2019-02-13 00:42:24,563 : Computed embeddings
2019-02-13 00:42:24,564 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:43:07,747 : [('reg:1e-05', 78.75), ('reg:0.0001', 78.76), ('reg:0.001', 78.73), ('reg:0.01', 80.03)]
2019-02-13 00:43:07,747 : Validation : best param found is reg = 0.01 with score             80.03
2019-02-13 00:43:07,747 : Evaluating...
2019-02-13 00:43:26,105 : 
Dev acc : 80.0 Test acc : 79.4 for SUBJNUMBER classification

2019-02-13 00:43:26,113 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 00:43:26,530 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 00:43:26,597 : loading BERT mode bert-base-uncased
2019-02-13 00:43:26,597 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:43:26,715 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:43:26,715 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsotnq160
2019-02-13 00:43:29,144 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:43:30,641 : Computing embeddings for train/dev/test
2019-02-13 00:46:14,356 : Computed embeddings
2019-02-13 00:46:14,356 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:47:31,777 : [('reg:1e-05', 78.98), ('reg:0.0001', 78.99), ('reg:0.001', 79.01), ('reg:0.01', 76.19)]
2019-02-13 00:47:31,777 : Validation : best param found is reg = 0.001 with score             79.01
2019-02-13 00:47:31,777 : Evaluating...
2019-02-13 00:47:52,420 : 
Dev acc : 79.0 Test acc : 79.8 for OBJNUMBER classification

2019-02-13 00:47:52,427 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 00:47:52,812 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 00:47:52,881 : loading BERT mode bert-base-uncased
2019-02-13 00:47:52,881 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:47:53,004 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:47:53,005 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg2tb9kgp
2019-02-13 00:47:55,440 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:47:56,923 : Computing embeddings for train/dev/test
2019-02-13 00:51:00,789 : Computed embeddings
2019-02-13 00:51:00,789 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:52:11,370 : [('reg:1e-05', 57.49), ('reg:0.0001', 57.5), ('reg:0.001', 57.42), ('reg:0.01', 57.94)]
2019-02-13 00:52:11,370 : Validation : best param found is reg = 0.01 with score             57.94
2019-02-13 00:52:11,370 : Evaluating...
2019-02-13 00:52:31,521 : 
Dev acc : 57.9 Test acc : 57.9 for ODDMANOUT classification

2019-02-13 00:52:31,528 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 00:52:32,126 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 00:52:32,203 : loading BERT mode bert-base-uncased
2019-02-13 00:52:32,203 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:52:32,234 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:52:32,235 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpybv61ag0
2019-02-13 00:52:34,699 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:52:36,284 : Computing embeddings for train/dev/test
2019-02-13 00:55:15,839 : Computed embeddings
2019-02-13 00:55:15,839 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:57:21,027 : [('reg:1e-05', 63.88), ('reg:0.0001', 63.9), ('reg:0.001', 61.55), ('reg:0.01', 58.48)]
2019-02-13 00:57:21,027 : Validation : best param found is reg = 0.0001 with score             63.9
2019-02-13 00:57:21,027 : Evaluating...
2019-02-13 00:57:52,385 : 
Dev acc : 63.9 Test acc : 63.0 for COORDINATIONINVERSION classification

2019-02-13 00:57:52,394 : {'STS12': {'MSRpar': {'pearson': (0.29800551466257164, 7.570030585662228e-17), 'spearman': SpearmanrResult(correlation=0.3357838994452631, pvalue=3.1871064299758365e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6318654782941254, 7.829773911024673e-85), 'spearman': SpearmanrResult(correlation=0.6431613848217653, pvalue=8.801965924341226e-89), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47823544208024454, 1.3131779577101142e-27), 'spearman': SpearmanrResult(correlation=0.5977241295275321, pvalue=8.461510031124956e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5489828624010895, 2.94839062559549e-60), 'spearman': SpearmanrResult(correlation=0.576296846708843, pvalue=1.382706352557444e-67), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5508526057190424, 4.855284161752994e-33), 'spearman': SpearmanrResult(correlation=0.4630552559461852, pvalue=1.3428181587715157e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5015883806314146, 'wmean': 0.49821127706405094}, 'spearman': {'mean': 0.5232043032899177, 'wmean': 0.5230199552147904}}}, 'STS13': {'FNWN': {'pearson': (0.15878847541711782, 0.029082685884684413), 'spearman': SpearmanrResult(correlation=0.16971754260450664, pvalue=0.019558304994350423), 'nsamples': 189}, 'headlines': {'pearson': (0.6382940187365744, 4.640487938195143e-87), 'spearman': SpearmanrResult(correlation=0.6341023378208493, pvalue=1.3328192244615853e-85), 'nsamples': 750}, 'OnWN': {'pearson': (0.49108460391801906, 2.166766313617893e-35), 'spearman': SpearmanrResult(correlation=0.49201404033687096, pvalue=1.5443738318460969e-35), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.42938903269057044, 'wmean': 0.5228199991361832}, 'spearman': {'mean': 0.43194464025407564, 'wmean': 0.5224488303645823}}}, 'STS14': {'deft-forum': {'pearson': (0.37681352774139115, 1.2495312238373274e-16), 'spearman': SpearmanrResult(correlation=0.383859513476216, pvalue=3.015380888237665e-17), 'nsamples': 450}, 'deft-news': {'pearson': (0.7247174111011488, 3.9903127055257606e-50), 'spearman': SpearmanrResult(correlation=0.7009095236196715, pvalue=1.2117275309064214e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5853450722123773, 3.609996219935461e-70), 'spearman': SpearmanrResult(correlation=0.5591613968853569, pvalue=6.574785945368578e-63), 'nsamples': 750}, 'images': {'pearson': (0.5666550070973194, 6.400075811623183e-65), 'spearman': SpearmanrResult(correlation=0.5653893535045296, pvalue=1.4111597758063578e-64), 'nsamples': 750}, 'OnWN': {'pearson': (0.6137588240461684, 7.809918431969947e-79), 'spearman': SpearmanrResult(correlation=0.6458595312704644, pvalue=9.472269417808565e-90), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5713477879572377, 3.310547659736987e-66), 'spearman': SpearmanrResult(correlation=0.544947981924023, pvalue=3.1314373774303546e-59), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5731062716926072, 'wmean': 0.5706163544796795}, 'spearman': {'mean': 0.5666878834467103, 'wmean': 0.5652075562235944}}}, 'STS15': {'answers-forums': {'pearson': (0.49503490215467066, 1.4072728784951827e-24), 'spearman': SpearmanrResult(correlation=0.4625020392396925, pvalue=2.8333131790114137e-21), 'nsamples': 375}, 'answers-students': {'pearson': (0.6637879905082753, 1.9322987253315367e-96), 'spearman': SpearmanrResult(correlation=0.6720960260677997, pvalue=1.0608778070740942e-99), 'nsamples': 750}, 'belief': {'pearson': (0.5285552163169923, 2.2544883048585113e-28), 'spearman': SpearmanrResult(correlation=0.56628126483599, pvalue=3.490418536372085e-33), 'nsamples': 375}, 'headlines': {'pearson': (0.6308657310762676, 1.7194821994492406e-84), 'spearman': SpearmanrResult(correlation=0.6354755545054342, pvalue=4.4621277405323506e-86), 'nsamples': 750}, 'images': {'pearson': (0.7008867576098062, 7.140485822954236e-112), 'spearman': SpearmanrResult(correlation=0.7111338357577126, pvalue=1.4490824278051948e-116), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6038261195332024, 'wmean': 0.6268338846075452}, 'spearman': {'mean': 0.6094977440813258, 'wmean': 0.6332742670921969}}}, 'STS16': {'answer-answer': {'pearson': (0.49408045905989084, 4.8669118776915505e-17), 'spearman': SpearmanrResult(correlation=0.5171346517701539, pvalue=8.990042747896874e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.6267219321937699, 1.4080780950613647e-28), 'spearman': SpearmanrResult(correlation=0.6334408836956236, pvalue=2.460030032657408e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6797395546341248, 1.559341013365047e-32), 'spearman': SpearmanrResult(correlation=0.6807850536909653, pvalue=1.1513361783574992e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.76367517122773, 6.86585145383923e-48), 'spearman': SpearmanrResult(correlation=0.8170363108936974, pvalue=8.425790255827406e-60), 'nsamples': 244}, 'question-question': {'pearson': (0.2585292321209124, 0.00015710860303836283), 'spearman': SpearmanrResult(correlation=0.28750159011548565, pvalue=2.433354455981776e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5645492698472856, 'wmean': 0.5718884035212027}, 'spearman': {'mean': 0.5871796980331851, 'wmean': 0.5945229646972597}}}, 'MR': {'devacc': 62.74, 'acc': 65.62, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.93, 'acc': 68.0, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.35, 'acc': 87.33, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.39, 'acc': 90.06, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.16, 'acc': 77.81, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.33, 'acc': 36.7, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.16, 'acc': 81.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.65, 'acc': 71.48, 'f1': 81.78, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 74.91, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8016525987092108, 'pearson': 0.776170577689326, 'spearman': 0.7123677296437668, 'mse': 0.405752075275095, 'yhat': array([2.39989118, 4.23611851, 2.72161094, ..., 3.05343511, 4.25065465,
       4.59093306]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6419749514192992, 'pearson': 0.6270375233457395, 'spearman': 0.6247146932789917, 'mse': 1.4881659884311775, 'yhat': array([1.26308335, 1.48939063, 2.2063324 , ..., 3.96600346, 3.4499822 ,
       4.31422085]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 70.11, 'acc': 70.79, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 17.58, 'acc': 17.69, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.3, 'acc': 29.84, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 59.11, 'acc': 59.07, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 84.31, 'acc': 83.62, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.43, 'acc': 86.03, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.03, 'acc': 79.38, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.01, 'acc': 79.83, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.94, 'acc': 57.94, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 63.9, 'acc': 63.02, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 00:57:52,394 : ********************************************************************************
2019-02-13 00:57:52,394 : ********************************************************************************
2019-02-13 00:57:52,394 : ********************************************************************************
2019-02-13 00:57:52,394 : layer 7
2019-02-13 00:57:52,394 : ********************************************************************************
2019-02-13 00:57:52,394 : ********************************************************************************
2019-02-13 00:57:52,394 : ********************************************************************************
2019-02-13 00:57:52,478 : ***** Transfer task : STS12 *****


2019-02-13 00:57:52,490 : loading BERT mode bert-base-uncased
2019-02-13 00:57:52,490 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:57:52,508 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:57:52,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr82rp78l
2019-02-13 00:57:54,943 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:57:59,470 : MSRpar : pearson = 0.2981, spearman = 0.3332
2019-02-13 00:58:01,659 : MSRvid : pearson = 0.6187, spearman = 0.6304
2019-02-13 00:58:03,275 : SMTeuroparl : pearson = 0.4711, spearman = 0.5950
2019-02-13 00:58:05,764 : surprise.OnWN : pearson = 0.5413, spearman = 0.5686
2019-02-13 00:58:07,147 : surprise.SMTnews : pearson = 0.5641, spearman = 0.4661
2019-02-13 00:58:07,148 : ALL (weighted average) : Pearson = 0.4939,             Spearman = 0.5175
2019-02-13 00:58:07,148 : ALL (average) : Pearson = 0.4987,             Spearman = 0.5187

2019-02-13 00:58:07,148 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 00:58:07,155 : loading BERT mode bert-base-uncased
2019-02-13 00:58:07,156 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:58:07,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:58:07,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphulhaq_3
2019-02-13 00:58:09,604 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:58:12,175 : FNWN : pearson = 0.1568, spearman = 0.1686
2019-02-13 00:58:14,448 : headlines : pearson = 0.6308, spearman = 0.6261
2019-02-13 00:58:16,325 : OnWN : pearson = 0.4739, spearman = 0.4654
2019-02-13 00:58:16,325 : ALL (weighted average) : Pearson = 0.5124,             Spearman = 0.5083
2019-02-13 00:58:16,325 : ALL (average) : Pearson = 0.4205,             Spearman = 0.4200

2019-02-13 00:58:16,325 : ***** Transfer task : STS14 *****


2019-02-13 00:58:16,345 : loading BERT mode bert-base-uncased
2019-02-13 00:58:16,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:58:16,363 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:58:16,363 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpki1j0k4c
2019-02-13 00:58:18,796 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:58:21,697 : deft-forum : pearson = 0.3724, spearman = 0.3795
2019-02-13 00:58:23,063 : deft-news : pearson = 0.7230, spearman = 0.7009
2019-02-13 00:58:25,635 : headlines : pearson = 0.5802, spearman = 0.5536
2019-02-13 00:58:28,222 : images : pearson = 0.5512, spearman = 0.5465
2019-02-13 00:58:30,717 : OnWN : pearson = 0.6084, spearman = 0.6377
2019-02-13 00:58:33,519 : tweet-news : pearson = 0.5872, spearman = 0.5591
2019-02-13 00:58:33,520 : ALL (weighted average) : Pearson = 0.5679,             Spearman = 0.5610
2019-02-13 00:58:33,520 : ALL (average) : Pearson = 0.5704,             Spearman = 0.5629

2019-02-13 00:58:33,520 : ***** Transfer task : STS15 *****


2019-02-13 00:58:33,552 : loading BERT mode bert-base-uncased
2019-02-13 00:58:33,552 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:58:33,569 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:58:33,569 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2_sy2dyw
2019-02-13 00:58:35,999 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:58:38,848 : answers-forums : pearson = 0.5036, spearman = 0.4883
2019-02-13 00:58:40,475 : answers-students : pearson = 0.6577, spearman = 0.6660
2019-02-13 00:58:41,697 : belief : pearson = 0.5305, spearman = 0.5718
2019-02-13 00:58:43,893 : headlines : pearson = 0.6244, spearman = 0.6323
2019-02-13 00:58:46,124 : images : pearson = 0.6921, spearman = 0.7004
2019-02-13 00:58:46,124 : ALL (weighted average) : Pearson = 0.6228,             Spearman = 0.6322
2019-02-13 00:58:46,124 : ALL (average) : Pearson = 0.6017,             Spearman = 0.6118

2019-02-13 00:58:46,124 : ***** Transfer task : STS16 *****


2019-02-13 00:58:46,197 : loading BERT mode bert-base-uncased
2019-02-13 00:58:46,197 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:58:46,215 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:58:46,215 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3hpg80l0
2019-02-13 00:58:48,647 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:58:51,099 : answer-answer : pearson = 0.4951, spearman = 0.5140
2019-02-13 00:58:51,984 : headlines : pearson = 0.6285, spearman = 0.6356
2019-02-13 00:58:52,893 : plagiarism : pearson = 0.6729, spearman = 0.6785
2019-02-13 00:58:54,018 : postediting : pearson = 0.7649, spearman = 0.8134
2019-02-13 00:58:54,631 : question-question : pearson = 0.2300, spearman = 0.2468
2019-02-13 00:58:54,632 : ALL (weighted average) : Pearson = 0.5664,             Spearman = 0.5859
2019-02-13 00:58:54,632 : ALL (average) : Pearson = 0.5583,             Spearman = 0.5777

2019-02-13 00:58:54,632 : ***** Transfer task : MR *****


2019-02-13 00:58:54,649 : loading BERT mode bert-base-uncased
2019-02-13 00:58:54,649 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:58:54,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:58:54,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4xn7tln8
2019-02-13 00:58:57,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:58:58,691 : Generating sentence embeddings
2019-02-13 00:59:21,507 : Generated sentence embeddings
2019-02-13 00:59:21,508 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:59:49,297 : Best param found at split 1: l2reg = 0.01                 with score 64.51
2019-02-13 01:00:19,743 : Best param found at split 2: l2reg = 1e-05                 with score 66.04
2019-02-13 01:00:52,936 : Best param found at split 3: l2reg = 1e-05                 with score 63.99
2019-02-13 01:01:24,036 : Best param found at split 4: l2reg = 1e-05                 with score 66.54
2019-02-13 01:01:53,369 : Best param found at split 5: l2reg = 0.0001                 with score 60.94
2019-02-13 01:01:55,404 : Dev acc : 64.4 Test acc : 65.46

2019-02-13 01:01:55,405 : ***** Transfer task : CR *****


2019-02-13 01:01:55,413 : loading BERT mode bert-base-uncased
2019-02-13 01:01:55,413 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:01:55,433 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:01:55,433 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjh_ocok5
2019-02-13 01:01:57,872 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:01:59,362 : Generating sentence embeddings
2019-02-13 01:02:05,933 : Generated sentence embeddings
2019-02-13 01:02:05,933 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:02:14,505 : Best param found at split 1: l2reg = 1e-05                 with score 72.74
2019-02-13 01:02:24,174 : Best param found at split 2: l2reg = 0.0001                 with score 76.31
2019-02-13 01:02:38,255 : Best param found at split 3: l2reg = 0.001                 with score 72.69
2019-02-13 01:02:51,708 : Best param found at split 4: l2reg = 0.01                 with score 75.97
2019-02-13 01:03:04,658 : Best param found at split 5: l2reg = 0.01                 with score 74.81
2019-02-13 01:03:05,310 : Dev acc : 74.5 Test acc : 73.72

2019-02-13 01:03:05,310 : ***** Transfer task : MPQA *****


2019-02-13 01:03:05,316 : loading BERT mode bert-base-uncased
2019-02-13 01:03:05,316 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:03:05,336 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:03:05,336 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp4zcbmk_
2019-02-13 01:03:07,781 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:03:09,307 : Generating sentence embeddings
2019-02-13 01:03:20,817 : Generated sentence embeddings
2019-02-13 01:03:20,817 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:03:52,174 : Best param found at split 1: l2reg = 0.001                 with score 87.78
2019-02-13 01:04:24,735 : Best param found at split 2: l2reg = 0.01                 with score 87.88
2019-02-13 01:04:52,247 : Best param found at split 3: l2reg = 0.001                 with score 87.28
2019-02-13 01:05:09,250 : Best param found at split 4: l2reg = 0.001                 with score 87.61
2019-02-13 01:05:37,145 : Best param found at split 5: l2reg = 0.0001                 with score 87.45
2019-02-13 01:05:38,281 : Dev acc : 87.6 Test acc : 86.78

2019-02-13 01:05:38,282 : ***** Transfer task : SUBJ *****


2019-02-13 01:05:38,296 : loading BERT mode bert-base-uncased
2019-02-13 01:05:38,297 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:05:38,319 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:05:38,319 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgv1qxdy0
2019-02-13 01:05:40,751 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:05:42,283 : Generating sentence embeddings
2019-02-13 01:06:00,597 : Generated sentence embeddings
2019-02-13 01:06:00,597 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:06:40,152 : Best param found at split 1: l2reg = 1e-05                 with score 92.18
2019-02-13 01:07:21,481 : Best param found at split 2: l2reg = 0.01                 with score 91.68
2019-02-13 01:07:48,700 : Best param found at split 3: l2reg = 0.001                 with score 91.51
2019-02-13 01:08:18,757 : Best param found at split 4: l2reg = 0.001                 with score 91.93
2019-02-13 01:08:40,085 : Best param found at split 5: l2reg = 0.01                 with score 91.99
2019-02-13 01:08:40,765 : Dev acc : 91.86 Test acc : 91.94

2019-02-13 01:08:40,766 : ***** Transfer task : SST Binary classification *****


2019-02-13 01:08:40,891 : loading BERT mode bert-base-uncased
2019-02-13 01:08:40,891 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:08:40,915 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:08:40,915 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6xqojc8g
2019-02-13 01:08:43,341 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:08:44,760 : Computing embedding for train
2019-02-13 01:10:01,639 : Computed train embeddings
2019-02-13 01:10:01,639 : Computing embedding for dev
2019-02-13 01:10:03,127 : Computed dev embeddings
2019-02-13 01:10:03,127 : Computing embedding for test
2019-02-13 01:10:06,285 : Computed test embeddings
2019-02-13 01:10:06,286 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:10:52,225 : [('reg:1e-05', 81.42), ('reg:0.0001', 81.31), ('reg:0.001', 77.29), ('reg:0.01', 77.29)]
2019-02-13 01:10:52,225 : Validation : best param found is reg = 1e-05 with score             81.42
2019-02-13 01:10:52,225 : Evaluating...
2019-02-13 01:11:07,509 : 
Dev acc : 81.42 Test acc : 79.35 for             SST Binary classification

2019-02-13 01:11:07,509 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 01:11:07,574 : loading BERT mode bert-base-uncased
2019-02-13 01:11:07,574 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:11:07,595 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:11:07,595 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk0i1m03c
2019-02-13 01:11:10,027 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:11:11,579 : Computing embedding for train
2019-02-13 01:11:29,944 : Computed train embeddings
2019-02-13 01:11:29,944 : Computing embedding for dev
2019-02-13 01:11:31,984 : Computed dev embeddings
2019-02-13 01:11:31,984 : Computing embedding for test
2019-02-13 01:11:36,145 : Computed test embeddings
2019-02-13 01:11:36,145 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:11:42,628 : [('reg:1e-05', 36.78), ('reg:0.0001', 35.6), ('reg:0.001', 36.88), ('reg:0.01', 38.15)]
2019-02-13 01:11:42,628 : Validation : best param found is reg = 0.01 with score             38.15
2019-02-13 01:11:42,628 : Evaluating...
2019-02-13 01:11:44,768 : 
Dev acc : 38.15 Test acc : 42.04 for             SST Fine-Grained classification

2019-02-13 01:11:44,768 : ***** Transfer task : TREC *****


2019-02-13 01:11:44,781 : loading BERT mode bert-base-uncased
2019-02-13 01:11:44,782 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:11:44,800 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:11:44,800 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphiha93n5
2019-02-13 01:11:47,226 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:11:56,900 : Computed train embeddings
2019-02-13 01:11:57,652 : Computed test embeddings
2019-02-13 01:11:57,653 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 01:12:16,256 : [('reg:1e-05', 69.01), ('reg:0.0001', 73.22), ('reg:0.001', 72.31), ('reg:0.01', 60.52)]
2019-02-13 01:12:16,256 : Cross-validation : best param found is reg = 0.0001             with score 73.22
2019-02-13 01:12:16,256 : Evaluating...
2019-02-13 01:12:17,128 : 
Dev acc : 73.22 Test acc : 88.8             for TREC

2019-02-13 01:12:17,129 : ***** Transfer task : MRPC *****


2019-02-13 01:12:17,150 : loading BERT mode bert-base-uncased
2019-02-13 01:12:17,150 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:12:17,171 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:12:17,171 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpia9wv_r8
2019-02-13 01:12:19,597 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:12:21,141 : Computing embedding for train
2019-02-13 01:12:38,550 : Computed train embeddings
2019-02-13 01:12:38,550 : Computing embedding for test
2019-02-13 01:12:46,323 : Computed test embeddings
2019-02-13 01:12:46,339 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 01:12:57,324 : [('reg:1e-05', 72.03), ('reg:0.0001', 71.13), ('reg:0.001', 71.61), ('reg:0.01', 71.86)]
2019-02-13 01:12:57,324 : Cross-validation : best param found is reg = 1e-05             with score 72.03
2019-02-13 01:12:57,324 : Evaluating...
2019-02-13 01:12:57,926 : Dev acc : 72.03 Test acc 69.62; Test F1 75.24 for MRPC.

2019-02-13 01:12:57,926 : ***** Transfer task : SICK-Entailment*****


2019-02-13 01:12:57,990 : loading BERT mode bert-base-uncased
2019-02-13 01:12:57,990 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:12:58,009 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:12:58,010 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_1zbqlq6
2019-02-13 01:13:00,438 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:13:01,909 : Computing embedding for train
2019-02-13 01:13:12,697 : Computed train embeddings
2019-02-13 01:13:12,697 : Computing embedding for dev
2019-02-13 01:13:13,640 : Computed dev embeddings
2019-02-13 01:13:13,641 : Computing embedding for test
2019-02-13 01:13:21,538 : Computed test embeddings
2019-02-13 01:13:21,565 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:13:24,210 : [('reg:1e-05', 74.0), ('reg:0.0001', 76.0), ('reg:0.001', 71.0), ('reg:0.01', 72.8)]
2019-02-13 01:13:24,210 : Validation : best param found is reg = 0.0001 with score             76.0
2019-02-13 01:13:24,210 : Evaluating...
2019-02-13 01:13:24,768 : 
Dev acc : 76.0 Test acc : 75.08 for                        SICK entailment

2019-02-13 01:13:24,768 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 01:13:24,795 : loading BERT mode bert-base-uncased
2019-02-13 01:13:24,795 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:13:24,851 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:13:24,851 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9h8indxa
2019-02-13 01:13:27,284 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:13:28,730 : Computing embedding for train
2019-02-13 01:13:38,773 : Computed train embeddings
2019-02-13 01:13:38,773 : Computing embedding for dev
2019-02-13 01:13:39,982 : Computed dev embeddings
2019-02-13 01:13:39,982 : Computing embedding for test
2019-02-13 01:13:51,354 : Computed test embeddings
2019-02-13 01:14:38,257 : Dev : Pearson 0.7874576173065023
2019-02-13 01:14:38,257 : Test : Pearson 0.7788641232113926 Spearman 0.7137650912235657 MSE 0.4040303642313609                        for SICK Relatedness

2019-02-13 01:14:38,258 : 

***** Transfer task : STSBenchmark*****


2019-02-13 01:14:38,296 : loading BERT mode bert-base-uncased
2019-02-13 01:14:38,296 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:14:38,325 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:14:38,325 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo76aetjo
2019-02-13 01:14:40,766 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:14:42,380 : Computing embedding for train
2019-02-13 01:15:04,814 : Computed train embeddings
2019-02-13 01:15:04,814 : Computing embedding for dev
2019-02-13 01:15:10,895 : Computed dev embeddings
2019-02-13 01:15:10,895 : Computing embedding for test
2019-02-13 01:15:15,544 : Computed test embeddings
2019-02-13 01:16:24,319 : Dev : Pearson 0.646405449255224
2019-02-13 01:16:24,319 : Test : Pearson 0.6474794577567471 Spearman 0.6458808812620485 MSE 1.4307438215016695                        for SICK Relatedness

2019-02-13 01:16:24,320 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 01:16:24,637 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 01:16:24,648 : loading BERT mode bert-base-uncased
2019-02-13 01:16:24,648 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:16:24,671 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:16:24,671 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3pn5f20_
2019-02-13 01:16:27,098 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:16:28,566 : Computing embeddings for train/dev/test
2019-02-13 01:19:18,720 : Computed embeddings
2019-02-13 01:19:18,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:20:38,613 : [('reg:1e-05', 63.96), ('reg:0.0001', 59.53), ('reg:0.001', 62.64), ('reg:0.01', 54.03)]
2019-02-13 01:20:38,614 : Validation : best param found is reg = 1e-05 with score             63.96
2019-02-13 01:20:38,614 : Evaluating...
2019-02-13 01:20:49,454 : 
Dev acc : 64.0 Test acc : 64.3 for LENGTH classification

2019-02-13 01:20:49,461 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 01:20:49,814 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 01:20:49,859 : loading BERT mode bert-base-uncased
2019-02-13 01:20:49,859 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:20:49,888 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:20:49,889 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppxt_r1ru
2019-02-13 01:20:52,353 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:20:53,777 : Computing embeddings for train/dev/test
2019-02-13 01:23:17,489 : Computed embeddings
2019-02-13 01:23:17,489 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:24:39,073 : [('reg:1e-05', 18.34), ('reg:0.0001', 8.3), ('reg:0.001', 0.57), ('reg:0.01', 0.19)]
2019-02-13 01:24:39,073 : Validation : best param found is reg = 1e-05 with score             18.34
2019-02-13 01:24:39,073 : Evaluating...
2019-02-13 01:24:56,013 : 
Dev acc : 18.3 Test acc : 18.3 for WORDCONTENT classification

2019-02-13 01:24:56,021 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 01:24:56,382 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 01:24:56,448 : loading BERT mode bert-base-uncased
2019-02-13 01:24:56,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:24:56,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:24:56,550 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnp08gwl4
2019-02-13 01:24:58,982 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:25:00,420 : Computing embeddings for train/dev/test
2019-02-13 01:27:47,062 : Computed embeddings
2019-02-13 01:27:47,062 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:28:32,973 : [('reg:1e-05', 29.5), ('reg:0.0001', 29.02), ('reg:0.001', 26.77), ('reg:0.01', 27.0)]
2019-02-13 01:28:32,973 : Validation : best param found is reg = 1e-05 with score             29.5
2019-02-13 01:28:32,973 : Evaluating...
2019-02-13 01:28:45,913 : 
Dev acc : 29.5 Test acc : 29.8 for DEPTH classification

2019-02-13 01:28:45,921 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 01:28:46,490 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 01:28:46,553 : loading BERT mode bert-base-uncased
2019-02-13 01:28:46,553 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:28:46,581 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:28:46,581 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf2cs3t8v
2019-02-13 01:28:49,017 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:28:50,450 : Computing embeddings for train/dev/test
2019-02-13 01:31:55,283 : Computed embeddings
2019-02-13 01:31:55,283 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:33:08,888 : [('reg:1e-05', 61.71), ('reg:0.0001', 59.04), ('reg:0.001', 55.3), ('reg:0.01', 39.39)]
2019-02-13 01:33:08,888 : Validation : best param found is reg = 1e-05 with score             61.71
2019-02-13 01:33:08,888 : Evaluating...
2019-02-13 01:33:22,244 : 
Dev acc : 61.7 Test acc : 61.6 for TOPCONSTITUENTS classification

2019-02-13 01:33:22,252 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 01:33:22,624 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 01:33:22,694 : loading BERT mode bert-base-uncased
2019-02-13 01:33:22,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:33:22,829 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:33:22,829 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8up59g42
2019-02-13 01:33:25,279 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:33:26,837 : Computing embeddings for train/dev/test
2019-02-13 01:36:39,516 : Computed embeddings
2019-02-13 01:36:39,516 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:37:40,235 : [('reg:1e-05', 86.1), ('reg:0.0001', 86.07), ('reg:0.001', 85.94), ('reg:0.01', 79.4)]
2019-02-13 01:37:40,235 : Validation : best param found is reg = 1e-05 with score             86.1
2019-02-13 01:37:40,236 : Evaluating...
2019-02-13 01:38:05,423 : 
Dev acc : 86.1 Test acc : 85.0 for BIGRAMSHIFT classification

2019-02-13 01:38:05,433 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 01:38:06,016 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 01:38:06,082 : loading BERT mode bert-base-uncased
2019-02-13 01:38:06,082 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:38:06,113 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:38:06,113 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4zyur8nu
2019-02-13 01:38:08,572 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:38:10,030 : Computing embeddings for train/dev/test
2019-02-13 01:40:58,979 : Computed embeddings
2019-02-13 01:40:58,979 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:41:46,304 : [('reg:1e-05', 87.61), ('reg:0.0001', 87.71), ('reg:0.001', 87.5), ('reg:0.01', 86.99)]
2019-02-13 01:41:46,305 : Validation : best param found is reg = 0.0001 with score             87.71
2019-02-13 01:41:46,305 : Evaluating...
2019-02-13 01:42:01,796 : 
Dev acc : 87.7 Test acc : 86.4 for TENSE classification

2019-02-13 01:42:01,803 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 01:42:02,381 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 01:42:02,444 : loading BERT mode bert-base-uncased
2019-02-13 01:42:02,444 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:42:02,473 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:42:02,473 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuzlpz9fz
2019-02-13 01:42:04,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:42:06,376 : Computing embeddings for train/dev/test
2019-02-13 01:44:47,577 : Computed embeddings
2019-02-13 01:44:47,578 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:46:02,384 : [('reg:1e-05', 81.25), ('reg:0.0001', 81.31), ('reg:0.001', 81.43), ('reg:0.01', 81.25)]
2019-02-13 01:46:02,384 : Validation : best param found is reg = 0.001 with score             81.43
2019-02-13 01:46:02,384 : Evaluating...
2019-02-13 01:46:25,061 : 
Dev acc : 81.4 Test acc : 80.6 for SUBJNUMBER classification

2019-02-13 01:46:25,069 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 01:46:25,507 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 01:46:25,577 : loading BERT mode bert-base-uncased
2019-02-13 01:46:25,577 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:46:25,605 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:46:25,606 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcy2n36ih
2019-02-13 01:46:28,033 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:46:29,523 : Computing embeddings for train/dev/test
2019-02-13 01:48:58,795 : Computed embeddings
2019-02-13 01:48:58,795 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:50:44,997 : [('reg:1e-05', 78.7), ('reg:0.0001', 78.8), ('reg:0.001', 78.8), ('reg:0.01', 77.66)]
2019-02-13 01:50:44,997 : Validation : best param found is reg = 0.0001 with score             78.8
2019-02-13 01:50:44,997 : Evaluating...
2019-02-13 01:51:14,282 : 
Dev acc : 78.8 Test acc : 79.4 for OBJNUMBER classification

2019-02-13 01:51:14,289 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 01:51:14,708 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 01:51:14,778 : loading BERT mode bert-base-uncased
2019-02-13 01:51:14,779 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:51:14,906 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:51:14,906 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu_tp2wr2
2019-02-13 01:51:17,361 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:51:18,829 : Computing embeddings for train/dev/test
2019-02-13 01:54:05,849 : Computed embeddings
2019-02-13 01:54:05,849 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:55:21,218 : [('reg:1e-05', 60.02), ('reg:0.0001', 59.98), ('reg:0.001', 60.01), ('reg:0.01', 60.47)]
2019-02-13 01:55:21,218 : Validation : best param found is reg = 0.01 with score             60.47
2019-02-13 01:55:21,218 : Evaluating...
2019-02-13 01:55:28,693 : 
Dev acc : 60.5 Test acc : 59.1 for ODDMANOUT classification

2019-02-13 01:55:28,701 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 01:55:29,108 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 01:55:29,186 : loading BERT mode bert-base-uncased
2019-02-13 01:55:29,186 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:55:29,315 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:55:29,316 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptlee581j
2019-02-13 01:55:31,829 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:55:33,328 : Computing embeddings for train/dev/test
2019-02-13 01:58:29,163 : Computed embeddings
2019-02-13 01:58:29,163 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:59:50,348 : [('reg:1e-05', 55.73), ('reg:0.0001', 55.68), ('reg:0.001', 55.29), ('reg:0.01', 51.64)]
2019-02-13 01:59:50,348 : Validation : best param found is reg = 1e-05 with score             55.73
2019-02-13 01:59:50,348 : Evaluating...
2019-02-13 02:00:06,266 : 
Dev acc : 55.7 Test acc : 55.4 for COORDINATIONINVERSION classification

2019-02-13 02:00:06,274 : {'STS12': {'MSRpar': {'pearson': (0.2981410492071199, 7.319850661186964e-17), 'spearman': SpearmanrResult(correlation=0.3331825448570781, pvalue=6.6828614201996355e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6186565673076134, 2.035232469419895e-80), 'spearman': SpearmanrResult(correlation=0.6304397991394215, pvalue=2.401999734591585e-84), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47107265805056026, 9.90345978754508e-27), 'spearman': SpearmanrResult(correlation=0.5950121401201401, pvalue=2.6765751582712125e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5413159011007462, 2.5562644865014283e-58), 'spearman': SpearmanrResult(correlation=0.5686482153884025, pvalue=1.8296772185126702e-65), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.564127802804599, 6.674611077818537e-35), 'spearman': SpearmanrResult(correlation=0.4660537254008056, pvalue=6.591969228250448e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.49866279569412775, 'wmean': 0.4938527932998236}, 'spearman': {'mean': 0.5186672849811697, 'wmean': 0.517461045137948}}}, 'STS13': {'FNWN': {'pearson': (0.156778583628512, 0.031209504070672363), 'spearman': SpearmanrResult(correlation=0.16860299612689955, pvalue=0.020386424368231997), 'nsamples': 189}, 'headlines': {'pearson': (0.630826140380286, 1.773786736844811e-84), 'spearman': SpearmanrResult(correlation=0.6260985444682912, pvalue=7.037366521598613e-83), 'nsamples': 750}, 'OnWN': {'pearson': (0.47390270950806895, 9.434464281811659e-33), 'spearman': SpearmanrResult(correlation=0.46535678937868863, pvalue=1.7083446010799967e-31), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.42050247783895567, 'wmean': 0.5124067850833532}, 'spearman': {'mean': 0.42001944332462643, 'wmean': 0.5083366889737645}}}, 'STS14': {'deft-forum': {'pearson': (0.37235916681612463, 3.0156804205097445e-16), 'spearman': SpearmanrResult(correlation=0.379466399961024, pvalue=7.34590925466866e-17), 'nsamples': 450}, 'deft-news': {'pearson': (0.7229884222384468, 8.755601030699382e-50), 'spearman': SpearmanrResult(correlation=0.700885708308919, pvalue=1.2236743630558444e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5802143265312536, 1.0770386836215897e-68), 'spearman': SpearmanrResult(correlation=0.553589769670222, pvalue=1.9086185397824357e-61), 'nsamples': 750}, 'images': {'pearson': (0.5511582499416293, 8.137837674132223e-61), 'spearman': SpearmanrResult(correlation=0.5464585749422874, pvalue=1.2977804957757058e-59), 'nsamples': 750}, 'OnWN': {'pearson': (0.6084401156092961, 3.815939536603471e-77), 'spearman': SpearmanrResult(correlation=0.6376657207376274, pvalue=7.701601335405377e-87), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5872107455632243, 1.0344527079680136e-70), 'spearman': SpearmanrResult(correlation=0.559111519513651, pvalue=6.777977788484154e-63), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5703951711166625, 'wmean': 0.5679268613260914}, 'spearman': {'mean': 0.5628629488556218, 'wmean': 0.560971941632794}}}, 'STS15': {'answers-forums': {'pearson': (0.5035991412131355, 1.6529581800386946e-25), 'spearman': SpearmanrResult(correlation=0.48827223724801727, pvalue=7.320517356638968e-24), 'nsamples': 375}, 'answers-students': {'pearson': (0.6577261125554303, 3.977507725688212e-94), 'spearman': SpearmanrResult(correlation=0.665964534848632, pvalue=2.7674846787225974e-97), 'nsamples': 750}, 'belief': {'pearson': (0.5305216635415728, 1.309326068349684e-28), 'spearman': SpearmanrResult(correlation=0.5718477504609795, pvalue=6.022633443655633e-34), 'nsamples': 375}, 'headlines': {'pearson': (0.6244384950764078, 2.5246285745667938e-82), 'spearman': SpearmanrResult(correlation=0.6322770371077504, pvalue=5.65900562690981e-85), 'nsamples': 750}, 'images': {'pearson': (0.6921014200382868, 5.24796803367259e-108), 'spearman': SpearmanrResult(correlation=0.7004131641223772, pvalue=1.1634765281438695e-111), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6016773664849666, 'wmean': 0.6228316075118697}, 'spearman': {'mean': 0.6117549447575513, 'wmean': 0.6321786824833145}}}, 'STS16': {'answer-answer': {'pearson': (0.4950797907235934, 4.1190297024495717e-17), 'spearman': SpearmanrResult(correlation=0.5139691106167098, pvalue=1.5835207172114458e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6284704361803324, 8.979086375885866e-29), 'spearman': SpearmanrResult(correlation=0.6356169093484715, pvalue=1.3852764866977421e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6729305835354212, 1.0907097938712454e-31), 'spearman': SpearmanrResult(correlation=0.678539147707006, pvalue=2.2055982331423917e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.7648531745441184, 4.0599783613769126e-48), 'spearman': SpearmanrResult(correlation=0.813402629165494, pvalue=7.173707270166841e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.23004956521313488, 0.0008054960668942018), 'spearman': SpearmanrResult(correlation=0.24676626826956574, pvalue=0.0003158681742155237), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.55827671003932, 'wmean': 0.5663726588399262}, 'spearman': {'mean': 0.5776588130214494, 'wmean': 0.5859402698834273}}}, 'MR': {'devacc': 64.4, 'acc': 65.46, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 74.5, 'acc': 73.72, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.6, 'acc': 86.78, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.86, 'acc': 91.94, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.42, 'acc': 79.35, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.15, 'acc': 42.04, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 73.22, 'acc': 88.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.03, 'acc': 69.62, 'f1': 75.24, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.0, 'acc': 75.08, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7874576173065023, 'pearson': 0.7788641232113926, 'spearman': 0.7137650912235657, 'mse': 0.4040303642313609, 'yhat': array([2.64468767, 4.0588832 , 3.05230148, ..., 2.98883684, 4.85290666,
       4.65678326]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.646405449255224, 'pearson': 0.6474794577567471, 'spearman': 0.6458808812620485, 'mse': 1.4307438215016695, 'yhat': array([1.02788539, 1.46765863, 2.16271286, ..., 3.96495983, 3.99179306,
       4.7884861 ]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 63.96, 'acc': 64.34, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 18.34, 'acc': 18.31, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.5, 'acc': 29.78, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 61.71, 'acc': 61.64, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.1, 'acc': 85.04, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.71, 'acc': 86.38, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.43, 'acc': 80.62, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.8, 'acc': 79.42, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.47, 'acc': 59.13, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.73, 'acc': 55.38, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 02:00:06,274 : ********************************************************************************
2019-02-13 02:00:06,274 : ********************************************************************************
2019-02-13 02:00:06,274 : ********************************************************************************
2019-02-13 02:00:06,274 : layer 8
2019-02-13 02:00:06,274 : ********************************************************************************
2019-02-13 02:00:06,274 : ********************************************************************************
2019-02-13 02:00:06,274 : ********************************************************************************
2019-02-13 02:00:06,362 : ***** Transfer task : STS12 *****


2019-02-13 02:00:06,398 : loading BERT mode bert-base-uncased
2019-02-13 02:00:06,398 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:00:06,414 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:00:06,415 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiferpe5n
2019-02-13 02:00:08,885 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:00:12,566 : MSRpar : pearson = 0.2988, spearman = 0.3272
2019-02-13 02:00:13,783 : MSRvid : pearson = 0.5666, spearman = 0.5779
2019-02-13 02:00:14,717 : SMTeuroparl : pearson = 0.4743, spearman = 0.5940
2019-02-13 02:00:16,353 : surprise.OnWN : pearson = 0.5380, spearman = 0.5650
2019-02-13 02:00:17,241 : surprise.SMTnews : pearson = 0.5826, spearman = 0.4821
2019-02-13 02:00:17,241 : ALL (weighted average) : Pearson = 0.4835,             Spearman = 0.5044
2019-02-13 02:00:17,241 : ALL (average) : Pearson = 0.4921,             Spearman = 0.5092

2019-02-13 02:00:17,241 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 02:00:17,250 : loading BERT mode bert-base-uncased
2019-02-13 02:00:17,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:00:17,267 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:00:17,267 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn606pkbp
2019-02-13 02:00:19,710 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:00:21,926 : FNWN : pearson = 0.1544, spearman = 0.1756
2019-02-13 02:00:23,252 : headlines : pearson = 0.6260, spearman = 0.6193
2019-02-13 02:00:24,254 : OnWN : pearson = 0.4485, spearman = 0.4440
2019-02-13 02:00:24,255 : ALL (weighted average) : Pearson = 0.5002,             Spearman = 0.4979
2019-02-13 02:00:24,255 : ALL (average) : Pearson = 0.4096,             Spearman = 0.4130

2019-02-13 02:00:24,255 : ***** Transfer task : STS14 *****


2019-02-13 02:00:24,273 : loading BERT mode bert-base-uncased
2019-02-13 02:00:24,273 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:00:24,321 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:00:24,322 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4wqy70t_
2019-02-13 02:00:26,820 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:00:29,127 : deft-forum : pearson = 0.3461, spearman = 0.3502
2019-02-13 02:00:30,033 : deft-news : pearson = 0.7273, spearman = 0.7059
2019-02-13 02:00:31,475 : headlines : pearson = 0.5715, spearman = 0.5435
2019-02-13 02:00:32,874 : images : pearson = 0.5352, spearman = 0.5248
2019-02-13 02:00:34,293 : OnWN : pearson = 0.5879, spearman = 0.6196
2019-02-13 02:00:36,021 : tweet-news : pearson = 0.5850, spearman = 0.5513
2019-02-13 02:00:36,021 : ALL (weighted average) : Pearson = 0.5556,             Spearman = 0.5463
2019-02-13 02:00:36,021 : ALL (average) : Pearson = 0.5588,             Spearman = 0.5492

2019-02-13 02:00:36,021 : ***** Transfer task : STS15 *****


2019-02-13 02:00:36,052 : loading BERT mode bert-base-uncased
2019-02-13 02:00:36,053 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:00:36,070 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:00:36,070 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi268ewh4
2019-02-13 02:00:38,496 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:00:40,854 : answers-forums : pearson = 0.4956, spearman = 0.4776
2019-02-13 02:00:42,009 : answers-students : pearson = 0.6637, spearman = 0.6720
2019-02-13 02:00:42,981 : belief : pearson = 0.5417, spearman = 0.5800
2019-02-13 02:00:44,292 : headlines : pearson = 0.6166, spearman = 0.6242
2019-02-13 02:00:45,864 : images : pearson = 0.6747, spearman = 0.6839
2019-02-13 02:00:45,864 : ALL (weighted average) : Pearson = 0.6184,             Spearman = 0.6272
2019-02-13 02:00:45,864 : ALL (average) : Pearson = 0.5985,             Spearman = 0.6076

2019-02-13 02:00:45,864 : ***** Transfer task : STS16 *****


2019-02-13 02:00:45,934 : loading BERT mode bert-base-uncased
2019-02-13 02:00:45,934 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:00:45,952 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:00:45,952 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0y63r2zu
2019-02-13 02:00:48,386 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:00:50,477 : answer-answer : pearson = 0.5111, spearman = 0.5206
2019-02-13 02:00:51,012 : headlines : pearson = 0.6275, spearman = 0.6322
2019-02-13 02:00:51,613 : plagiarism : pearson = 0.6815, spearman = 0.6888
2019-02-13 02:00:52,497 : postediting : pearson = 0.7623, spearman = 0.8117
2019-02-13 02:00:53,040 : question-question : pearson = 0.2197, spearman = 0.2355
2019-02-13 02:00:53,040 : ALL (weighted average) : Pearson = 0.5689,             Spearman = 0.5863
2019-02-13 02:00:53,040 : ALL (average) : Pearson = 0.5604,             Spearman = 0.5778

2019-02-13 02:00:53,040 : ***** Transfer task : MR *****


2019-02-13 02:00:53,056 : loading BERT mode bert-base-uncased
2019-02-13 02:00:53,056 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:00:53,077 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:00:53,077 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfl8s4n6v
2019-02-13 02:00:55,503 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:00:57,013 : Generating sentence embeddings
2019-02-13 02:01:16,297 : Generated sentence embeddings
2019-02-13 02:01:16,297 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:01:58,090 : Best param found at split 1: l2reg = 0.0001                 with score 68.93
2019-02-13 02:02:49,990 : Best param found at split 2: l2reg = 1e-05                 with score 64.63
2019-02-13 02:03:36,682 : Best param found at split 3: l2reg = 0.001                 with score 68.83
2019-02-13 02:04:08,360 : Best param found at split 4: l2reg = 1e-05                 with score 65.31
2019-02-13 02:04:28,392 : Best param found at split 5: l2reg = 0.0001                 with score 65.38
2019-02-13 02:04:29,369 : Dev acc : 66.62 Test acc : 68.34

2019-02-13 02:04:29,370 : ***** Transfer task : CR *****


2019-02-13 02:04:29,378 : loading BERT mode bert-base-uncased
2019-02-13 02:04:29,378 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:04:29,397 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:04:29,398 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkjl6vguw
2019-02-13 02:04:31,850 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:04:33,264 : Generating sentence embeddings
2019-02-13 02:04:38,350 : Generated sentence embeddings
2019-02-13 02:04:38,351 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:04:42,706 : Best param found at split 1: l2reg = 1e-05                 with score 74.16
2019-02-13 02:04:47,366 : Best param found at split 2: l2reg = 0.0001                 with score 75.62
2019-02-13 02:04:54,314 : Best param found at split 3: l2reg = 1e-05                 with score 78.61
2019-02-13 02:05:05,907 : Best param found at split 4: l2reg = 0.0001                 with score 76.7
2019-02-13 02:05:17,677 : Best param found at split 5: l2reg = 0.001                 with score 74.48
2019-02-13 02:05:18,192 : Dev acc : 75.91 Test acc : 65.91

2019-02-13 02:05:18,193 : ***** Transfer task : MPQA *****


2019-02-13 02:05:18,198 : loading BERT mode bert-base-uncased
2019-02-13 02:05:18,198 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:05:18,217 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:05:18,217 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp52c2pbe8
2019-02-13 02:05:20,643 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:05:22,243 : Generating sentence embeddings
2019-02-13 02:05:36,480 : Generated sentence embeddings
2019-02-13 02:05:36,481 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:06:13,077 : Best param found at split 1: l2reg = 0.01                 with score 87.42
2019-02-13 02:06:57,958 : Best param found at split 2: l2reg = 0.01                 with score 88.25
2019-02-13 02:07:37,314 : Best param found at split 3: l2reg = 0.01                 with score 86.89
2019-02-13 02:07:54,886 : Best param found at split 4: l2reg = 0.0001                 with score 87.96
2019-02-13 02:08:11,724 : Best param found at split 5: l2reg = 0.001                 with score 88.02
2019-02-13 02:08:12,300 : Dev acc : 87.71 Test acc : 87.15

2019-02-13 02:08:12,301 : ***** Transfer task : SUBJ *****


2019-02-13 02:08:12,315 : loading BERT mode bert-base-uncased
2019-02-13 02:08:12,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:08:12,337 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:08:12,337 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm7d2ovgq
2019-02-13 02:08:14,768 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:08:16,204 : Generating sentence embeddings
2019-02-13 02:08:32,145 : Generated sentence embeddings
2019-02-13 02:08:32,146 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:08:54,257 : Best param found at split 1: l2reg = 0.01                 with score 92.56
2019-02-13 02:09:26,019 : Best param found at split 2: l2reg = 0.001                 with score 92.4
2019-02-13 02:10:07,945 : Best param found at split 3: l2reg = 0.001                 with score 92.54
2019-02-13 02:10:55,099 : Best param found at split 4: l2reg = 0.0001                 with score 92.84
2019-02-13 02:11:30,576 : Best param found at split 5: l2reg = 0.001                 with score 92.69
2019-02-13 02:11:32,730 : Dev acc : 92.61 Test acc : 92.68

2019-02-13 02:11:32,731 : ***** Transfer task : SST Binary classification *****


2019-02-13 02:11:32,870 : loading BERT mode bert-base-uncased
2019-02-13 02:11:32,870 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:11:32,893 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:11:32,893 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpalurq2tf
2019-02-13 02:11:35,355 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:11:36,833 : Computing embedding for train
2019-02-13 02:12:43,422 : Computed train embeddings
2019-02-13 02:12:43,422 : Computing embedding for dev
2019-02-13 02:12:44,818 : Computed dev embeddings
2019-02-13 02:12:44,818 : Computing embedding for test
2019-02-13 02:12:47,840 : Computed test embeddings
2019-02-13 02:12:47,840 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:13:33,788 : [('reg:1e-05', 81.54), ('reg:0.0001', 81.54), ('reg:0.001', 81.42), ('reg:0.01', 81.77)]
2019-02-13 02:13:33,788 : Validation : best param found is reg = 0.01 with score             81.77
2019-02-13 02:13:33,788 : Evaluating...
2019-02-13 02:13:46,689 : 
Dev acc : 81.77 Test acc : 81.16 for             SST Binary classification

2019-02-13 02:13:46,693 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 02:13:46,743 : loading BERT mode bert-base-uncased
2019-02-13 02:13:46,743 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:13:46,765 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:13:46,765 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeeilvsf2
2019-02-13 02:13:49,200 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:13:50,771 : Computing embedding for train
2019-02-13 02:14:10,400 : Computed train embeddings
2019-02-13 02:14:10,400 : Computing embedding for dev
2019-02-13 02:14:12,994 : Computed dev embeddings
2019-02-13 02:14:12,994 : Computing embedding for test
2019-02-13 02:14:18,284 : Computed test embeddings
2019-02-13 02:14:18,284 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:14:26,721 : [('reg:1e-05', 31.79), ('reg:0.0001', 34.24), ('reg:0.001', 35.79), ('reg:0.01', 39.6)]
2019-02-13 02:14:26,721 : Validation : best param found is reg = 0.01 with score             39.6
2019-02-13 02:14:26,721 : Evaluating...
2019-02-13 02:14:28,948 : 
Dev acc : 39.6 Test acc : 39.95 for             SST Fine-Grained classification

2019-02-13 02:14:28,948 : ***** Transfer task : TREC *****


2019-02-13 02:14:28,962 : loading BERT mode bert-base-uncased
2019-02-13 02:14:28,962 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:14:28,981 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:14:28,981 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvakttzkg
2019-02-13 02:14:31,407 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:14:44,487 : Computed train embeddings
2019-02-13 02:14:45,555 : Computed test embeddings
2019-02-13 02:14:45,555 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 02:15:10,813 : [('reg:1e-05', 63.13), ('reg:0.0001', 69.99), ('reg:0.001', 65.74), ('reg:0.01', 61.52)]
2019-02-13 02:15:10,813 : Cross-validation : best param found is reg = 0.0001             with score 69.99
2019-02-13 02:15:10,813 : Evaluating...
2019-02-13 02:15:12,121 : 
Dev acc : 69.99 Test acc : 86.2             for TREC

2019-02-13 02:15:12,122 : ***** Transfer task : MRPC *****


2019-02-13 02:15:12,179 : loading BERT mode bert-base-uncased
2019-02-13 02:15:12,180 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:15:12,198 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:15:12,199 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcushdaoe
2019-02-13 02:15:14,625 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:15:16,193 : Computing embedding for train
2019-02-13 02:15:36,264 : Computed train embeddings
2019-02-13 02:15:36,264 : Computing embedding for test
2019-02-13 02:15:43,880 : Computed test embeddings
2019-02-13 02:15:43,896 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 02:15:57,105 : [('reg:1e-05', 71.39), ('reg:0.0001', 71.52), ('reg:0.001', 71.74), ('reg:0.01', 71.69)]
2019-02-13 02:15:57,105 : Cross-validation : best param found is reg = 0.001             with score 71.74
2019-02-13 02:15:57,105 : Evaluating...
2019-02-13 02:15:57,778 : Dev acc : 71.74 Test acc 68.41; Test F1 73.81 for MRPC.

2019-02-13 02:15:57,778 : ***** Transfer task : SICK-Entailment*****


2019-02-13 02:15:57,802 : loading BERT mode bert-base-uncased
2019-02-13 02:15:57,803 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:15:57,823 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:15:57,823 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpod7dpwx7
2019-02-13 02:16:00,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:16:01,798 : Computing embedding for train
2019-02-13 02:16:13,571 : Computed train embeddings
2019-02-13 02:16:13,571 : Computing embedding for dev
2019-02-13 02:16:15,220 : Computed dev embeddings
2019-02-13 02:16:15,220 : Computing embedding for test
2019-02-13 02:16:24,389 : Computed test embeddings
2019-02-13 02:16:24,417 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:16:26,552 : [('reg:1e-05', 76.8), ('reg:0.0001', 77.4), ('reg:0.001', 77.6), ('reg:0.01', 78.2)]
2019-02-13 02:16:26,552 : Validation : best param found is reg = 0.01 with score             78.2
2019-02-13 02:16:26,553 : Evaluating...
2019-02-13 02:16:27,106 : 
Dev acc : 78.2 Test acc : 76.17 for                        SICK entailment

2019-02-13 02:16:27,106 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 02:16:27,133 : loading BERT mode bert-base-uncased
2019-02-13 02:16:27,133 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:16:27,152 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:16:27,153 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjylyesg3
2019-02-13 02:16:29,585 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:16:31,068 : Computing embedding for train
2019-02-13 02:16:38,911 : Computed train embeddings
2019-02-13 02:16:38,911 : Computing embedding for dev
2019-02-13 02:16:39,898 : Computed dev embeddings
2019-02-13 02:16:39,898 : Computing embedding for test
2019-02-13 02:16:48,422 : Computed test embeddings
2019-02-13 02:17:23,781 : Dev : Pearson 0.7877903470144794
2019-02-13 02:17:23,781 : Test : Pearson 0.7830765028664892 Spearman 0.7235250441660269 MSE 0.39576590142065426                        for SICK Relatedness

2019-02-13 02:17:23,782 : 

***** Transfer task : STSBenchmark*****


2019-02-13 02:17:23,887 : loading BERT mode bert-base-uncased
2019-02-13 02:17:23,888 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:17:23,907 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:17:23,908 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdjzipnwg
2019-02-13 02:17:26,332 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:17:27,816 : Computing embedding for train
2019-02-13 02:17:42,395 : Computed train embeddings
2019-02-13 02:17:42,395 : Computing embedding for dev
2019-02-13 02:17:48,364 : Computed dev embeddings
2019-02-13 02:17:48,364 : Computing embedding for test
2019-02-13 02:17:53,365 : Computed test embeddings
2019-02-13 02:19:05,849 : Dev : Pearson 0.6383269239371329
2019-02-13 02:19:05,849 : Test : Pearson 0.6075716365189994 Spearman 0.6043524616674351 MSE 1.5038752906319992                        for SICK Relatedness

2019-02-13 02:19:05,849 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 02:19:06,104 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 02:19:06,114 : loading BERT mode bert-base-uncased
2019-02-13 02:19:06,114 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:19:06,209 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:19:06,209 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuxy3z96e
2019-02-13 02:19:08,643 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:19:10,124 : Computing embeddings for train/dev/test
2019-02-13 02:21:31,077 : Computed embeddings
2019-02-13 02:21:31,077 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:23:04,507 : [('reg:1e-05', 67.5), ('reg:0.0001', 69.16), ('reg:0.001', 63.35), ('reg:0.01', 59.72)]
2019-02-13 02:23:04,507 : Validation : best param found is reg = 0.0001 with score             69.16
2019-02-13 02:23:04,507 : Evaluating...
2019-02-13 02:23:21,310 : 
Dev acc : 69.2 Test acc : 68.5 for LENGTH classification

2019-02-13 02:23:21,318 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 02:23:21,802 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 02:23:21,846 : loading BERT mode bert-base-uncased
2019-02-13 02:23:21,846 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:23:21,878 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:23:21,878 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzgzky134
2019-02-13 02:23:24,309 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:23:25,722 : Computing embeddings for train/dev/test
2019-02-13 02:25:45,877 : Computed embeddings
2019-02-13 02:25:45,878 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:27:19,770 : [('reg:1e-05', 12.59), ('reg:0.0001', 6.39), ('reg:0.001', 0.37), ('reg:0.01', 0.14)]
2019-02-13 02:27:19,770 : Validation : best param found is reg = 1e-05 with score             12.59
2019-02-13 02:27:19,770 : Evaluating...
2019-02-13 02:27:48,633 : 
Dev acc : 12.6 Test acc : 13.0 for WORDCONTENT classification

2019-02-13 02:27:48,642 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 02:27:48,989 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 02:27:49,053 : loading BERT mode bert-base-uncased
2019-02-13 02:27:49,054 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:27:49,149 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:27:49,150 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpls3pv4f5
2019-02-13 02:27:51,579 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:27:53,064 : Computing embeddings for train/dev/test
2019-02-13 02:30:30,815 : Computed embeddings
2019-02-13 02:30:30,816 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:31:48,702 : [('reg:1e-05', 30.25), ('reg:0.0001', 27.37), ('reg:0.001', 30.72), ('reg:0.01', 24.6)]
2019-02-13 02:31:48,702 : Validation : best param found is reg = 0.001 with score             30.72
2019-02-13 02:31:48,702 : Evaluating...
2019-02-13 02:32:05,935 : 
Dev acc : 30.7 Test acc : 30.2 for DEPTH classification

2019-02-13 02:32:05,942 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 02:32:06,501 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 02:32:06,563 : loading BERT mode bert-base-uncased
2019-02-13 02:32:06,563 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:32:06,589 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:32:06,590 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo3gscco9
2019-02-13 02:32:09,021 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:32:10,491 : Computing embeddings for train/dev/test
2019-02-13 02:34:26,304 : Computed embeddings
2019-02-13 02:34:26,304 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:35:55,573 : [('reg:1e-05', 53.8), ('reg:0.0001', 62.93), ('reg:0.001', 59.54), ('reg:0.01', 38.73)]
2019-02-13 02:35:55,573 : Validation : best param found is reg = 0.0001 with score             62.93
2019-02-13 02:35:55,573 : Evaluating...
2019-02-13 02:36:18,819 : 
Dev acc : 62.9 Test acc : 63.0 for TOPCONSTITUENTS classification

2019-02-13 02:36:18,827 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 02:36:19,355 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 02:36:19,421 : loading BERT mode bert-base-uncased
2019-02-13 02:36:19,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:36:19,451 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:36:19,451 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2imoeskw
2019-02-13 02:36:21,888 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:36:23,368 : Computing embeddings for train/dev/test
2019-02-13 02:39:05,210 : Computed embeddings
2019-02-13 02:39:05,210 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:40:22,247 : [('reg:1e-05', 88.03), ('reg:0.0001', 88.11), ('reg:0.001', 88.06), ('reg:0.01', 87.55)]
2019-02-13 02:40:22,247 : Validation : best param found is reg = 0.0001 with score             88.11
2019-02-13 02:40:22,247 : Evaluating...
2019-02-13 02:40:46,736 : 
Dev acc : 88.1 Test acc : 87.4 for BIGRAMSHIFT classification

2019-02-13 02:40:46,744 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 02:40:47,147 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 02:40:47,212 : loading BERT mode bert-base-uncased
2019-02-13 02:40:47,213 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:40:47,242 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:40:47,242 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkmzwnjyi
2019-02-13 02:40:49,668 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:40:51,140 : Computing embeddings for train/dev/test
2019-02-13 02:43:21,732 : Computed embeddings
2019-02-13 02:43:21,732 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:44:18,729 : [('reg:1e-05', 88.19), ('reg:0.0001', 88.2), ('reg:0.001', 88.27), ('reg:0.01', 87.9)]
2019-02-13 02:44:18,729 : Validation : best param found is reg = 0.001 with score             88.27
2019-02-13 02:44:18,730 : Evaluating...
2019-02-13 02:44:33,644 : 
Dev acc : 88.3 Test acc : 86.6 for TENSE classification

2019-02-13 02:44:33,651 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 02:44:34,059 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 02:44:34,122 : loading BERT mode bert-base-uncased
2019-02-13 02:44:34,122 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:44:34,239 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:44:34,240 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsgmd3o11
2019-02-13 02:44:36,689 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:44:38,162 : Computing embeddings for train/dev/test
2019-02-13 02:47:13,265 : Computed embeddings
2019-02-13 02:47:13,266 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:48:21,084 : [('reg:1e-05', 80.07), ('reg:0.0001', 80.47), ('reg:0.001', 81.86), ('reg:0.01', 81.37)]
2019-02-13 02:48:21,084 : Validation : best param found is reg = 0.001 with score             81.86
2019-02-13 02:48:21,084 : Evaluating...
2019-02-13 02:48:44,267 : 
Dev acc : 81.9 Test acc : 80.8 for SUBJNUMBER classification

2019-02-13 02:48:44,274 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 02:48:44,675 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 02:48:44,741 : loading BERT mode bert-base-uncased
2019-02-13 02:48:44,741 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:48:44,861 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:48:44,861 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp85z0603j
2019-02-13 02:48:47,287 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:48:48,840 : Computing embeddings for train/dev/test
2019-02-13 02:51:22,092 : Computed embeddings
2019-02-13 02:51:22,093 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:53:11,666 : [('reg:1e-05', 76.05), ('reg:0.0001', 76.17), ('reg:0.001', 78.76), ('reg:0.01', 75.56)]
2019-02-13 02:53:11,666 : Validation : best param found is reg = 0.001 with score             78.76
2019-02-13 02:53:11,666 : Evaluating...
2019-02-13 02:53:33,220 : 
Dev acc : 78.8 Test acc : 79.9 for OBJNUMBER classification

2019-02-13 02:53:33,228 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 02:53:33,799 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 02:53:33,869 : loading BERT mode bert-base-uncased
2019-02-13 02:53:33,869 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:53:33,898 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:53:33,899 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvq71xsk_
2019-02-13 02:53:36,335 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:53:37,759 : Computing embeddings for train/dev/test
2019-02-13 02:56:06,030 : Computed embeddings
2019-02-13 02:56:06,030 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:58:04,075 : [('reg:1e-05', 60.95), ('reg:0.0001', 60.95), ('reg:0.001', 60.33), ('reg:0.01', 63.21)]
2019-02-13 02:58:04,076 : Validation : best param found is reg = 0.01 with score             63.21
2019-02-13 02:58:04,076 : Evaluating...
2019-02-13 02:58:28,130 : 
Dev acc : 63.2 Test acc : 62.8 for ODDMANOUT classification

2019-02-13 02:58:28,138 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 02:58:28,740 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 02:58:28,816 : loading BERT mode bert-base-uncased
2019-02-13 02:58:28,816 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:58:28,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:58:28,846 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9k36gj6x
2019-02-13 02:58:31,272 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:58:32,765 : Computing embeddings for train/dev/test
2019-02-13 03:01:06,129 : Computed embeddings
2019-02-13 03:01:06,129 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:02:41,222 : [('reg:1e-05', 69.04), ('reg:0.0001', 68.95), ('reg:0.001', 59.81), ('reg:0.01', 53.3)]
2019-02-13 03:02:41,222 : Validation : best param found is reg = 1e-05 with score             69.04
2019-02-13 03:02:41,222 : Evaluating...
2019-02-13 03:03:01,149 : 
Dev acc : 69.0 Test acc : 68.9 for COORDINATIONINVERSION classification

2019-02-13 03:03:01,157 : {'STS12': {'MSRpar': {'pearson': (0.29879998470238733, 6.214876824522798e-17), 'spearman': SpearmanrResult(correlation=0.32719096674972736, pvalue=3.5807684497938974e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5665534721064202, 6.820056168243729e-65), 'spearman': SpearmanrResult(correlation=0.5778848349208165, pvalue=4.933960571356812e-68), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4743052478319482, 4.0026360924153866e-27), 'spearman': SpearmanrResult(correlation=0.5939576528059204, pvalue=4.176022687440376e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5380492578759628, 1.6529663523151526e-57), 'spearman': SpearmanrResult(correlation=0.5650143642135739, pvalue=1.782508584390095e-64), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.582627666634786, 1.217351035089455e-37), 'spearman': SpearmanrResult(correlation=0.4820683055276228, pvalue=1.3063788175602618e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.492067125830301, 'wmean': 0.48350276182616525}, 'spearman': {'mean': 0.5092232248435321, 'wmean': 0.5043563194840822}}}, 'STS13': {'FNWN': {'pearson': (0.15441431832086935, 0.03387941899864418), 'spearman': SpearmanrResult(correlation=0.17558606190746254, pvalue=0.0156618709554831), 'nsamples': 189}, 'headlines': {'pearson': (0.6259600844155241, 7.830901797830514e-83), 'spearman': SpearmanrResult(correlation=0.6193499514457952, pvalue=1.2080686392560593e-80), 'nsamples': 750}, 'OnWN': {'pearson': (0.44851826203656, 4.068566933379204e-29), 'spearman': SpearmanrResult(correlation=0.44403185337995793, pvalue=1.662212620222331e-28), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.40963088825765115, 'wmean': 0.5001820763178649}, 'spearman': {'mean': 0.41298928891107184, 'wmean': 0.49786673268734216}}}, 'STS14': {'deft-forum': {'pearson': (0.3460675038622933, 4.1727087283636444e-14), 'spearman': SpearmanrResult(correlation=0.3501926542356272, pvalue=1.98381418415068e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7272921992354662, 1.2243945460274939e-50), 'spearman': SpearmanrResult(correlation=0.7059289898634915, pvalue=1.4990718655110774e-46), 'nsamples': 300}, 'headlines': {'pearson': (0.5715095514994751, 2.986693318557726e-66), 'spearman': SpearmanrResult(correlation=0.5434667098325954, pvalue=7.395629539864906e-59), 'nsamples': 750}, 'images': {'pearson': (0.5351771711334926, 8.388842171643619e-57), 'spearman': SpearmanrResult(correlation=0.5248000785170979, pvalue=2.6105097057299523e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.5878804333560607, 6.591910579909776e-71), 'spearman': SpearmanrResult(correlation=0.619603212073914, pvalue=9.98187333149154e-81), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5849523567115926, 4.6915330355169724e-70), 'spearman': SpearmanrResult(correlation=0.5512942717721375, pvalue=7.506103734515621e-61), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5588132026330634, 'wmean': 0.5556153789424366}, 'spearman': {'mean': 0.5492143193824772, 'wmean': 0.5463302921365036}}}, 'STS15': {'answers-forums': {'pearson': (0.49564245932164214, 1.211309727341745e-24), 'spearman': SpearmanrResult(correlation=0.47763997892936205, pvalue=9.094984043156116e-23), 'nsamples': 375}, 'answers-students': {'pearson': (0.6636850349988911, 2.1174872223170806e-96), 'spearman': SpearmanrResult(correlation=0.6719748161725907, pvalue=1.1857643625088108e-99), 'nsamples': 750}, 'belief': {'pearson': (0.541739792985768, 5.5047946156417556e-30), 'spearman': SpearmanrResult(correlation=0.5800216594320853, pvalue=4.2910915535074715e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.6166133126591805, 9.393801802802987e-80), 'spearman': SpearmanrResult(correlation=0.6241700963309899, pvalue=3.101571238278989e-82), 'nsamples': 750}, 'images': {'pearson': (0.674719547808599, 9.41566633220472e-101), 'spearman': SpearmanrResult(correlation=0.683943980747165, pvalue=1.5398029626179588e-104), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.598480029554816, 'wmean': 0.6184272554050938}, 'spearman': {'mean': 0.6075501063224386, 'wmean': 0.6272299281078674}}}, 'STS16': {'answer-answer': {'pearson': (0.5110746951480559, 2.643622500828177e-18), 'spearman': SpearmanrResult(correlation=0.5205900980662171, pvalue=4.813296738592338e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.627517224112993, 1.1479079226079992e-28), 'spearman': SpearmanrResult(correlation=0.6322003516780754, pvalue=3.405928206613921e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6815027029036544, 9.342357318811391e-33), 'spearman': SpearmanrResult(correlation=0.6888458672715829, pvalue=1.063615775717041e-33), 'nsamples': 230}, 'postediting': {'pearson': (0.7622612939709552, 1.284693677134475e-47), 'spearman': SpearmanrResult(correlation=0.8116750175160692, pvalue=1.953712106434047e-58), 'nsamples': 244}, 'question-question': {'pearson': (0.2197035648413153, 0.0013919598502565458), 'spearman': SpearmanrResult(correlation=0.23549262804069673, pvalue=0.000598130614532572), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5604118961953948, 'wmean': 0.5689040335753204}, 'spearman': {'mean': 0.5777607925145283, 'wmean': 0.5862976268832636}}}, 'MR': {'devacc': 66.62, 'acc': 68.34, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.91, 'acc': 65.91, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.71, 'acc': 87.15, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.61, 'acc': 92.68, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.77, 'acc': 81.16, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.6, 'acc': 39.95, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 69.99, 'acc': 86.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.74, 'acc': 68.41, 'f1': 73.81, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 76.17, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7877903470144794, 'pearson': 0.7830765028664892, 'spearman': 0.7235250441660269, 'mse': 0.39576590142065426, 'yhat': array([2.01878814, 3.74509233, 2.76615837, ..., 3.24776367, 4.16729955,
       3.64642097]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6383269239371329, 'pearson': 0.6075716365189994, 'spearman': 0.6043524616674351, 'mse': 1.5038752906319992, 'yhat': array([1.06439409, 1.64141715, 1.64131424, ..., 3.55992977, 3.955018  ,
       4.14952057]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 69.16, 'acc': 68.52, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 12.59, 'acc': 12.98, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.72, 'acc': 30.16, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 62.93, 'acc': 63.03, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.11, 'acc': 87.43, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.27, 'acc': 86.64, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.86, 'acc': 80.75, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.76, 'acc': 79.93, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.21, 'acc': 62.84, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.04, 'acc': 68.86, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 03:03:01,158 : ********************************************************************************
2019-02-13 03:03:01,158 : ********************************************************************************
2019-02-13 03:03:01,158 : ********************************************************************************
2019-02-13 03:03:01,158 : layer 9
2019-02-13 03:03:01,158 : ********************************************************************************
2019-02-13 03:03:01,158 : ********************************************************************************
2019-02-13 03:03:01,158 : ********************************************************************************
2019-02-13 03:03:01,241 : ***** Transfer task : STS12 *****


2019-02-13 03:03:01,253 : loading BERT mode bert-base-uncased
2019-02-13 03:03:01,253 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:03:01,271 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:03:01,271 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpja50ezgc
2019-02-13 03:03:03,702 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:03:07,678 : MSRpar : pearson = 0.2979, spearman = 0.3253
2019-02-13 03:03:09,359 : MSRvid : pearson = 0.5219, spearman = 0.5373
2019-02-13 03:03:10,644 : SMTeuroparl : pearson = 0.4636, spearman = 0.5761
2019-02-13 03:03:12,891 : surprise.OnWN : pearson = 0.5090, spearman = 0.5390
2019-02-13 03:03:14,162 : surprise.SMTnews : pearson = 0.5896, spearman = 0.4906
2019-02-13 03:03:14,163 : ALL (weighted average) : Pearson = 0.4648,             Spearman = 0.4863
2019-02-13 03:03:14,163 : ALL (average) : Pearson = 0.4764,             Spearman = 0.4937

2019-02-13 03:03:14,163 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 03:03:14,173 : loading BERT mode bert-base-uncased
2019-02-13 03:03:14,173 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:03:14,190 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:03:14,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplipiv74j
2019-02-13 03:03:16,616 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:03:18,849 : FNWN : pearson = 0.1677, spearman = 0.1828
2019-02-13 03:03:20,617 : headlines : pearson = 0.6147, spearman = 0.6040
2019-02-13 03:03:21,990 : OnWN : pearson = 0.4403, spearman = 0.4337
2019-02-13 03:03:21,991 : ALL (weighted average) : Pearson = 0.4932,             Spearman = 0.4873
2019-02-13 03:03:21,991 : ALL (average) : Pearson = 0.4076,             Spearman = 0.4068

2019-02-13 03:03:21,991 : ***** Transfer task : STS14 *****


2019-02-13 03:03:22,007 : loading BERT mode bert-base-uncased
2019-02-13 03:03:22,007 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:03:22,024 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:03:22,024 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpohscqh03
2019-02-13 03:03:24,453 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:03:27,315 : deft-forum : pearson = 0.3193, spearman = 0.3244
2019-02-13 03:03:28,644 : deft-news : pearson = 0.7171, spearman = 0.6943
2019-02-13 03:03:30,561 : headlines : pearson = 0.5549, spearman = 0.5291
2019-02-13 03:03:32,076 : images : pearson = 0.4585, spearman = 0.4626
2019-02-13 03:03:33,585 : OnWN : pearson = 0.5856, spearman = 0.6133
2019-02-13 03:03:35,413 : tweet-news : pearson = 0.5823, spearman = 0.5397
2019-02-13 03:03:35,413 : ALL (weighted average) : Pearson = 0.5319,             Spearman = 0.5234
2019-02-13 03:03:35,413 : ALL (average) : Pearson = 0.5363,             Spearman = 0.5272

2019-02-13 03:03:35,413 : ***** Transfer task : STS15 *****


2019-02-13 03:03:35,447 : loading BERT mode bert-base-uncased
2019-02-13 03:03:35,447 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:03:35,464 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:03:35,465 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpulevp4wg
2019-02-13 03:03:37,904 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:03:40,492 : answers-forums : pearson = 0.5003, spearman = 0.4787
2019-02-13 03:03:41,986 : answers-students : pearson = 0.6544, spearman = 0.6623
2019-02-13 03:03:43,146 : belief : pearson = 0.5441, spearman = 0.5796
2019-02-13 03:03:44,704 : headlines : pearson = 0.6068, spearman = 0.6147
2019-02-13 03:03:46,220 : images : pearson = 0.6332, spearman = 0.6419
2019-02-13 03:03:46,220 : ALL (weighted average) : Pearson = 0.6041,             Spearman = 0.6120
2019-02-13 03:03:46,220 : ALL (average) : Pearson = 0.5877,             Spearman = 0.5954

2019-02-13 03:03:46,221 : ***** Transfer task : STS16 *****


2019-02-13 03:03:46,258 : loading BERT mode bert-base-uncased
2019-02-13 03:03:46,259 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:03:46,277 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:03:46,277 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp19et90nw
2019-02-13 03:03:48,722 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:03:50,675 : answer-answer : pearson = 0.4932, spearman = 0.4916
2019-02-13 03:03:51,081 : headlines : pearson = 0.6283, spearman = 0.6325
2019-02-13 03:03:51,569 : plagiarism : pearson = 0.6631, spearman = 0.6751
2019-02-13 03:03:52,303 : postediting : pearson = 0.7541, spearman = 0.7930
2019-02-13 03:03:52,669 : question-question : pearson = 0.2320, spearman = 0.2528
2019-02-13 03:03:52,669 : ALL (weighted average) : Pearson = 0.5621,             Spearman = 0.5767
2019-02-13 03:03:52,669 : ALL (average) : Pearson = 0.5541,             Spearman = 0.5690

2019-02-13 03:03:52,669 : ***** Transfer task : MR *****


2019-02-13 03:03:52,725 : loading BERT mode bert-base-uncased
2019-02-13 03:03:52,725 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:03:52,747 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:03:52,747 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjmh_m3n2
2019-02-13 03:03:55,190 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:03:56,663 : Generating sentence embeddings
2019-02-13 03:04:17,242 : Generated sentence embeddings
2019-02-13 03:04:17,242 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:04:45,341 : Best param found at split 1: l2reg = 1e-05                 with score 75.52
2019-02-13 03:05:25,654 : Best param found at split 2: l2reg = 1e-05                 with score 72.42
2019-02-13 03:06:16,950 : Best param found at split 3: l2reg = 0.001                 with score 74.03
2019-02-13 03:06:45,509 : Best param found at split 4: l2reg = 0.0001                 with score 73.51
2019-02-13 03:07:18,976 : Best param found at split 5: l2reg = 1e-05                 with score 72.53
2019-02-13 03:07:20,794 : Dev acc : 73.6 Test acc : 73.01

2019-02-13 03:07:20,795 : ***** Transfer task : CR *****


2019-02-13 03:07:20,803 : loading BERT mode bert-base-uncased
2019-02-13 03:07:20,803 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:07:20,826 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:07:20,826 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq6bo_9bz
2019-02-13 03:07:23,336 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:07:24,872 : Generating sentence embeddings
2019-02-13 03:07:30,600 : Generated sentence embeddings
2019-02-13 03:07:30,600 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:07:38,332 : Best param found at split 1: l2reg = 0.001                 with score 74.49
2019-02-13 03:07:43,177 : Best param found at split 2: l2reg = 0.0001                 with score 80.29
2019-02-13 03:07:48,476 : Best param found at split 3: l2reg = 0.0001                 with score 80.82
2019-02-13 03:07:54,288 : Best param found at split 4: l2reg = 0.001                 with score 82.03
2019-02-13 03:08:03,061 : Best param found at split 5: l2reg = 0.01                 with score 78.68
2019-02-13 03:08:03,567 : Dev acc : 79.26 Test acc : 73.25

2019-02-13 03:08:03,568 : ***** Transfer task : MPQA *****


2019-02-13 03:08:03,573 : loading BERT mode bert-base-uncased
2019-02-13 03:08:03,573 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:08:03,593 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:08:03,593 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7v92t9lb
2019-02-13 03:08:06,018 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:08:07,543 : Generating sentence embeddings
2019-02-13 03:08:18,078 : Generated sentence embeddings
2019-02-13 03:08:18,078 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:08:46,349 : Best param found at split 1: l2reg = 1e-05                 with score 86.8
2019-02-13 03:09:24,419 : Best param found at split 2: l2reg = 1e-05                 with score 88.31
2019-02-13 03:10:09,244 : Best param found at split 3: l2reg = 0.0001                 with score 87.53
2019-02-13 03:10:35,301 : Best param found at split 4: l2reg = 0.001                 with score 87.53
2019-02-13 03:11:02,439 : Best param found at split 5: l2reg = 0.0001                 with score 87.76
2019-02-13 03:11:03,688 : Dev acc : 87.59 Test acc : 87.68

2019-02-13 03:11:03,689 : ***** Transfer task : SUBJ *****


2019-02-13 03:11:03,704 : loading BERT mode bert-base-uncased
2019-02-13 03:11:03,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:11:03,762 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:11:03,762 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi9dm64xc
2019-02-13 03:11:06,188 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:11:07,655 : Generating sentence embeddings
2019-02-13 03:11:27,762 : Generated sentence embeddings
2019-02-13 03:11:27,762 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:11:44,376 : Best param found at split 1: l2reg = 1e-05                 with score 93.45
2019-02-13 03:12:08,216 : Best param found at split 2: l2reg = 1e-05                 with score 93.22
2019-02-13 03:12:37,726 : Best param found at split 3: l2reg = 1e-05                 with score 93.21
2019-02-13 03:13:10,580 : Best param found at split 4: l2reg = 1e-05                 with score 93.34
2019-02-13 03:13:53,654 : Best param found at split 5: l2reg = 0.001                 with score 93.14
2019-02-13 03:13:55,697 : Dev acc : 93.27 Test acc : 92.91

2019-02-13 03:13:55,698 : ***** Transfer task : SST Binary classification *****


2019-02-13 03:13:55,835 : loading BERT mode bert-base-uncased
2019-02-13 03:13:55,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:13:55,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:13:55,858 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps_0rqnoq
2019-02-13 03:13:58,286 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:13:59,847 : Computing embedding for train
2019-02-13 03:15:41,575 : Computed train embeddings
2019-02-13 03:15:41,575 : Computing embedding for dev
2019-02-13 03:15:43,202 : Computed dev embeddings
2019-02-13 03:15:43,202 : Computing embedding for test
2019-02-13 03:15:46,899 : Computed test embeddings
2019-02-13 03:15:46,899 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:16:12,375 : [('reg:1e-05', 83.83), ('reg:0.0001', 83.94), ('reg:0.001', 84.06), ('reg:0.01', 76.72)]
2019-02-13 03:16:12,375 : Validation : best param found is reg = 0.001 with score             84.06
2019-02-13 03:16:12,375 : Evaluating...
2019-02-13 03:16:19,879 : 
Dev acc : 84.06 Test acc : 83.2 for             SST Binary classification

2019-02-13 03:16:19,884 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 03:16:19,940 : loading BERT mode bert-base-uncased
2019-02-13 03:16:19,941 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:16:19,962 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:16:19,963 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph9nkyi1y
2019-02-13 03:16:22,445 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:16:23,956 : Computing embedding for train
2019-02-13 03:16:37,507 : Computed train embeddings
2019-02-13 03:16:37,507 : Computing embedding for dev
2019-02-13 03:16:39,683 : Computed dev embeddings
2019-02-13 03:16:39,684 : Computing embedding for test
2019-02-13 03:16:43,420 : Computed test embeddings
2019-02-13 03:16:43,420 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:16:46,526 : [('reg:1e-05', 37.33), ('reg:0.0001', 42.51), ('reg:0.001', 38.87), ('reg:0.01', 32.97)]
2019-02-13 03:16:46,527 : Validation : best param found is reg = 0.0001 with score             42.51
2019-02-13 03:16:46,527 : Evaluating...
2019-02-13 03:16:47,388 : 
Dev acc : 42.51 Test acc : 43.26 for             SST Fine-Grained classification

2019-02-13 03:16:47,388 : ***** Transfer task : TREC *****


2019-02-13 03:16:47,401 : loading BERT mode bert-base-uncased
2019-02-13 03:16:47,401 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:16:47,422 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:16:47,422 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpysn9k1zx
2019-02-13 03:16:49,854 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:16:56,078 : Computed train embeddings
2019-02-13 03:16:56,773 : Computed test embeddings
2019-02-13 03:16:56,773 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 03:17:16,757 : [('reg:1e-05', 74.42), ('reg:0.0001', 68.02), ('reg:0.001', 72.63), ('reg:0.01', 63.14)]
2019-02-13 03:17:16,757 : Cross-validation : best param found is reg = 1e-05             with score 74.42
2019-02-13 03:17:16,758 : Evaluating...
2019-02-13 03:17:17,630 : 
Dev acc : 74.42 Test acc : 89.2             for TREC

2019-02-13 03:17:17,630 : ***** Transfer task : MRPC *****


2019-02-13 03:17:17,650 : loading BERT mode bert-base-uncased
2019-02-13 03:17:17,650 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:17:17,673 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:17:17,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwi32_ga4
2019-02-13 03:17:20,109 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:17:21,590 : Computing embedding for train
2019-02-13 03:17:35,367 : Computed train embeddings
2019-02-13 03:17:35,367 : Computing embedding for test
2019-02-13 03:17:41,431 : Computed test embeddings
2019-02-13 03:17:41,447 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 03:17:52,956 : [('reg:1e-05', 71.74), ('reg:0.0001', 72.86), ('reg:0.001', 72.28), ('reg:0.01', 72.84)]
2019-02-13 03:17:52,957 : Cross-validation : best param found is reg = 0.0001             with score 72.86
2019-02-13 03:17:52,957 : Evaluating...
2019-02-13 03:17:53,570 : Dev acc : 72.86 Test acc 69.91; Test F1 75.06 for MRPC.

2019-02-13 03:17:53,570 : ***** Transfer task : SICK-Entailment*****


2019-02-13 03:17:53,593 : loading BERT mode bert-base-uncased
2019-02-13 03:17:53,594 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:17:53,650 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:17:53,650 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeyvge6lr
2019-02-13 03:17:56,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:17:57,535 : Computing embedding for train
2019-02-13 03:18:08,903 : Computed train embeddings
2019-02-13 03:18:08,903 : Computing embedding for dev
2019-02-13 03:18:10,336 : Computed dev embeddings
2019-02-13 03:18:10,336 : Computing embedding for test
2019-02-13 03:18:23,357 : Computed test embeddings
2019-02-13 03:18:23,385 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:18:26,312 : [('reg:1e-05', 76.4), ('reg:0.0001', 68.4), ('reg:0.001', 74.6), ('reg:0.01', 74.6)]
2019-02-13 03:18:26,312 : Validation : best param found is reg = 1e-05 with score             76.4
2019-02-13 03:18:26,312 : Evaluating...
2019-02-13 03:18:27,052 : 
Dev acc : 76.4 Test acc : 73.03 for                        SICK entailment

2019-02-13 03:18:27,053 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 03:18:27,079 : loading BERT mode bert-base-uncased
2019-02-13 03:18:27,080 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:18:27,100 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:18:27,100 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3tkptpjg
2019-02-13 03:18:29,536 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:18:31,011 : Computing embedding for train
2019-02-13 03:18:43,926 : Computed train embeddings
2019-02-13 03:18:43,926 : Computing embedding for dev
2019-02-13 03:18:45,550 : Computed dev embeddings
2019-02-13 03:18:45,550 : Computing embedding for test
2019-02-13 03:19:00,473 : Computed test embeddings
2019-02-13 03:19:38,600 : Dev : Pearson 0.7849114722360065
2019-02-13 03:19:38,600 : Test : Pearson 0.7791974566428523 Spearman 0.7155194844797105 MSE 0.40100860132932276                        for SICK Relatedness

2019-02-13 03:19:38,601 : 

***** Transfer task : STSBenchmark*****


2019-02-13 03:19:38,640 : loading BERT mode bert-base-uncased
2019-02-13 03:19:38,640 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:19:38,668 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:19:38,668 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt8rmckf7
2019-02-13 03:19:41,130 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:19:42,566 : Computing embedding for train
2019-02-13 03:19:56,607 : Computed train embeddings
2019-02-13 03:19:56,608 : Computing embedding for dev
2019-02-13 03:19:59,993 : Computed dev embeddings
2019-02-13 03:19:59,993 : Computing embedding for test
2019-02-13 03:20:02,836 : Computed test embeddings
2019-02-13 03:20:44,192 : Dev : Pearson 0.6350037223493994
2019-02-13 03:20:44,192 : Test : Pearson 0.625147973339803 Spearman 0.6210754290394137 MSE 1.4506891891495912                        for SICK Relatedness

2019-02-13 03:20:44,193 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 03:20:44,445 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 03:20:44,456 : loading BERT mode bert-base-uncased
2019-02-13 03:20:44,456 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:20:44,552 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:20:44,552 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp378iwdhc
2019-02-13 03:20:46,980 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:20:48,454 : Computing embeddings for train/dev/test
2019-02-13 03:23:42,003 : Computed embeddings
2019-02-13 03:23:42,003 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:24:38,611 : [('reg:1e-05', 64.73), ('reg:0.0001', 58.14), ('reg:0.001', 53.06), ('reg:0.01', 48.69)]
2019-02-13 03:24:38,612 : Validation : best param found is reg = 1e-05 with score             64.73
2019-02-13 03:24:38,612 : Evaluating...
2019-02-13 03:24:59,213 : 
Dev acc : 64.7 Test acc : 64.6 for LENGTH classification

2019-02-13 03:24:59,221 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 03:24:59,698 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 03:24:59,742 : loading BERT mode bert-base-uncased
2019-02-13 03:24:59,743 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:24:59,773 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:24:59,773 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp8a603xd
2019-02-13 03:25:02,204 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:25:03,676 : Computing embeddings for train/dev/test
2019-02-13 03:28:08,505 : Computed embeddings
2019-02-13 03:28:08,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:29:19,046 : [('reg:1e-05', 12.36), ('reg:0.0001', 5.18), ('reg:0.001', 0.89), ('reg:0.01', 0.1)]
2019-02-13 03:29:19,047 : Validation : best param found is reg = 1e-05 with score             12.36
2019-02-13 03:29:19,047 : Evaluating...
2019-02-13 03:29:39,324 : 
Dev acc : 12.4 Test acc : 12.8 for WORDCONTENT classification

2019-02-13 03:29:39,332 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 03:29:39,854 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 03:29:39,919 : loading BERT mode bert-base-uncased
2019-02-13 03:29:39,919 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:29:39,944 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:29:39,945 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqddy3ya6
2019-02-13 03:29:42,380 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:29:43,975 : Computing embeddings for train/dev/test
2019-02-13 03:32:37,364 : Computed embeddings
2019-02-13 03:32:37,364 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:33:35,466 : [('reg:1e-05', 28.78), ('reg:0.0001', 29.16), ('reg:0.001', 32.28), ('reg:0.01', 26.92)]
2019-02-13 03:33:35,467 : Validation : best param found is reg = 0.001 with score             32.28
2019-02-13 03:33:35,467 : Evaluating...
2019-02-13 03:33:50,897 : 
Dev acc : 32.3 Test acc : 30.9 for DEPTH classification

2019-02-13 03:33:50,904 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 03:33:51,451 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 03:33:51,513 : loading BERT mode bert-base-uncased
2019-02-13 03:33:51,513 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:33:51,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:33:51,539 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzjt9saxl
2019-02-13 03:33:53,964 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:33:55,436 : Computing embeddings for train/dev/test
2019-02-13 03:37:18,764 : Computed embeddings
2019-02-13 03:37:18,765 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:38:04,449 : [('reg:1e-05', 56.63), ('reg:0.0001', 55.01), ('reg:0.001', 54.09), ('reg:0.01', 43.84)]
2019-02-13 03:38:04,449 : Validation : best param found is reg = 1e-05 with score             56.63
2019-02-13 03:38:04,449 : Evaluating...
2019-02-13 03:38:13,622 : 
Dev acc : 56.6 Test acc : 56.9 for TOPCONSTITUENTS classification

2019-02-13 03:38:13,629 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 03:38:14,012 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 03:38:14,078 : loading BERT mode bert-base-uncased
2019-02-13 03:38:14,078 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:38:14,109 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:38:14,110 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbr_m6hrt
2019-02-13 03:38:16,549 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:38:17,961 : Computing embeddings for train/dev/test
2019-02-13 03:40:48,286 : Computed embeddings
2019-02-13 03:40:48,286 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:41:59,992 : [('reg:1e-05', 88.29), ('reg:0.0001', 88.33), ('reg:0.001', 88.31), ('reg:0.01', 87.86)]
2019-02-13 03:41:59,992 : Validation : best param found is reg = 0.0001 with score             88.33
2019-02-13 03:41:59,992 : Evaluating...
2019-02-13 03:42:12,493 : 
Dev acc : 88.3 Test acc : 87.8 for BIGRAMSHIFT classification

2019-02-13 03:42:12,501 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 03:42:12,893 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 03:42:12,959 : loading BERT mode bert-base-uncased
2019-02-13 03:42:12,959 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:42:13,080 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:42:13,080 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpifs3b5rr
2019-02-13 03:42:15,509 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:42:16,959 : Computing embeddings for train/dev/test
2019-02-13 03:44:37,667 : Computed embeddings
2019-02-13 03:44:37,667 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:45:46,974 : [('reg:1e-05', 89.48), ('reg:0.0001', 89.46), ('reg:0.001', 89.58), ('reg:0.01', 89.58)]
2019-02-13 03:45:46,974 : Validation : best param found is reg = 0.001 with score             89.58
2019-02-13 03:45:46,975 : Evaluating...
2019-02-13 03:45:55,455 : 
Dev acc : 89.6 Test acc : 88.0 for TENSE classification

2019-02-13 03:45:55,463 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 03:45:56,053 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 03:45:56,117 : loading BERT mode bert-base-uncased
2019-02-13 03:45:56,117 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:45:56,144 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:45:56,145 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp836gw6mt
2019-02-13 03:45:58,583 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:46:00,003 : Computing embeddings for train/dev/test
2019-02-13 03:48:17,459 : Computed embeddings
2019-02-13 03:48:17,459 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:49:50,669 : [('reg:1e-05', 82.4), ('reg:0.0001', 82.46), ('reg:0.001', 81.86), ('reg:0.01', 81.2)]
2019-02-13 03:49:50,669 : Validation : best param found is reg = 0.0001 with score             82.46
2019-02-13 03:49:50,669 : Evaluating...
2019-02-13 03:50:02,265 : 
Dev acc : 82.5 Test acc : 81.5 for SUBJNUMBER classification

2019-02-13 03:50:02,272 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 03:50:02,674 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 03:50:02,740 : loading BERT mode bert-base-uncased
2019-02-13 03:50:02,741 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:50:02,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:50:02,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpghyn6x3q
2019-02-13 03:50:05,290 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:50:06,701 : Computing embeddings for train/dev/test
2019-02-13 03:52:48,094 : Computed embeddings
2019-02-13 03:52:48,094 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:54:10,412 : [('reg:1e-05', 76.91), ('reg:0.0001', 76.99), ('reg:0.001', 76.91), ('reg:0.01', 76.89)]
2019-02-13 03:54:10,412 : Validation : best param found is reg = 0.0001 with score             76.99
2019-02-13 03:54:10,412 : Evaluating...
2019-02-13 03:54:26,975 : 
Dev acc : 77.0 Test acc : 78.2 for OBJNUMBER classification

2019-02-13 03:54:26,983 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 03:54:27,561 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 03:54:27,630 : loading BERT mode bert-base-uncased
2019-02-13 03:54:27,631 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:54:27,660 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:54:27,660 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp888q8sm6
2019-02-13 03:54:30,093 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:54:31,499 : Computing embeddings for train/dev/test
2019-02-13 03:57:21,891 : Computed embeddings
2019-02-13 03:57:21,891 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:59:22,593 : [('reg:1e-05', 64.6), ('reg:0.0001', 64.61), ('reg:0.001', 64.68), ('reg:0.01', 64.2)]
2019-02-13 03:59:22,594 : Validation : best param found is reg = 0.001 with score             64.68
2019-02-13 03:59:22,594 : Evaluating...
2019-02-13 03:59:38,188 : 
Dev acc : 64.7 Test acc : 64.3 for ODDMANOUT classification

2019-02-13 03:59:38,196 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 03:59:38,795 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 03:59:38,871 : loading BERT mode bert-base-uncased
2019-02-13 03:59:38,871 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:59:38,903 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:59:38,903 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgtx2sqqz
2019-02-13 03:59:41,351 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:59:42,783 : Computing embeddings for train/dev/test
2019-02-13 04:03:15,425 : Computed embeddings
2019-02-13 04:03:15,425 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:04:12,406 : [('reg:1e-05', 68.33), ('reg:0.0001', 68.29), ('reg:0.001', 57.04), ('reg:0.01', 66.9)]
2019-02-13 04:04:12,406 : Validation : best param found is reg = 1e-05 with score             68.33
2019-02-13 04:04:12,406 : Evaluating...
2019-02-13 04:04:22,955 : 
Dev acc : 68.3 Test acc : 67.9 for COORDINATIONINVERSION classification

2019-02-13 04:04:22,963 : {'STS12': {'MSRpar': {'pearson': (0.29793203523206657, 7.709163093254985e-17), 'spearman': SpearmanrResult(correlation=0.3253386555372184, pvalue=5.971644488826531e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.521904618729797, 1.2504356479926868e-53), 'spearman': SpearmanrResult(correlation=0.5373335379988915, pvalue=2.481386485908897e-57), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.46360777732975544, 7.735605908649822e-26), 'spearman': SpearmanrResult(correlation=0.5761071524590785, pvalue=6.105407663029169e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5090432998861961, 1.0991047250407898e-50), 'spearman': SpearmanrResult(correlation=0.5389988746703394, pvalue=9.627666999652771e-58), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.589615569832882, 1.0105445002872701e-38), 'spearman': SpearmanrResult(correlation=0.49063834435164594, pvalue=1.4697765722897426e-25), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4764206602021394, 'wmean': 0.4648367270089197}, 'spearman': {'mean': 0.4936833130034347, 'wmean': 0.4863099046106373}}}, 'STS13': {'FNWN': {'pearson': (0.16774053660300872, 0.021047899617708664), 'spearman': SpearmanrResult(correlation=0.18279542249642852, pvalue=0.011815766249091246), 'nsamples': 189}, 'headlines': {'pearson': (0.6147169548123854, 3.845547368223589e-79), 'spearman': SpearmanrResult(correlation=0.6040281001921605, pvalue=9.089408833249743e-76), 'nsamples': 750}, 'OnWN': {'pearson': (0.44033828631054356, 5.21346182951842e-28), 'spearman': SpearmanrResult(correlation=0.43370540215032527, pvalue=3.921655992003065e-27), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.40759859257531256, 'wmean': 0.49318030409831515}, 'spearman': {'mean': 0.4068429749463047, 'wmean': 0.48725209373485184}}}, 'STS14': {'deft-forum': {'pearson': (0.319319192600371, 3.994210355953463e-12), 'spearman': SpearmanrResult(correlation=0.32435819432805185, pvalue=1.749454956063282e-12), 'nsamples': 450}, 'deft-news': {'pearson': (0.7170746422738099, 1.2309551447739739e-48), 'spearman': SpearmanrResult(correlation=0.6943467362098951, pvalue=1.745089066094122e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.554885131418249, 8.771688022575813e-62), 'spearman': SpearmanrResult(correlation=0.5290768678795359, pvalue=2.510280408278949e-55), 'nsamples': 750}, 'images': {'pearson': (0.4584824390511578, 2.9615188926409804e-40), 'spearman': SpearmanrResult(correlation=0.46259436536981136, pvalue=4.863041391059824e-41), 'nsamples': 750}, 'OnWN': {'pearson': (0.5855907561832608, 3.063578908698617e-70), 'spearman': SpearmanrResult(correlation=0.6132835279796067, pvalue=1.1088891857475936e-78), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5823367544430249, 2.66295636847571e-69), 'spearman': SpearmanrResult(correlation=0.539745974379931, pvalue=6.285112059096976e-58), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5362814859949788, 'wmean': 0.5319432907130878}, 'spearman': {'mean': 0.5272342776911386, 'wmean': 0.5234108693379348}}}, 'STS15': {'answers-forums': {'pearson': (0.5003471228155241, 3.754378276194958e-25), 'spearman': SpearmanrResult(correlation=0.47872627894145076, pvalue=7.059043450956342e-23), 'nsamples': 375}, 'answers-students': {'pearson': (0.65436837519356, 7.2134155003563335e-93), 'spearman': SpearmanrResult(correlation=0.6622915603854383, pvalue=7.2815793567328606e-96), 'nsamples': 750}, 'belief': {'pearson': (0.544065325524441, 2.811368776120776e-30), 'spearman': SpearmanrResult(correlation=0.5796309699112007, pvalue=4.87683433795372e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.6067518702626525, 1.2913210925210671e-76), 'spearman': SpearmanrResult(correlation=0.6147196359224287, pvalue=3.837917932905326e-79), 'nsamples': 750}, 'images': {'pearson': (0.6331594848360754, 2.8162651318915163e-85), 'spearman': SpearmanrResult(correlation=0.6418795334566697, pvalue=2.518310181911213e-88), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5877384357264506, 'wmean': 0.6041214886155677}, 'spearman': {'mean': 0.5954495957234376, 'wmean': 0.6120173385477157}}}, 'STS16': {'answer-answer': {'pearson': (0.49315371271871794, 5.678555071137419e-17), 'spearman': SpearmanrResult(correlation=0.49162448591271696, pvalue=7.316884410908604e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.628267538483566, 9.461709459931456e-29), 'spearman': SpearmanrResult(correlation=0.632497580763512, pvalue=3.150938126260474e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.663062964663489, 1.6699968345508015e-30), 'spearman': SpearmanrResult(correlation=0.6750648318473675, pvalue=5.961487072830946e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.7541349628582156, 4.332399365935241e-46), 'spearman': SpearmanrResult(correlation=0.7929888280800257, pvalue=5.338358796023229e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.23204096780166983, 0.0007229609337872613), 'spearman': SpearmanrResult(correlation=0.252789676308243, pvalue=0.00022181779152539248), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5541320293051316, 'wmean': 0.562149776723034}, 'spearman': {'mean': 0.5689930805823731, 'wmean': 0.5766878117679496}}}, 'MR': {'devacc': 73.6, 'acc': 73.01, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.26, 'acc': 73.25, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.59, 'acc': 87.68, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.27, 'acc': 92.91, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.06, 'acc': 83.2, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.51, 'acc': 43.26, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 74.42, 'acc': 89.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.86, 'acc': 69.91, 'f1': 75.06, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.4, 'acc': 73.03, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7849114722360065, 'pearson': 0.7791974566428523, 'spearman': 0.7155194844797105, 'mse': 0.40100860132932276, 'yhat': array([1.69784252, 3.7252335 , 1.81158338, ..., 3.09173285, 4.16478261,
       4.08256874]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6350037223493994, 'pearson': 0.625147973339803, 'spearman': 0.6210754290394137, 'mse': 1.4506891891495912, 'yhat': array([1.33419468, 1.39959447, 2.11318258, ..., 3.81336286, 3.78110322,
       4.02130302]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 64.73, 'acc': 64.58, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 12.36, 'acc': 12.8, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.28, 'acc': 30.92, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 56.63, 'acc': 56.9, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.33, 'acc': 87.83, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.58, 'acc': 87.99, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.46, 'acc': 81.46, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.99, 'acc': 78.24, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.68, 'acc': 64.32, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 68.33, 'acc': 67.89, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 04:04:22,963 : ********************************************************************************
2019-02-13 04:04:22,963 : ********************************************************************************
2019-02-13 04:04:22,964 : ********************************************************************************
2019-02-13 04:04:22,964 : layer 10
2019-02-13 04:04:22,964 : ********************************************************************************
2019-02-13 04:04:22,964 : ********************************************************************************
2019-02-13 04:04:22,964 : ********************************************************************************
2019-02-13 04:04:23,047 : ***** Transfer task : STS12 *****


2019-02-13 04:04:23,059 : loading BERT mode bert-base-uncased
2019-02-13 04:04:23,059 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:04:23,076 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:04:23,077 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5ne4efj8
2019-02-13 04:04:25,509 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:04:29,236 : MSRpar : pearson = 0.3177, spearman = 0.3499
2019-02-13 04:04:30,330 : MSRvid : pearson = 0.5318, spearman = 0.5517
2019-02-13 04:04:31,194 : SMTeuroparl : pearson = 0.4933, spearman = 0.5934
2019-02-13 04:04:32,678 : surprise.OnWN : pearson = 0.5113, spearman = 0.5411
2019-02-13 04:04:33,503 : surprise.SMTnews : pearson = 0.5777, spearman = 0.4969
2019-02-13 04:04:33,503 : ALL (weighted average) : Pearson = 0.4754,             Spearman = 0.4996
2019-02-13 04:04:33,503 : ALL (average) : Pearson = 0.4864,             Spearman = 0.5066

2019-02-13 04:04:33,503 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 04:04:33,513 : loading BERT mode bert-base-uncased
2019-02-13 04:04:33,513 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:04:33,531 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:04:33,531 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7bveclsu
2019-02-13 04:04:35,977 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:04:38,278 : FNWN : pearson = 0.2312, spearman = 0.2522
2019-02-13 04:04:39,939 : headlines : pearson = 0.6394, spearman = 0.6270
2019-02-13 04:04:41,257 : OnWN : pearson = 0.5518, spearman = 0.5427
2019-02-13 04:04:41,257 : ALL (weighted average) : Pearson = 0.5552,             Spearman = 0.5483
2019-02-13 04:04:41,257 : ALL (average) : Pearson = 0.4741,             Spearman = 0.4740

2019-02-13 04:04:41,257 : ***** Transfer task : STS14 *****


2019-02-13 04:04:41,273 : loading BERT mode bert-base-uncased
2019-02-13 04:04:41,274 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:04:41,291 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:04:41,291 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqcqiz3rr
2019-02-13 04:04:43,723 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:04:46,440 : deft-forum : pearson = 0.3375, spearman = 0.3397
2019-02-13 04:04:47,565 : deft-news : pearson = 0.7413, spearman = 0.7210
2019-02-13 04:04:49,492 : headlines : pearson = 0.5794, spearman = 0.5454
2019-02-13 04:04:51,378 : images : pearson = 0.4654, spearman = 0.4666
2019-02-13 04:04:53,316 : OnWN : pearson = 0.6501, spearman = 0.6751
2019-02-13 04:04:55,520 : tweet-news : pearson = 0.6091, spearman = 0.5568
2019-02-13 04:04:55,521 : ALL (weighted average) : Pearson = 0.5606,             Spearman = 0.5472
2019-02-13 04:04:55,521 : ALL (average) : Pearson = 0.5638,             Spearman = 0.5508

2019-02-13 04:04:55,521 : ***** Transfer task : STS15 *****


2019-02-13 04:04:55,555 : loading BERT mode bert-base-uncased
2019-02-13 04:04:55,555 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:04:55,572 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:04:55,572 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxzv3gsqy
2019-02-13 04:04:58,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:05:00,814 : answers-forums : pearson = 0.5325, spearman = 0.5137
2019-02-13 04:05:02,796 : answers-students : pearson = 0.6244, spearman = 0.6396
2019-02-13 04:05:04,216 : belief : pearson = 0.5829, spearman = 0.6120
2019-02-13 04:05:06,312 : headlines : pearson = 0.6406, spearman = 0.6457
2019-02-13 04:05:08,365 : images : pearson = 0.6460, spearman = 0.6576
2019-02-13 04:05:08,365 : ALL (weighted average) : Pearson = 0.6172,             Spearman = 0.6265
2019-02-13 04:05:08,365 : ALL (average) : Pearson = 0.6053,             Spearman = 0.6137

2019-02-13 04:05:08,365 : ***** Transfer task : STS16 *****


2019-02-13 04:05:08,434 : loading BERT mode bert-base-uncased
2019-02-13 04:05:08,434 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:05:08,452 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:05:08,453 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuh3bhse5
2019-02-13 04:05:10,883 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:05:12,942 : answer-answer : pearson = 0.5145, spearman = 0.5004
2019-02-13 04:05:13,448 : headlines : pearson = 0.6537, spearman = 0.6568
2019-02-13 04:05:14,027 : plagiarism : pearson = 0.7186, spearman = 0.7258
2019-02-13 04:05:14,809 : postediting : pearson = 0.7791, spearman = 0.8085
2019-02-13 04:05:15,307 : question-question : pearson = 0.3818, spearman = 0.3936
2019-02-13 04:05:15,307 : ALL (weighted average) : Pearson = 0.6144,             Spearman = 0.6215
2019-02-13 04:05:15,307 : ALL (average) : Pearson = 0.6095,             Spearman = 0.6170

2019-02-13 04:05:15,307 : ***** Transfer task : MR *****


2019-02-13 04:05:15,324 : loading BERT mode bert-base-uncased
2019-02-13 04:05:15,324 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:05:15,344 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:05:15,345 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl1h8sd5e
2019-02-13 04:05:17,775 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:05:19,351 : Generating sentence embeddings
2019-02-13 04:05:43,929 : Generated sentence embeddings
2019-02-13 04:05:43,929 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:06:22,249 : Best param found at split 1: l2reg = 0.01                 with score 74.28
2019-02-13 04:07:15,072 : Best param found at split 2: l2reg = 1e-05                 with score 75.04
2019-02-13 04:07:49,467 : Best param found at split 3: l2reg = 1e-05                 with score 71.04
2019-02-13 04:08:12,894 : Best param found at split 4: l2reg = 0.01                 with score 75.1
2019-02-13 04:08:33,512 : Best param found at split 5: l2reg = 0.001                 with score 76.18
2019-02-13 04:08:34,530 : Dev acc : 74.33 Test acc : 74.48

2019-02-13 04:08:34,531 : ***** Transfer task : CR *****


2019-02-13 04:08:34,540 : loading BERT mode bert-base-uncased
2019-02-13 04:08:34,540 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:08:34,561 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:08:34,561 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaqpjw9u9
2019-02-13 04:08:37,022 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:08:38,803 : Generating sentence embeddings
2019-02-13 04:08:44,395 : Generated sentence embeddings
2019-02-13 04:08:44,395 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:08:52,461 : Best param found at split 1: l2reg = 1e-05                 with score 77.31
2019-02-13 04:09:01,549 : Best param found at split 2: l2reg = 0.001                 with score 79.86
2019-02-13 04:09:15,493 : Best param found at split 3: l2reg = 1e-05                 with score 80.43
2019-02-13 04:09:28,650 : Best param found at split 4: l2reg = 0.001                 with score 81.53
2019-02-13 04:09:42,476 : Best param found at split 5: l2reg = 0.0001                 with score 75.34
2019-02-13 04:09:43,251 : Dev acc : 78.89 Test acc : 80.71

2019-02-13 04:09:43,251 : ***** Transfer task : MPQA *****


2019-02-13 04:09:43,256 : loading BERT mode bert-base-uncased
2019-02-13 04:09:43,257 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:09:43,276 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:09:43,276 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnplsk1h8
2019-02-13 04:09:45,703 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:09:47,361 : Generating sentence embeddings
2019-02-13 04:10:03,447 : Generated sentence embeddings
2019-02-13 04:10:03,448 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:10:43,869 : Best param found at split 1: l2reg = 0.0001                 with score 87.38
2019-02-13 04:11:25,215 : Best param found at split 2: l2reg = 0.001                 with score 87.54
2019-02-13 04:11:49,185 : Best param found at split 3: l2reg = 0.001                 with score 86.88
2019-02-13 04:12:05,904 : Best param found at split 4: l2reg = 0.01                 with score 87.11
2019-02-13 04:12:25,038 : Best param found at split 5: l2reg = 1e-05                 with score 86.72
2019-02-13 04:12:26,752 : Dev acc : 87.13 Test acc : 87.62

2019-02-13 04:12:26,752 : ***** Transfer task : SUBJ *****


2019-02-13 04:12:26,766 : loading BERT mode bert-base-uncased
2019-02-13 04:12:26,766 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:12:26,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:12:26,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnocr_n29
2019-02-13 04:12:29,219 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:12:30,762 : Generating sentence embeddings
2019-02-13 04:12:48,760 : Generated sentence embeddings
2019-02-13 04:12:48,761 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:13:31,312 : Best param found at split 1: l2reg = 0.01                 with score 94.06
2019-02-13 04:14:17,712 : Best param found at split 2: l2reg = 1e-05                 with score 94.26
2019-02-13 04:15:05,166 : Best param found at split 3: l2reg = 1e-05                 with score 93.72
2019-02-13 04:15:37,088 : Best param found at split 4: l2reg = 1e-05                 with score 94.52
2019-02-13 04:15:55,173 : Best param found at split 5: l2reg = 0.001                 with score 94.01
2019-02-13 04:15:56,091 : Dev acc : 94.11 Test acc : 93.65

2019-02-13 04:15:56,092 : ***** Transfer task : SST Binary classification *****


2019-02-13 04:15:56,224 : loading BERT mode bert-base-uncased
2019-02-13 04:15:56,224 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:15:56,246 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:15:56,247 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2ag_678q
2019-02-13 04:15:58,675 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:16:00,101 : Computing embedding for train
2019-02-13 04:17:08,140 : Computed train embeddings
2019-02-13 04:17:08,140 : Computing embedding for dev
2019-02-13 04:17:09,092 : Computed dev embeddings
2019-02-13 04:17:09,092 : Computing embedding for test
2019-02-13 04:17:12,725 : Computed test embeddings
2019-02-13 04:17:12,725 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:18:05,585 : [('reg:1e-05', 84.75), ('reg:0.0001', 84.63), ('reg:0.001', 84.4), ('reg:0.01', 85.32)]
2019-02-13 04:18:05,585 : Validation : best param found is reg = 0.01 with score             85.32
2019-02-13 04:18:05,586 : Evaluating...
2019-02-13 04:18:15,496 : 
Dev acc : 85.32 Test acc : 83.31 for             SST Binary classification

2019-02-13 04:18:15,497 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 04:18:15,547 : loading BERT mode bert-base-uncased
2019-02-13 04:18:15,547 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:18:15,568 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:18:15,568 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_de_p8wn
2019-02-13 04:18:18,004 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:18:19,490 : Computing embedding for train
2019-02-13 04:18:35,110 : Computed train embeddings
2019-02-13 04:18:35,110 : Computing embedding for dev
2019-02-13 04:18:37,225 : Computed dev embeddings
2019-02-13 04:18:37,225 : Computing embedding for test
2019-02-13 04:18:41,483 : Computed test embeddings
2019-02-13 04:18:41,483 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:18:48,810 : [('reg:1e-05', 42.05), ('reg:0.0001', 37.6), ('reg:0.001', 37.24), ('reg:0.01', 40.42)]
2019-02-13 04:18:48,810 : Validation : best param found is reg = 1e-05 with score             42.05
2019-02-13 04:18:48,811 : Evaluating...
2019-02-13 04:18:50,361 : 
Dev acc : 42.05 Test acc : 42.26 for             SST Fine-Grained classification

2019-02-13 04:18:50,362 : ***** Transfer task : TREC *****


2019-02-13 04:18:50,375 : loading BERT mode bert-base-uncased
2019-02-13 04:18:50,375 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:18:50,393 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:18:50,394 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe9d3kzl8
2019-02-13 04:18:52,821 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:19:03,157 : Computed train embeddings
2019-02-13 04:19:03,956 : Computed test embeddings
2019-02-13 04:19:03,956 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 04:19:22,246 : [('reg:1e-05', 75.63), ('reg:0.0001', 73.7), ('reg:0.001', 67.18), ('reg:0.01', 74.28)]
2019-02-13 04:19:22,247 : Cross-validation : best param found is reg = 1e-05             with score 75.63
2019-02-13 04:19:22,247 : Evaluating...
2019-02-13 04:19:23,071 : 
Dev acc : 75.63 Test acc : 74.8             for TREC

2019-02-13 04:19:23,071 : ***** Transfer task : MRPC *****


2019-02-13 04:19:23,093 : loading BERT mode bert-base-uncased
2019-02-13 04:19:23,093 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:19:23,113 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:19:23,113 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeel3k_ev
2019-02-13 04:19:25,539 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:19:27,087 : Computing embedding for train
2019-02-13 04:19:40,039 : Computed train embeddings
2019-02-13 04:19:40,039 : Computing embedding for test
2019-02-13 04:19:45,400 : Computed test embeddings
2019-02-13 04:19:45,416 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 04:19:52,592 : [('reg:1e-05', 72.08), ('reg:0.0001', 71.79), ('reg:0.001', 72.25), ('reg:0.01', 71.05)]
2019-02-13 04:19:52,593 : Cross-validation : best param found is reg = 0.001             with score 72.25
2019-02-13 04:19:52,593 : Evaluating...
2019-02-13 04:19:52,958 : Dev acc : 72.25 Test acc 68.29; Test F1 74.64 for MRPC.

2019-02-13 04:19:52,959 : ***** Transfer task : SICK-Entailment*****


2019-02-13 04:19:53,024 : loading BERT mode bert-base-uncased
2019-02-13 04:19:53,024 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:19:53,045 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:19:53,046 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp828l0e2b
2019-02-13 04:19:55,610 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:19:57,123 : Computing embedding for train
2019-02-13 04:20:04,873 : Computed train embeddings
2019-02-13 04:20:04,873 : Computing embedding for dev
2019-02-13 04:20:05,845 : Computed dev embeddings
2019-02-13 04:20:05,845 : Computing embedding for test
2019-02-13 04:20:13,615 : Computed test embeddings
2019-02-13 04:20:13,643 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:20:16,878 : [('reg:1e-05', 76.8), ('reg:0.0001', 77.0), ('reg:0.001', 77.2), ('reg:0.01', 73.2)]
2019-02-13 04:20:16,878 : Validation : best param found is reg = 0.001 with score             77.2
2019-02-13 04:20:16,878 : Evaluating...
2019-02-13 04:20:17,707 : 
Dev acc : 77.2 Test acc : 76.46 for                        SICK entailment

2019-02-13 04:20:17,707 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 04:20:17,734 : loading BERT mode bert-base-uncased
2019-02-13 04:20:17,734 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:20:17,793 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:20:17,794 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr4_gxq5p
2019-02-13 04:20:20,228 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:20:21,648 : Computing embedding for train
2019-02-13 04:20:29,412 : Computed train embeddings
2019-02-13 04:20:29,412 : Computing embedding for dev
2019-02-13 04:20:30,388 : Computed dev embeddings
2019-02-13 04:20:30,388 : Computing embedding for test
2019-02-13 04:20:38,743 : Computed test embeddings
2019-02-13 04:21:21,487 : Dev : Pearson 0.7721522474429817
2019-02-13 04:21:21,487 : Test : Pearson 0.7852996423354045 Spearman 0.7144851283534263 MSE 0.3907123645302785                        for SICK Relatedness

2019-02-13 04:21:21,488 : 

***** Transfer task : STSBenchmark*****


2019-02-13 04:21:21,528 : loading BERT mode bert-base-uncased
2019-02-13 04:21:21,528 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:21:21,556 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:21:21,557 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk1rbjeh9
2019-02-13 04:21:23,989 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:21:25,538 : Computing embedding for train
2019-02-13 04:21:42,229 : Computed train embeddings
2019-02-13 04:21:42,229 : Computing embedding for dev
2019-02-13 04:21:47,084 : Computed dev embeddings
2019-02-13 04:21:47,084 : Computing embedding for test
2019-02-13 04:21:51,361 : Computed test embeddings
2019-02-13 04:22:52,469 : Dev : Pearson 0.6290811178773263
2019-02-13 04:22:52,469 : Test : Pearson 0.6357128194002151 Spearman 0.6331347975190704 MSE 1.4463465968284188                        for SICK Relatedness

2019-02-13 04:22:52,469 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 04:22:52,783 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 04:22:52,792 : loading BERT mode bert-base-uncased
2019-02-13 04:22:52,793 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:22:52,817 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:22:52,817 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp79x9t4df
2019-02-13 04:22:55,249 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:22:56,661 : Computing embeddings for train/dev/test
2019-02-13 04:25:24,774 : Computed embeddings
2019-02-13 04:25:24,774 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:26:38,592 : [('reg:1e-05', 60.17), ('reg:0.0001', 59.76), ('reg:0.001', 57.93), ('reg:0.01', 53.06)]
2019-02-13 04:26:38,592 : Validation : best param found is reg = 1e-05 with score             60.17
2019-02-13 04:26:38,592 : Evaluating...
2019-02-13 04:26:59,097 : 
Dev acc : 60.2 Test acc : 60.4 for LENGTH classification

2019-02-13 04:26:59,104 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 04:26:59,465 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 04:26:59,509 : loading BERT mode bert-base-uncased
2019-02-13 04:26:59,509 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:26:59,609 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:26:59,609 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi7ky84yc
2019-02-13 04:27:02,041 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:27:03,527 : Computing embeddings for train/dev/test
2019-02-13 04:29:44,230 : Computed embeddings
2019-02-13 04:29:44,230 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:31:49,310 : [('reg:1e-05', 20.98), ('reg:0.0001', 7.45), ('reg:0.001', 0.83), ('reg:0.01', 0.17)]
2019-02-13 04:31:49,310 : Validation : best param found is reg = 1e-05 with score             20.98
2019-02-13 04:31:49,310 : Evaluating...
2019-02-13 04:32:10,034 : 
Dev acc : 21.0 Test acc : 21.0 for WORDCONTENT classification

2019-02-13 04:32:10,042 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 04:32:10,577 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 04:32:10,642 : loading BERT mode bert-base-uncased
2019-02-13 04:32:10,642 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:32:10,669 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:32:10,669 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0ygsh4b1
2019-02-13 04:32:13,096 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:32:14,524 : Computing embeddings for train/dev/test
2019-02-13 04:34:42,079 : Computed embeddings
2019-02-13 04:34:42,080 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:36:28,144 : [('reg:1e-05', 28.41), ('reg:0.0001', 28.82), ('reg:0.001', 27.29), ('reg:0.01', 22.25)]
2019-02-13 04:36:28,145 : Validation : best param found is reg = 0.0001 with score             28.82
2019-02-13 04:36:28,145 : Evaluating...
2019-02-13 04:36:48,559 : 
Dev acc : 28.8 Test acc : 28.6 for DEPTH classification

2019-02-13 04:36:48,566 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 04:36:48,937 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 04:36:49,000 : loading BERT mode bert-base-uncased
2019-02-13 04:36:49,000 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:36:49,112 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:36:49,112 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1um96q8t
2019-02-13 04:36:51,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:36:53,030 : Computing embeddings for train/dev/test
2019-02-13 04:38:56,288 : Computed embeddings
2019-02-13 04:38:56,289 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:40:14,337 : [('reg:1e-05', 51.4), ('reg:0.0001', 52.49), ('reg:0.001', 50.24), ('reg:0.01', 43.19)]
2019-02-13 04:40:14,337 : Validation : best param found is reg = 0.0001 with score             52.49
2019-02-13 04:40:14,337 : Evaluating...
2019-02-13 04:40:39,438 : 
Dev acc : 52.5 Test acc : 52.7 for TOPCONSTITUENTS classification

2019-02-13 04:40:39,446 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 04:40:39,976 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 04:40:40,042 : loading BERT mode bert-base-uncased
2019-02-13 04:40:40,042 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:40:40,074 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:40:40,075 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ij_o6kh
2019-02-13 04:40:42,510 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:40:43,984 : Computing embeddings for train/dev/test
2019-02-13 04:43:06,045 : Computed embeddings
2019-02-13 04:43:06,045 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:44:38,301 : [('reg:1e-05', 87.58), ('reg:0.0001', 87.79), ('reg:0.001', 87.83), ('reg:0.01', 87.18)]
2019-02-13 04:44:38,301 : Validation : best param found is reg = 0.001 with score             87.83
2019-02-13 04:44:38,301 : Evaluating...
2019-02-13 04:45:08,311 : 
Dev acc : 87.8 Test acc : 86.9 for BIGRAMSHIFT classification

2019-02-13 04:45:08,318 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 04:45:08,886 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 04:45:08,951 : loading BERT mode bert-base-uncased
2019-02-13 04:45:08,951 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:45:08,981 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:45:08,982 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7plpxgl_
2019-02-13 04:45:11,410 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:45:12,802 : Computing embeddings for train/dev/test
2019-02-13 04:47:27,306 : Computed embeddings
2019-02-13 04:47:27,306 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:48:48,855 : [('reg:1e-05', 89.6), ('reg:0.0001', 89.66), ('reg:0.001', 89.65), ('reg:0.01', 89.82)]
2019-02-13 04:48:48,856 : Validation : best param found is reg = 0.01 with score             89.82
2019-02-13 04:48:48,856 : Evaluating...
2019-02-13 04:49:04,862 : 
Dev acc : 89.8 Test acc : 88.3 for TENSE classification

2019-02-13 04:49:04,870 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 04:49:05,298 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 04:49:05,361 : loading BERT mode bert-base-uncased
2019-02-13 04:49:05,361 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:49:05,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:49:05,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0losl3rp
2019-02-13 04:49:07,818 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:49:09,257 : Computing embeddings for train/dev/test
2019-02-13 04:51:38,163 : Computed embeddings
2019-02-13 04:51:38,163 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:52:59,949 : [('reg:1e-05', 81.76), ('reg:0.0001', 73.3), ('reg:0.001', 79.39), ('reg:0.01', 80.6)]
2019-02-13 04:52:59,949 : Validation : best param found is reg = 1e-05 with score             81.76
2019-02-13 04:52:59,949 : Evaluating...
2019-02-13 04:53:16,943 : 
Dev acc : 81.8 Test acc : 81.2 for SUBJNUMBER classification

2019-02-13 04:53:16,950 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 04:53:17,361 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 04:53:17,427 : loading BERT mode bert-base-uncased
2019-02-13 04:53:17,427 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:53:17,544 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:53:17,544 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps3sz_18p
2019-02-13 04:53:19,970 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:53:21,380 : Computing embeddings for train/dev/test
2019-02-13 04:55:31,490 : Computed embeddings
2019-02-13 04:55:31,490 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:57:32,090 : [('reg:1e-05', 76.56), ('reg:0.0001', 77.19), ('reg:0.001', 76.0), ('reg:0.01', 74.48)]
2019-02-13 04:57:32,090 : Validation : best param found is reg = 0.0001 with score             77.19
2019-02-13 04:57:32,090 : Evaluating...
2019-02-13 04:57:52,215 : 
Dev acc : 77.2 Test acc : 78.0 for OBJNUMBER classification

2019-02-13 04:57:52,222 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 04:57:52,604 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 04:57:52,673 : loading BERT mode bert-base-uncased
2019-02-13 04:57:52,673 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:57:52,798 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:57:52,798 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr2ritk7o
2019-02-13 04:57:55,232 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:57:56,684 : Computing embeddings for train/dev/test
2019-02-13 05:00:38,512 : Computed embeddings
2019-02-13 05:00:38,512 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:01:52,548 : [('reg:1e-05', 61.84), ('reg:0.0001', 61.83), ('reg:0.001', 61.75), ('reg:0.01', 61.12)]
2019-02-13 05:01:52,548 : Validation : best param found is reg = 1e-05 with score             61.84
2019-02-13 05:01:52,548 : Evaluating...
2019-02-13 05:02:06,370 : 
Dev acc : 61.8 Test acc : 60.8 for ODDMANOUT classification

2019-02-13 05:02:06,377 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 05:02:06,987 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 05:02:07,065 : loading BERT mode bert-base-uncased
2019-02-13 05:02:07,065 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:02:07,097 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:02:07,098 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm0ws1sj0
2019-02-13 05:02:09,530 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:02:11,011 : Computing embeddings for train/dev/test
2019-02-13 05:05:11,555 : Computed embeddings
2019-02-13 05:05:11,555 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:06:34,631 : [('reg:1e-05', 67.12), ('reg:0.0001', 67.09), ('reg:0.001', 67.09), ('reg:0.01', 57.33)]
2019-02-13 05:06:34,632 : Validation : best param found is reg = 1e-05 with score             67.12
2019-02-13 05:06:34,632 : Evaluating...
2019-02-13 05:06:52,512 : 
Dev acc : 67.1 Test acc : 67.4 for COORDINATIONINVERSION classification

2019-02-13 05:06:52,520 : {'STS12': {'MSRpar': {'pearson': (0.3177114267772954, 4.727847183135348e-19), 'spearman': SpearmanrResult(correlation=0.3499168752297693, pvalue=5.035205656223861e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5318149239190572, 5.507467473705454e-56), 'spearman': SpearmanrResult(correlation=0.5516689876322041, pvalue=6.0069226364554184e-61), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4933395755640929, 1.5804102241274114e-29), 'spearman': SpearmanrResult(correlation=0.5934293939153934, pvalue=5.21522285902493e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5112644090069811, 3.4799335307210674e-51), 'spearman': SpearmanrResult(correlation=0.5411381758857834, pvalue=2.8309993326193958e-58), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5777264487368039, 6.731033207179055e-37), 'spearman': SpearmanrResult(correlation=0.49693317366691375, pvalue=2.838515326067615e-26), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48637135680084603, 'wmean': 0.47540179794318005}, 'spearman': {'mean': 0.5066173212660128, 'wmean': 0.4995828369244149}}}, 'STS13': {'FNWN': {'pearson': (0.23118197837895474, 0.0013713584019167083), 'spearman': SpearmanrResult(correlation=0.2521752725341372, pvalue=0.00046423869216876047), 'nsamples': 189}, 'headlines': {'pearson': (0.6394082101351356, 1.8842223676951607e-87), 'spearman': SpearmanrResult(correlation=0.6270260023297658, pvalue=3.435498168009744e-83), 'nsamples': 750}, 'OnWN': {'pearson': (0.551772468466727, 5.185677579213662e-46), 'spearman': SpearmanrResult(correlation=0.5426976734286421, pvalue=2.773261025593889e-44), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4741208856602725, 'wmean': 0.555195937549872}, 'spearman': {'mean': 0.473966316097515, 'wmean': 0.5482560153664963}}}, 'STS14': {'deft-forum': {'pearson': (0.3374661416425509, 1.899211939761301e-13), 'spearman': SpearmanrResult(correlation=0.33970121496729017, pvalue=1.2867853489854492e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7412917859372268, 1.5555471944315878e-53), 'spearman': SpearmanrResult(correlation=0.7209626829922081, pvalue=2.1820003028036598e-49), 'nsamples': 300}, 'headlines': {'pearson': (0.5793712820748428, 1.8708947850123745e-68), 'spearman': SpearmanrResult(correlation=0.5454422646471138, pvalue=2.3484748937665537e-59), 'nsamples': 750}, 'images': {'pearson': (0.46540785808934, 1.39273068011811e-41), 'spearman': SpearmanrResult(correlation=0.46659944632112677, pvalue=8.172324620310519e-42), 'nsamples': 750}, 'OnWN': {'pearson': (0.6500834612461802, 2.7606413424763127e-91), 'spearman': SpearmanrResult(correlation=0.6750857328186214, pvalue=6.701524753287493e-101), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6091385646641065, 2.2995102772238432e-77), 'spearman': SpearmanrResult(correlation=0.5567529364339914, pvalue=2.8419843294685394e-62), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5637931822757078, 'wmean': 0.5605995130869782}, 'spearman': {'mean': 0.5507573796967252, 'wmean': 0.5472172364796222}}}, 'STS15': {'answers-forums': {'pearson': (0.5324864628689328, 7.58050754536243e-29), 'spearman': SpearmanrResult(correlation=0.5137325363553714, pvalue=1.2113726990362248e-26), 'nsamples': 375}, 'answers-students': {'pearson': (0.6243591684234505, 2.68302404740675e-82), 'spearman': SpearmanrResult(correlation=0.6396216793985848, pvalue=1.5847187041905987e-87), 'nsamples': 750}, 'belief': {'pearson': (0.5829396487372465, 1.641147566853056e-35), 'spearman': SpearmanrResult(correlation=0.6119691275953213, pvalue=6.648769163360828e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6406077613937924, 7.110437054299213e-88), 'spearman': SpearmanrResult(correlation=0.6457355513727421, pvalue=1.049915692172674e-89), 'nsamples': 750}, 'images': {'pearson': (0.6459502560923823, 8.78477081736151e-90), 'spearman': SpearmanrResult(correlation=0.6576285600760874, pvalue=4.329157726919352e-94), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6052686595031609, 'wmean': 0.6171575604281787}, 'spearman': {'mean': 0.6137374909596215, 'wmean': 0.6264591557056901}}}, 'STS16': {'answer-answer': {'pearson': (0.5145166361674754, 1.4364194809809388e-18), 'spearman': SpearmanrResult(correlation=0.5004123284666717, pvalue=1.6754005001631563e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6537489191754112, 9.621269748742019e-32), 'spearman': SpearmanrResult(correlation=0.6567974382085588, pvalue=4.033786866631412e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7185760083970633, 7.95567389677719e-38), 'spearman': SpearmanrResult(correlation=0.7257582814804022, pvalue=6.652142249914582e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.779131680989704, 5.396994562214571e-51), 'spearman': SpearmanrResult(correlation=0.8084694684517145, pvalue=1.2202097911184729e-57), 'nsamples': 244}, 'question-question': {'pearson': (0.3817748968108354, 1.1725437139989676e-08), 'spearman': SpearmanrResult(correlation=0.3935699364502679, pvalue=3.739870912110002e-09), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6095496283080978, 'wmean': 0.614369537932119}, 'spearman': {'mean': 0.617001490611523, 'wmean': 0.6214960921629702}}}, 'MR': {'devacc': 74.33, 'acc': 74.48, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 78.89, 'acc': 80.71, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.13, 'acc': 87.62, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.11, 'acc': 93.65, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 85.32, 'acc': 83.31, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.05, 'acc': 42.26, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 75.63, 'acc': 74.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.25, 'acc': 68.29, 'f1': 74.64, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.2, 'acc': 76.46, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7721522474429817, 'pearson': 0.7852996423354045, 'spearman': 0.7144851283534263, 'mse': 0.3907123645302785, 'yhat': array([2.7546782 , 3.58835174, 1.92281702, ..., 3.18413261, 4.05649257,
       4.71688018]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6290811178773263, 'pearson': 0.6357128194002151, 'spearman': 0.6331347975190704, 'mse': 1.4463465968284188, 'yhat': array([1.87483528, 1.15069896, 1.86073579, ..., 3.88181083, 4.11178007,
       3.9461092 ]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 60.17, 'acc': 60.38, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 20.98, 'acc': 21.03, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.82, 'acc': 28.55, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 52.49, 'acc': 52.7, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.83, 'acc': 86.91, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.82, 'acc': 88.28, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.76, 'acc': 81.25, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.19, 'acc': 78.05, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 61.84, 'acc': 60.77, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 67.12, 'acc': 67.43, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 05:06:52,520 : ********************************************************************************
2019-02-13 05:06:52,520 : ********************************************************************************
2019-02-13 05:06:52,520 : ********************************************************************************
2019-02-13 05:06:52,520 : layer 11
2019-02-13 05:06:52,520 : ********************************************************************************
2019-02-13 05:06:52,520 : ********************************************************************************
2019-02-13 05:06:52,520 : ********************************************************************************
2019-02-13 05:06:52,604 : ***** Transfer task : STS12 *****


2019-02-13 05:06:52,616 : loading BERT mode bert-base-uncased
2019-02-13 05:06:52,616 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:06:52,633 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:06:52,633 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp618rvht7
2019-02-13 05:06:55,061 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:06:59,283 : MSRpar : pearson = 0.3160, spearman = 0.3507
2019-02-13 05:07:01,145 : MSRvid : pearson = 0.5576, spearman = 0.5726
2019-02-13 05:07:02,479 : SMTeuroparl : pearson = 0.4934, spearman = 0.5902
2019-02-13 05:07:04,727 : surprise.OnWN : pearson = 0.5098, spearman = 0.5336
2019-02-13 05:07:05,967 : surprise.SMTnews : pearson = 0.5582, spearman = 0.4885
2019-02-13 05:07:05,968 : ALL (weighted average) : Pearson = 0.4783,             Spearman = 0.5014
2019-02-13 05:07:05,968 : ALL (average) : Pearson = 0.4870,             Spearman = 0.5071

2019-02-13 05:07:05,968 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 05:07:05,976 : loading BERT mode bert-base-uncased
2019-02-13 05:07:05,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:07:05,993 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:07:05,994 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1ncymakr
2019-02-13 05:07:08,421 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:07:10,865 : FNWN : pearson = 0.2562, spearman = 0.2735
2019-02-13 05:07:12,871 : headlines : pearson = 0.6487, spearman = 0.6361
2019-02-13 05:07:14,410 : OnWN : pearson = 0.5590, spearman = 0.5502
2019-02-13 05:07:14,410 : ALL (weighted average) : Pearson = 0.5657,             Spearman = 0.5583
2019-02-13 05:07:14,410 : ALL (average) : Pearson = 0.4880,             Spearman = 0.4866

2019-02-13 05:07:14,410 : ***** Transfer task : STS14 *****


2019-02-13 05:07:14,428 : loading BERT mode bert-base-uncased
2019-02-13 05:07:14,428 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:07:14,446 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:07:14,446 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwz5385ly
2019-02-13 05:07:16,871 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:07:19,647 : deft-forum : pearson = 0.3459, spearman = 0.3487
2019-02-13 05:07:20,824 : deft-news : pearson = 0.7386, spearman = 0.7196
2019-02-13 05:07:22,904 : headlines : pearson = 0.5839, spearman = 0.5494
2019-02-13 05:07:25,043 : images : pearson = 0.4736, spearman = 0.4719
2019-02-13 05:07:27,270 : OnWN : pearson = 0.6657, spearman = 0.6876
2019-02-13 05:07:29,712 : tweet-news : pearson = 0.6038, spearman = 0.5487
2019-02-13 05:07:29,713 : ALL (weighted average) : Pearson = 0.5660,             Spearman = 0.5509
2019-02-13 05:07:29,713 : ALL (average) : Pearson = 0.5686,             Spearman = 0.5543

2019-02-13 05:07:29,713 : ***** Transfer task : STS15 *****


2019-02-13 05:07:29,746 : loading BERT mode bert-base-uncased
2019-02-13 05:07:29,746 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:07:29,764 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:07:29,764 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo2i6tc35
2019-02-13 05:07:32,191 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:07:35,057 : answers-forums : pearson = 0.5246, spearman = 0.5148
2019-02-13 05:07:37,255 : answers-students : pearson = 0.6059, spearman = 0.6241
2019-02-13 05:07:38,777 : belief : pearson = 0.5875, spearman = 0.6152
2019-02-13 05:07:41,154 : headlines : pearson = 0.6512, spearman = 0.6539
2019-02-13 05:07:43,441 : images : pearson = 0.6498, spearman = 0.6601
2019-02-13 05:07:43,441 : ALL (weighted average) : Pearson = 0.6157,             Spearman = 0.6258
2019-02-13 05:07:43,441 : ALL (average) : Pearson = 0.6038,             Spearman = 0.6136

2019-02-13 05:07:43,441 : ***** Transfer task : STS16 *****


2019-02-13 05:07:43,510 : loading BERT mode bert-base-uncased
2019-02-13 05:07:43,510 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:07:43,528 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:07:43,528 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsp24yuqx
2019-02-13 05:07:45,972 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:07:48,293 : answer-answer : pearson = 0.5254, spearman = 0.5052
2019-02-13 05:07:49,068 : headlines : pearson = 0.6546, spearman = 0.6575
2019-02-13 05:07:49,905 : plagiarism : pearson = 0.7183, spearman = 0.7299
2019-02-13 05:07:50,827 : postediting : pearson = 0.7808, spearman = 0.8120
2019-02-13 05:07:51,437 : question-question : pearson = 0.4230, spearman = 0.4299
2019-02-13 05:07:51,437 : ALL (weighted average) : Pearson = 0.6244,             Spearman = 0.6306
2019-02-13 05:07:51,437 : ALL (average) : Pearson = 0.6204,             Spearman = 0.6269

2019-02-13 05:07:51,437 : ***** Transfer task : MR *****


2019-02-13 05:07:51,454 : loading BERT mode bert-base-uncased
2019-02-13 05:07:51,454 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:07:51,475 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:07:51,475 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph2uqugk6
2019-02-13 05:07:53,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:07:55,479 : Generating sentence embeddings
2019-02-13 05:08:19,549 : Generated sentence embeddings
2019-02-13 05:08:19,550 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:08:51,908 : Best param found at split 1: l2reg = 0.0001                 with score 75.9
2019-02-13 05:09:25,593 : Best param found at split 2: l2reg = 0.0001                 with score 73.09
2019-02-13 05:10:00,588 : Best param found at split 3: l2reg = 0.001                 with score 73.66
2019-02-13 05:10:41,783 : Best param found at split 4: l2reg = 0.001                 with score 73.46
2019-02-13 05:11:10,975 : Best param found at split 5: l2reg = 1e-05                 with score 75.19
2019-02-13 05:11:12,257 : Dev acc : 74.26 Test acc : 75.56

2019-02-13 05:11:12,258 : ***** Transfer task : CR *****


2019-02-13 05:11:12,266 : loading BERT mode bert-base-uncased
2019-02-13 05:11:12,266 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:11:12,285 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:11:12,286 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgvgmji3a
2019-02-13 05:11:14,721 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:11:16,220 : Generating sentence embeddings
2019-02-13 05:11:22,961 : Generated sentence embeddings
2019-02-13 05:11:22,961 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:11:32,195 : Best param found at split 1: l2reg = 0.001                 with score 80.79
2019-02-13 05:11:41,853 : Best param found at split 2: l2reg = 0.0001                 with score 78.8
2019-02-13 05:11:51,577 : Best param found at split 3: l2reg = 1e-05                 with score 77.95
2019-02-13 05:11:58,540 : Best param found at split 4: l2reg = 0.001                 with score 79.31
2019-02-13 05:12:04,423 : Best param found at split 5: l2reg = 1e-05                 with score 77.56
2019-02-13 05:12:04,863 : Dev acc : 78.88 Test acc : 75.44

2019-02-13 05:12:04,863 : ***** Transfer task : MPQA *****


2019-02-13 05:12:04,869 : loading BERT mode bert-base-uncased
2019-02-13 05:12:04,869 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:12:04,889 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:12:04,890 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy4gn7gnq
2019-02-13 05:12:07,365 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:12:08,876 : Generating sentence embeddings
2019-02-13 05:12:19,310 : Generated sentence embeddings
2019-02-13 05:12:19,311 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:12:44,354 : Best param found at split 1: l2reg = 1e-05                 with score 87.36
2019-02-13 05:13:11,796 : Best param found at split 2: l2reg = 0.001                 with score 87.49
2019-02-13 05:13:47,293 : Best param found at split 3: l2reg = 0.001                 with score 87.17
2019-02-13 05:14:25,743 : Best param found at split 4: l2reg = 0.001                 with score 87.63
2019-02-13 05:14:48,879 : Best param found at split 5: l2reg = 0.0001                 with score 87.24
2019-02-13 05:14:49,999 : Dev acc : 87.38 Test acc : 86.58

2019-02-13 05:14:50,000 : ***** Transfer task : SUBJ *****


2019-02-13 05:14:50,014 : loading BERT mode bert-base-uncased
2019-02-13 05:14:50,014 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:14:50,036 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:14:50,036 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxpv0wlh1
2019-02-13 05:14:52,464 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:14:53,939 : Generating sentence embeddings
2019-02-13 05:15:14,971 : Generated sentence embeddings
2019-02-13 05:15:14,971 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:15:43,631 : Best param found at split 1: l2reg = 1e-05                 with score 93.96
2019-02-13 05:16:05,322 : Best param found at split 2: l2reg = 1e-05                 with score 93.8
2019-02-13 05:16:28,369 : Best param found at split 3: l2reg = 1e-05                 with score 93.94
2019-02-13 05:16:52,031 : Best param found at split 4: l2reg = 0.001                 with score 94.19
2019-02-13 05:17:19,116 : Best param found at split 5: l2reg = 0.0001                 with score 93.95
2019-02-13 05:17:20,428 : Dev acc : 93.97 Test acc : 93.7

2019-02-13 05:17:20,429 : ***** Transfer task : SST Binary classification *****


2019-02-13 05:17:20,555 : loading BERT mode bert-base-uncased
2019-02-13 05:17:20,555 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:17:20,577 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:17:20,577 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmujm0sgu
2019-02-13 05:17:23,006 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:17:24,461 : Computing embedding for train
2019-02-13 05:18:59,072 : Computed train embeddings
2019-02-13 05:18:59,072 : Computing embedding for dev
2019-02-13 05:19:00,852 : Computed dev embeddings
2019-02-13 05:19:00,852 : Computing embedding for test
2019-02-13 05:19:04,644 : Computed test embeddings
2019-02-13 05:19:04,644 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:19:42,586 : [('reg:1e-05', 83.94), ('reg:0.0001', 84.06), ('reg:0.001', 84.52), ('reg:0.01', 84.17)]
2019-02-13 05:19:42,586 : Validation : best param found is reg = 0.001 with score             84.52
2019-02-13 05:19:42,586 : Evaluating...
2019-02-13 05:19:49,932 : 
Dev acc : 84.52 Test acc : 83.75 for             SST Binary classification

2019-02-13 05:19:49,932 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 05:19:49,984 : loading BERT mode bert-base-uncased
2019-02-13 05:19:49,985 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:19:50,006 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:19:50,006 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl341pm9k
2019-02-13 05:19:52,446 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:19:53,885 : Computing embedding for train
2019-02-13 05:20:06,008 : Computed train embeddings
2019-02-13 05:20:06,009 : Computing embedding for dev
2019-02-13 05:20:07,589 : Computed dev embeddings
2019-02-13 05:20:07,589 : Computing embedding for test
2019-02-13 05:20:10,733 : Computed test embeddings
2019-02-13 05:20:10,733 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:20:15,537 : [('reg:1e-05', 43.87), ('reg:0.0001', 42.23), ('reg:0.001', 43.51), ('reg:0.01', 35.51)]
2019-02-13 05:20:15,538 : Validation : best param found is reg = 1e-05 with score             43.87
2019-02-13 05:20:15,538 : Evaluating...
2019-02-13 05:20:16,936 : 
Dev acc : 43.87 Test acc : 45.7 for             SST Fine-Grained classification

2019-02-13 05:20:16,937 : ***** Transfer task : TREC *****


2019-02-13 05:20:16,950 : loading BERT mode bert-base-uncased
2019-02-13 05:20:16,950 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:20:16,968 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:20:16,968 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplibwrh4o
2019-02-13 05:20:19,399 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:20:26,069 : Computed train embeddings
2019-02-13 05:20:26,488 : Computed test embeddings
2019-02-13 05:20:26,488 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 05:20:38,789 : [('reg:1e-05', 75.7), ('reg:0.0001', 69.62), ('reg:0.001', 74.94), ('reg:0.01', 71.02)]
2019-02-13 05:20:38,789 : Cross-validation : best param found is reg = 1e-05             with score 75.7
2019-02-13 05:20:38,789 : Evaluating...
2019-02-13 05:20:39,363 : 
Dev acc : 75.7 Test acc : 88.4             for TREC

2019-02-13 05:20:39,364 : ***** Transfer task : MRPC *****


2019-02-13 05:20:39,386 : loading BERT mode bert-base-uncased
2019-02-13 05:20:39,386 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:20:39,406 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:20:39,407 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7sjklt6y
2019-02-13 05:20:41,858 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:20:43,371 : Computing embedding for train
2019-02-13 05:20:57,087 : Computed train embeddings
2019-02-13 05:20:57,088 : Computing embedding for test
2019-02-13 05:21:03,166 : Computed test embeddings
2019-02-13 05:21:03,183 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 05:21:14,757 : [('reg:1e-05', 73.01), ('reg:0.0001', 71.86), ('reg:0.001', 71.64), ('reg:0.01', 72.64)]
2019-02-13 05:21:14,757 : Cross-validation : best param found is reg = 1e-05             with score 73.01
2019-02-13 05:21:14,757 : Evaluating...
2019-02-13 05:21:15,314 : Dev acc : 73.01 Test acc 60.35; Test F1 61.62 for MRPC.

2019-02-13 05:21:15,314 : ***** Transfer task : SICK-Entailment*****


2019-02-13 05:21:15,378 : loading BERT mode bert-base-uncased
2019-02-13 05:21:15,379 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:21:15,398 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:21:15,398 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzz1wjq4w
2019-02-13 05:21:17,830 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:21:19,326 : Computing embedding for train
2019-02-13 05:21:30,453 : Computed train embeddings
2019-02-13 05:21:30,454 : Computing embedding for dev
2019-02-13 05:21:31,844 : Computed dev embeddings
2019-02-13 05:21:31,844 : Computing embedding for test
2019-02-13 05:21:44,777 : Computed test embeddings
2019-02-13 05:21:44,806 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:21:48,231 : [('reg:1e-05', 70.0), ('reg:0.0001', 74.2), ('reg:0.001', 76.6), ('reg:0.01', 73.4)]
2019-02-13 05:21:48,231 : Validation : best param found is reg = 0.001 with score             76.6
2019-02-13 05:21:48,231 : Evaluating...
2019-02-13 05:21:49,157 : 
Dev acc : 76.6 Test acc : 75.34 for                        SICK entailment

2019-02-13 05:21:49,157 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 05:21:49,184 : loading BERT mode bert-base-uncased
2019-02-13 05:21:49,184 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:21:49,240 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:21:49,241 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpio_6ahku
2019-02-13 05:21:51,680 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:21:53,141 : Computing embedding for train
2019-02-13 05:22:05,886 : Computed train embeddings
2019-02-13 05:22:05,886 : Computing embedding for dev
2019-02-13 05:22:07,473 : Computed dev embeddings
2019-02-13 05:22:07,473 : Computing embedding for test
2019-02-13 05:22:22,208 : Computed test embeddings
2019-02-13 05:23:08,177 : Dev : Pearson 0.789778892712242
2019-02-13 05:23:08,178 : Test : Pearson 0.7838590893186084 Spearman 0.7140007001331513 MSE 0.3956690813288107                        for SICK Relatedness

2019-02-13 05:23:08,178 : 

***** Transfer task : STSBenchmark*****


2019-02-13 05:23:08,219 : loading BERT mode bert-base-uncased
2019-02-13 05:23:08,219 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:23:08,247 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:23:08,247 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplbdb5d4p
2019-02-13 05:23:10,712 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:23:12,234 : Computing embedding for train
2019-02-13 05:23:31,328 : Computed train embeddings
2019-02-13 05:23:31,328 : Computing embedding for dev
2019-02-13 05:23:34,748 : Computed dev embeddings
2019-02-13 05:23:34,749 : Computing embedding for test
2019-02-13 05:23:37,587 : Computed test embeddings
2019-02-13 05:24:10,977 : Dev : Pearson 0.6229046588663187
2019-02-13 05:24:10,977 : Test : Pearson 0.6397960231710759 Spearman 0.6345147084797617 MSE 1.4430591637292234                        for SICK Relatedness

2019-02-13 05:24:10,977 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 05:24:11,302 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 05:24:11,312 : loading BERT mode bert-base-uncased
2019-02-13 05:24:11,312 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:24:11,335 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:24:11,336 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj78orgab
2019-02-13 05:24:13,761 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:24:15,211 : Computing embeddings for train/dev/test
2019-02-13 05:27:36,130 : Computed embeddings
2019-02-13 05:27:36,131 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:28:28,936 : [('reg:1e-05', 51.57), ('reg:0.0001', 59.27), ('reg:0.001', 54.96), ('reg:0.01', 46.43)]
2019-02-13 05:28:28,936 : Validation : best param found is reg = 0.0001 with score             59.27
2019-02-13 05:28:28,936 : Evaluating...
2019-02-13 05:28:53,477 : 
Dev acc : 59.3 Test acc : 58.8 for LENGTH classification

2019-02-13 05:28:53,483 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 05:28:53,827 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 05:28:53,872 : loading BERT mode bert-base-uncased
2019-02-13 05:28:53,872 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:28:53,901 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:28:53,901 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgkbuaod0
2019-02-13 05:28:56,328 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:28:57,788 : Computing embeddings for train/dev/test
2019-02-13 05:32:08,417 : Computed embeddings
2019-02-13 05:32:08,417 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:33:21,820 : [('reg:1e-05', 24.82), ('reg:0.0001', 6.58), ('reg:0.001', 0.81), ('reg:0.01', 0.2)]
2019-02-13 05:33:21,820 : Validation : best param found is reg = 1e-05 with score             24.82
2019-02-13 05:33:21,820 : Evaluating...
2019-02-13 05:33:54,427 : 
Dev acc : 24.8 Test acc : 25.3 for WORDCONTENT classification

2019-02-13 05:33:54,434 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 05:33:54,789 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 05:33:54,853 : loading BERT mode bert-base-uncased
2019-02-13 05:33:54,853 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:33:54,879 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:33:54,879 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8dprju_r
2019-02-13 05:33:57,312 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:33:58,834 : Computing embeddings for train/dev/test
2019-02-13 05:37:22,332 : Computed embeddings
2019-02-13 05:37:22,332 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:38:09,973 : [('reg:1e-05', 25.86), ('reg:0.0001', 27.48), ('reg:0.001', 26.92), ('reg:0.01', 23.95)]
2019-02-13 05:38:09,973 : Validation : best param found is reg = 0.0001 with score             27.48
2019-02-13 05:38:09,973 : Evaluating...
2019-02-13 05:38:21,863 : 
Dev acc : 27.5 Test acc : 27.1 for DEPTH classification

2019-02-13 05:38:21,870 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 05:38:22,262 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 05:38:22,326 : loading BERT mode bert-base-uncased
2019-02-13 05:38:22,326 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:38:22,440 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:38:22,440 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe3mezxa2
2019-02-13 05:38:24,897 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:38:26,321 : Computing embeddings for train/dev/test
2019-02-13 05:41:18,480 : Computed embeddings
2019-02-13 05:41:18,480 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:42:13,015 : [('reg:1e-05', 54.37), ('reg:0.0001', 51.54), ('reg:0.001', 51.58), ('reg:0.01', 40.38)]
2019-02-13 05:42:13,015 : Validation : best param found is reg = 1e-05 with score             54.37
2019-02-13 05:42:13,015 : Evaluating...
2019-02-13 05:42:24,153 : 
Dev acc : 54.4 Test acc : 54.2 for TOPCONSTITUENTS classification

2019-02-13 05:42:24,160 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 05:42:24,504 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 05:42:24,570 : loading BERT mode bert-base-uncased
2019-02-13 05:42:24,570 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:42:24,696 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:42:24,696 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9d_phikg
2019-02-13 05:42:27,135 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:42:28,544 : Computing embeddings for train/dev/test
2019-02-13 05:45:11,358 : Computed embeddings
2019-02-13 05:45:11,358 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:46:20,000 : [('reg:1e-05', 87.57), ('reg:0.0001', 87.47), ('reg:0.001', 87.59), ('reg:0.01', 85.63)]
2019-02-13 05:46:20,000 : Validation : best param found is reg = 0.001 with score             87.59
2019-02-13 05:46:20,000 : Evaluating...
2019-02-13 05:46:40,439 : 
Dev acc : 87.6 Test acc : 86.7 for BIGRAMSHIFT classification

2019-02-13 05:46:40,446 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 05:46:41,028 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 05:46:41,095 : loading BERT mode bert-base-uncased
2019-02-13 05:46:41,095 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:46:41,127 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:46:41,127 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp51qhxb8d
2019-02-13 05:46:43,555 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:46:45,060 : Computing embeddings for train/dev/test
2019-02-13 05:49:36,281 : Computed embeddings
2019-02-13 05:49:36,281 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:50:18,526 : [('reg:1e-05', 89.5), ('reg:0.0001', 89.56), ('reg:0.001', 89.58), ('reg:0.01', 89.76)]
2019-02-13 05:50:18,526 : Validation : best param found is reg = 0.01 with score             89.76
2019-02-13 05:50:18,526 : Evaluating...
2019-02-13 05:50:34,823 : 
Dev acc : 89.8 Test acc : 87.6 for TENSE classification

2019-02-13 05:50:34,830 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 05:50:35,407 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 05:50:35,470 : loading BERT mode bert-base-uncased
2019-02-13 05:50:35,471 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:50:35,496 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:50:35,497 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp79avowhx
2019-02-13 05:50:37,938 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:50:39,533 : Computing embeddings for train/dev/test
2019-02-13 05:53:44,803 : Computed embeddings
2019-02-13 05:53:44,803 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:54:32,025 : [('reg:1e-05', 81.42), ('reg:0.0001', 81.5), ('reg:0.001', 81.51), ('reg:0.01', 79.55)]
2019-02-13 05:54:32,025 : Validation : best param found is reg = 0.001 with score             81.51
2019-02-13 05:54:32,025 : Evaluating...
2019-02-13 05:54:51,990 : 
Dev acc : 81.5 Test acc : 81.0 for SUBJNUMBER classification

2019-02-13 05:54:51,997 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 05:54:52,437 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 05:54:52,504 : loading BERT mode bert-base-uncased
2019-02-13 05:54:52,504 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:54:52,532 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:54:52,532 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfz7qi6bd
2019-02-13 05:54:54,960 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:54:56,418 : Computing embeddings for train/dev/test
2019-02-13 05:58:00,186 : Computed embeddings
2019-02-13 05:58:00,186 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:59:35,858 : [('reg:1e-05', 75.88), ('reg:0.0001', 75.9), ('reg:0.001', 75.17), ('reg:0.01', 75.87)]
2019-02-13 05:59:35,858 : Validation : best param found is reg = 0.0001 with score             75.9
2019-02-13 05:59:35,858 : Evaluating...
2019-02-13 05:59:56,446 : 
Dev acc : 75.9 Test acc : 75.9 for OBJNUMBER classification

2019-02-13 05:59:56,453 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 05:59:56,846 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 05:59:56,915 : loading BERT mode bert-base-uncased
2019-02-13 05:59:56,915 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:59:57,040 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:59:57,040 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2kdt8lfx
2019-02-13 05:59:59,473 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:00:00,929 : Computing embeddings for train/dev/test
2019-02-13 06:03:00,593 : Computed embeddings
2019-02-13 06:03:00,593 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:04:00,403 : [('reg:1e-05', 62.74), ('reg:0.0001', 62.78), ('reg:0.001', 62.75), ('reg:0.01', 61.74)]
2019-02-13 06:04:00,404 : Validation : best param found is reg = 0.0001 with score             62.78
2019-02-13 06:04:00,404 : Evaluating...
2019-02-13 06:04:13,159 : 
Dev acc : 62.8 Test acc : 61.2 for ODDMANOUT classification

2019-02-13 06:04:13,167 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 06:04:13,779 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 06:04:13,856 : loading BERT mode bert-base-uncased
2019-02-13 06:04:13,856 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:04:13,887 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:04:13,887 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz16oa4mn
2019-02-13 06:04:16,320 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:04:17,793 : Computing embeddings for train/dev/test
2019-02-13 06:07:16,795 : Computed embeddings
2019-02-13 06:07:16,795 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:08:29,450 : [('reg:1e-05', 62.31), ('reg:0.0001', 62.29), ('reg:0.001', 61.79), ('reg:0.01', 58.07)]
2019-02-13 06:08:29,450 : Validation : best param found is reg = 1e-05 with score             62.31
2019-02-13 06:08:29,450 : Evaluating...
2019-02-13 06:08:54,210 : 
Dev acc : 62.3 Test acc : 61.2 for COORDINATIONINVERSION classification

2019-02-13 06:08:54,218 : {'STS12': {'MSRpar': {'pearson': (0.31595073731379864, 7.558803380977722e-19), 'spearman': SpearmanrResult(correlation=0.3507040638809848, pvalue=3.9714507140897083e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5575935603843336, 1.7073410073756885e-62), 'spearman': SpearmanrResult(correlation=0.5725552750595269, pvalue=1.53308792654326e-66), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49336720517585514, 1.5673651713872784e-29), 'spearman': SpearmanrResult(correlation=0.5902238890023571, pvalue=1.9911188933723535e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5097771563156831, 7.52352844503072e-51), 'spearman': SpearmanrResult(correlation=0.5336071085313794, pvalue=2.0252427364855171e-56), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5582401104401848, 4.575928216265622e-34), 'spearman': SpearmanrResult(correlation=0.4884534453217214, pvalue=2.580443141559188e-25), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48698575392597104, 'wmean': 0.478341197474811}, 'spearman': {'mean': 0.507108756359194, 'wmean': 0.501433566711508}}}, 'STS13': {'FNWN': {'pearson': (0.2562418521418283, 0.0003722975169509068), 'spearman': SpearmanrResult(correlation=0.2734701485595284, pvalue=0.00014038489993970096), 'nsamples': 189}, 'headlines': {'pearson': (0.6486541597373371, 9.189989167488396e-91), 'spearman': SpearmanrResult(correlation=0.636128120523308, pvalue=2.6476590201321118e-86), 'nsamples': 750}, 'OnWN': {'pearson': (0.5589889484116807, 2.0054232116690815e-47), 'spearman': SpearmanrResult(correlation=0.5501994661327523, pvalue=1.0427125011227551e-45), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.487961653430282, 'wmean': 0.5656754199445075}, 'spearman': {'mean': 0.48659924507186286, 'wmean': 0.5582958993138039}}}, 'STS14': {'deft-forum': {'pearson': (0.34592294519944955, 4.28201422817043e-14), 'spearman': SpearmanrResult(correlation=0.3487129913932867, pvalue=2.5934379125233717e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7385992305902515, 5.798374670524147e-53), 'spearman': SpearmanrResult(correlation=0.7196459411378964, pvalue=3.933222005575042e-49), 'nsamples': 300}, 'headlines': {'pearson': (0.5839210339835171, 9.320762298251137e-70), 'spearman': SpearmanrResult(correlation=0.5494007346998849, pvalue=2.3041472469995847e-60), 'nsamples': 750}, 'images': {'pearson': (0.47364153869089815, 3.3518920910544224e-43), 'spearman': SpearmanrResult(correlation=0.47191058458384033, pvalue=7.399996796750927e-43), 'nsamples': 750}, 'OnWN': {'pearson': (0.6656985461655095, 3.512442447297624e-97), 'spearman': SpearmanrResult(correlation=0.6875563420399124, pvalue=4.635648706071364e-106), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6037703115280189, 1.0922525070519205e-75), 'spearman': SpearmanrResult(correlation=0.548651511870599, pvalue=3.5841178970132267e-60), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5685922676929408, 'wmean': 0.5660049779447428}, 'spearman': {'mean': 0.5543130176209033, 'wmean': 0.5509210688970735}}}, 'STS15': {'answers-forums': {'pearson': (0.5245719182638092, 6.7049828787197625e-28), 'spearman': SpearmanrResult(correlation=0.5147618067574177, pvalue=9.243928217784293e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.6058567957143661, 2.4572071474216956e-76), 'spearman': SpearmanrResult(correlation=0.6241363429727577, pvalue=3.1828516395414385e-82), 'nsamples': 750}, 'belief': {'pearson': (0.5874884897138652, 3.597296101081283e-36), 'spearman': SpearmanrResult(correlation=0.6151998962123019, pvalue=2.0204993361955903e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6511688841158828, 1.1027526400754407e-91), 'spearman': SpearmanrResult(correlation=0.6538806726095209, pvalue=1.0954748136179296e-92), 'nsamples': 750}, 'images': {'pearson': (0.6498204534175535, 3.446131844872639e-91), 'spearman': SpearmanrResult(correlation=0.6601115411538103, pvalue=4.961797588513171e-95), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6037813082450953, 'wmean': 0.61571908430916}, 'spearman': {'mean': 0.6136180519411617, 'wmean': 0.6257773520552372}}}, 'STS16': {'answer-answer': {'pearson': (0.5253662947487908, 2.0058294772970947e-19), 'spearman': SpearmanrResult(correlation=0.5052147186561342, pvalue=7.351861596622249e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6545604703691251, 7.641143469634103e-32), 'spearman': SpearmanrResult(correlation=0.6575242208239908, pvalue=3.273881386382618e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7183139656308635, 8.696842494834758e-38), 'spearman': SpearmanrResult(correlation=0.7299194447868416, pvalue=1.5232442712024837e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.7807919548123804, 2.41961672242791e-51), 'spearman': SpearmanrResult(correlation=0.8119908805576954, pvalue=1.6279586530901575e-58), 'nsamples': 244}, 'question-question': {'pearson': (0.42304662788595127, 1.7547804757258025e-10), 'spearman': SpearmanrResult(correlation=0.4298998295006182, pvalue=8.249168668870685e-11), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6204158626894222, 'wmean': 0.6244273105274775}, 'spearman': {'mean': 0.6269098188650559, 'wmean': 0.6306107765990829}}}, 'MR': {'devacc': 74.26, 'acc': 75.56, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 78.88, 'acc': 75.44, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.38, 'acc': 86.58, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.97, 'acc': 93.7, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.52, 'acc': 83.75, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.87, 'acc': 45.7, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 75.7, 'acc': 88.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.01, 'acc': 60.35, 'f1': 61.62, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.6, 'acc': 75.34, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.789778892712242, 'pearson': 0.7838590893186084, 'spearman': 0.7140007001331513, 'mse': 0.3956690813288107, 'yhat': array([3.08877657, 3.73863326, 1.56991219, ..., 3.32772398, 4.39358179,
       4.97569228]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6229046588663187, 'pearson': 0.6397960231710759, 'spearman': 0.6345147084797617, 'mse': 1.4430591637292234, 'yhat': array([2.88225204, 1.1489056 , 1.24742531, ..., 3.89198362, 4.01768007,
       3.73548216]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 59.27, 'acc': 58.76, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 24.82, 'acc': 25.3, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.48, 'acc': 27.13, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.37, 'acc': 54.19, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.59, 'acc': 86.7, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.76, 'acc': 87.61, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.51, 'acc': 81.01, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.9, 'acc': 75.86, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.78, 'acc': 61.23, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 62.31, 'acc': 61.19, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 06:08:54,218 : ********************************************************************************
2019-02-13 06:08:54,218 : ********************************************************************************
2019-02-13 06:08:54,218 : ********************************************************************************
2019-02-13 06:08:54,218 : layer 12
2019-02-13 06:08:54,218 : ********************************************************************************
2019-02-13 06:08:54,218 : ********************************************************************************
2019-02-13 06:08:54,218 : ********************************************************************************
2019-02-13 06:08:54,307 : ***** Transfer task : STS12 *****


2019-02-13 06:08:54,320 : loading BERT mode bert-base-uncased
2019-02-13 06:08:54,320 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:08:54,337 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:08:54,337 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp84ixeg3t
2019-02-13 06:08:56,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:09:01,769 : MSRpar : pearson = 0.3293, spearman = 0.3646
2019-02-13 06:09:04,503 : MSRvid : pearson = 0.4814, spearman = 0.4985
2019-02-13 06:09:06,498 : SMTeuroparl : pearson = 0.4833, spearman = 0.5695
2019-02-13 06:09:09,175 : surprise.OnWN : pearson = 0.5282, spearman = 0.5469
2019-02-13 06:09:10,472 : surprise.SMTnews : pearson = 0.5668, spearman = 0.5071
2019-02-13 06:09:10,472 : ALL (weighted average) : Pearson = 0.4672,             Spearman = 0.4895
2019-02-13 06:09:10,472 : ALL (average) : Pearson = 0.4778,             Spearman = 0.4973

2019-02-13 06:09:10,472 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 06:09:10,480 : loading BERT mode bert-base-uncased
2019-02-13 06:09:10,480 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:09:10,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:09:10,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2vfkxfcf
2019-02-13 06:09:12,928 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:09:15,372 : FNWN : pearson = 0.2547, spearman = 0.2773
2019-02-13 06:09:17,501 : headlines : pearson = 0.6675, spearman = 0.6471
2019-02-13 06:09:19,121 : OnWN : pearson = 0.4801, spearman = 0.4808
2019-02-13 06:09:19,122 : ALL (weighted average) : Pearson = 0.5454,             Spearman = 0.5383
2019-02-13 06:09:19,122 : ALL (average) : Pearson = 0.4674,             Spearman = 0.4684

2019-02-13 06:09:19,122 : ***** Transfer task : STS14 *****


2019-02-13 06:09:19,140 : loading BERT mode bert-base-uncased
2019-02-13 06:09:19,140 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:09:19,190 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:09:19,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplrlb5xm_
2019-02-13 06:09:21,618 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:09:24,419 : deft-forum : pearson = 0.3504, spearman = 0.3455
2019-02-13 06:09:25,708 : deft-news : pearson = 0.7528, spearman = 0.7214
2019-02-13 06:09:27,934 : headlines : pearson = 0.6161, spearman = 0.5727
2019-02-13 06:09:30,238 : images : pearson = 0.4399, spearman = 0.4380
2019-02-13 06:09:32,552 : OnWN : pearson = 0.6272, spearman = 0.6495
2019-02-13 06:09:35,175 : tweet-news : pearson = 0.6140, spearman = 0.5610
2019-02-13 06:09:35,176 : ALL (weighted average) : Pearson = 0.5617,             Spearman = 0.5434
2019-02-13 06:09:35,176 : ALL (average) : Pearson = 0.5667,             Spearman = 0.5480

2019-02-13 06:09:35,176 : ***** Transfer task : STS15 *****


2019-02-13 06:09:35,208 : loading BERT mode bert-base-uncased
2019-02-13 06:09:35,208 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:09:35,225 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:09:35,226 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1uf19m17
2019-02-13 06:09:37,653 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:09:40,580 : answers-forums : pearson = 0.5328, spearman = 0.5148
2019-02-13 06:09:42,928 : answers-students : pearson = 0.5998, spearman = 0.6083
2019-02-13 06:09:44,500 : belief : pearson = 0.6006, spearman = 0.6129
2019-02-13 06:09:46,956 : headlines : pearson = 0.6795, spearman = 0.6763
2019-02-13 06:09:49,386 : images : pearson = 0.6061, spearman = 0.6144
2019-02-13 06:09:49,387 : ALL (weighted average) : Pearson = 0.6130,             Spearman = 0.6157
2019-02-13 06:09:49,387 : ALL (average) : Pearson = 0.6038,             Spearman = 0.6053

2019-02-13 06:09:49,387 : ***** Transfer task : STS16 *****


2019-02-13 06:09:49,459 : loading BERT mode bert-base-uncased
2019-02-13 06:09:49,459 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:09:49,477 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:09:49,477 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpisnpg7te
2019-02-13 06:09:51,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:09:54,246 : answer-answer : pearson = 0.5267, spearman = 0.5141
2019-02-13 06:09:55,060 : headlines : pearson = 0.6548, spearman = 0.6601
2019-02-13 06:09:55,942 : plagiarism : pearson = 0.7288, spearman = 0.7331
2019-02-13 06:09:57,042 : postediting : pearson = 0.7815, spearman = 0.8027
2019-02-13 06:09:57,744 : question-question : pearson = 0.4896, spearman = 0.4826
2019-02-13 06:09:57,744 : ALL (weighted average) : Pearson = 0.6387,             Spearman = 0.6410
2019-02-13 06:09:57,744 : ALL (average) : Pearson = 0.6363,             Spearman = 0.6385

2019-02-13 06:09:57,744 : ***** Transfer task : MR *****


2019-02-13 06:09:57,762 : loading BERT mode bert-base-uncased
2019-02-13 06:09:57,762 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:09:57,783 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:09:57,783 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1hpztt_9
2019-02-13 06:10:00,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:10:01,735 : Generating sentence embeddings
2019-02-13 06:10:25,664 : Generated sentence embeddings
2019-02-13 06:10:25,665 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 06:10:59,308 : Best param found at split 1: l2reg = 1e-05                 with score 76.2
2019-02-13 06:11:33,672 : Best param found at split 2: l2reg = 1e-05                 with score 74.0
2019-02-13 06:12:01,872 : Best param found at split 3: l2reg = 1e-05                 with score 71.07
2019-02-13 06:12:39,632 : Best param found at split 4: l2reg = 1e-05                 with score 70.94
2019-02-13 06:13:14,497 : Best param found at split 5: l2reg = 0.001                 with score 75.96
2019-02-13 06:13:16,394 : Dev acc : 73.63 Test acc : 74.68

2019-02-13 06:13:16,395 : ***** Transfer task : CR *****


2019-02-13 06:13:16,403 : loading BERT mode bert-base-uncased
2019-02-13 06:13:16,403 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:13:16,423 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:13:16,423 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5ja5w6ce
2019-02-13 06:13:18,863 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:13:20,347 : Generating sentence embeddings
2019-02-13 06:13:26,930 : Generated sentence embeddings
2019-02-13 06:13:26,930 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 06:13:36,513 : Best param found at split 1: l2reg = 0.0001                 with score 77.05
2019-02-13 06:13:45,565 : Best param found at split 2: l2reg = 1e-05                 with score 75.22
2019-02-13 06:13:55,003 : Best param found at split 3: l2reg = 1e-05                 with score 81.26
2019-02-13 06:14:04,733 : Best param found at split 4: l2reg = 0.0001                 with score 78.42
2019-02-13 06:14:13,473 : Best param found at split 5: l2reg = 0.0001                 with score 75.91
2019-02-13 06:14:13,822 : Dev acc : 77.57 Test acc : 73.32

2019-02-13 06:14:13,823 : ***** Transfer task : MPQA *****


2019-02-13 06:14:13,828 : loading BERT mode bert-base-uncased
2019-02-13 06:14:13,828 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:14:13,848 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:14:13,848 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvb_h0grd
2019-02-13 06:14:16,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:14:17,817 : Generating sentence embeddings
2019-02-13 06:14:31,216 : Generated sentence embeddings
2019-02-13 06:14:31,217 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 06:14:56,785 : Best param found at split 1: l2reg = 0.001                 with score 86.5
2019-02-13 06:15:22,718 : Best param found at split 2: l2reg = 1e-05                 with score 86.34
2019-02-13 06:15:50,977 : Best param found at split 3: l2reg = 1e-05                 with score 86.33
2019-02-13 06:16:20,992 : Best param found at split 4: l2reg = 0.0001                 with score 86.31
2019-02-13 06:16:34,851 : Best param found at split 5: l2reg = 0.01                 with score 85.96
2019-02-13 06:16:35,426 : Dev acc : 86.29 Test acc : 86.33

2019-02-13 06:16:35,427 : ***** Transfer task : SUBJ *****


2019-02-13 06:16:35,442 : loading BERT mode bert-base-uncased
2019-02-13 06:16:35,442 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:16:35,463 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:16:35,464 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxj6wtl9d
2019-02-13 06:16:37,910 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:16:39,340 : Generating sentence embeddings
2019-02-13 06:16:54,026 : Generated sentence embeddings
2019-02-13 06:16:54,026 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 06:17:09,101 : Best param found at split 1: l2reg = 0.001                 with score 93.69
2019-02-13 06:17:23,682 : Best param found at split 2: l2reg = 0.01                 with score 93.46
2019-02-13 06:17:47,326 : Best param found at split 3: l2reg = 0.0001                 with score 93.65
2019-02-13 06:18:11,859 : Best param found at split 4: l2reg = 0.0001                 with score 93.72
2019-02-13 06:18:37,915 : Best param found at split 5: l2reg = 0.0001                 with score 93.65
2019-02-13 06:18:39,488 : Dev acc : 93.63 Test acc : 93.42

2019-02-13 06:18:39,489 : ***** Transfer task : SST Binary classification *****


2019-02-13 06:18:39,615 : loading BERT mode bert-base-uncased
2019-02-13 06:18:39,615 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:18:39,639 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:18:39,640 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg8m6j1y3
2019-02-13 06:18:42,097 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:18:43,550 : Computing embedding for train
2019-02-13 06:19:44,818 : Computed train embeddings
2019-02-13 06:19:44,818 : Computing embedding for dev
2019-02-13 06:19:45,928 : Computed dev embeddings
2019-02-13 06:19:45,928 : Computing embedding for test
2019-02-13 06:19:48,232 : Computed test embeddings
2019-02-13 06:19:48,232 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:20:08,733 : [('reg:1e-05', 83.03), ('reg:0.0001', 82.68), ('reg:0.001', 81.77), ('reg:0.01', 77.18)]
2019-02-13 06:20:08,733 : Validation : best param found is reg = 1e-05 with score             83.03
2019-02-13 06:20:08,734 : Evaluating...
2019-02-13 06:20:17,396 : 
Dev acc : 83.03 Test acc : 81.71 for             SST Binary classification

2019-02-13 06:20:17,397 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 06:20:17,447 : loading BERT mode bert-base-uncased
2019-02-13 06:20:17,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:20:17,469 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:20:17,469 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxyt_j1o0
2019-02-13 06:20:19,902 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:20:21,365 : Computing embedding for train
2019-02-13 06:20:33,011 : Computed train embeddings
2019-02-13 06:20:33,011 : Computing embedding for dev
2019-02-13 06:20:34,585 : Computed dev embeddings
2019-02-13 06:20:34,585 : Computing embedding for test
2019-02-13 06:20:37,748 : Computed test embeddings
2019-02-13 06:20:37,748 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:20:42,804 : [('reg:1e-05', 36.88), ('reg:0.0001', 41.78), ('reg:0.001', 40.78), ('reg:0.01', 32.24)]
2019-02-13 06:20:42,804 : Validation : best param found is reg = 0.0001 with score             41.78
2019-02-13 06:20:42,804 : Evaluating...
2019-02-13 06:20:43,996 : 
Dev acc : 41.78 Test acc : 42.62 for             SST Fine-Grained classification

2019-02-13 06:20:43,996 : ***** Transfer task : TREC *****


2019-02-13 06:20:44,009 : loading BERT mode bert-base-uncased
2019-02-13 06:20:44,010 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:20:44,028 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:20:44,028 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0117jzwd
2019-02-13 06:20:46,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:20:54,114 : Computed train embeddings
2019-02-13 06:20:54,661 : Computed test embeddings
2019-02-13 06:20:54,661 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 06:21:09,551 : [('reg:1e-05', 67.31), ('reg:0.0001', 70.61), ('reg:0.001', 68.87), ('reg:0.01', 63.4)]
2019-02-13 06:21:09,551 : Cross-validation : best param found is reg = 0.0001             with score 70.61
2019-02-13 06:21:09,551 : Evaluating...
2019-02-13 06:21:10,486 : 
Dev acc : 70.61 Test acc : 81.4             for TREC

2019-02-13 06:21:10,486 : ***** Transfer task : MRPC *****


2019-02-13 06:21:10,508 : loading BERT mode bert-base-uncased
2019-02-13 06:21:10,508 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:21:10,529 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:21:10,529 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnt1z1lbl
2019-02-13 06:21:12,956 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:21:14,493 : Computing embedding for train
2019-02-13 06:21:28,848 : Computed train embeddings
2019-02-13 06:21:28,848 : Computing embedding for test
2019-02-13 06:21:35,344 : Computed test embeddings
2019-02-13 06:21:35,360 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 06:21:45,702 : [('reg:1e-05', 72.2), ('reg:0.0001', 72.35), ('reg:0.001', 71.52), ('reg:0.01', 73.01)]
2019-02-13 06:21:45,702 : Cross-validation : best param found is reg = 0.01             with score 73.01
2019-02-13 06:21:45,702 : Evaluating...
2019-02-13 06:21:46,267 : Dev acc : 73.01 Test acc 72.12; Test F1 81.92 for MRPC.

2019-02-13 06:21:46,267 : ***** Transfer task : SICK-Entailment*****


2019-02-13 06:21:46,330 : loading BERT mode bert-base-uncased
2019-02-13 06:21:46,330 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:21:46,349 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:21:46,349 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwfvqkzfy
2019-02-13 06:21:48,778 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:21:50,263 : Computing embedding for train
2019-02-13 06:21:59,775 : Computed train embeddings
2019-02-13 06:21:59,775 : Computing embedding for dev
2019-02-13 06:22:00,627 : Computed dev embeddings
2019-02-13 06:22:00,627 : Computing embedding for test
2019-02-13 06:22:07,769 : Computed test embeddings
2019-02-13 06:22:07,797 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:22:09,479 : [('reg:1e-05', 75.6), ('reg:0.0001', 75.8), ('reg:0.001', 71.4), ('reg:0.01', 76.4)]
2019-02-13 06:22:09,479 : Validation : best param found is reg = 0.01 with score             76.4
2019-02-13 06:22:09,479 : Evaluating...
2019-02-13 06:22:10,097 : 
Dev acc : 76.4 Test acc : 75.42 for                        SICK entailment

2019-02-13 06:22:10,097 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 06:22:10,124 : loading BERT mode bert-base-uncased
2019-02-13 06:22:10,124 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:22:10,181 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:22:10,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdu6ckgea
2019-02-13 06:22:12,617 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:22:14,031 : Computing embedding for train
2019-02-13 06:22:20,477 : Computed train embeddings
2019-02-13 06:22:20,477 : Computing embedding for dev
2019-02-13 06:22:21,299 : Computed dev embeddings
2019-02-13 06:22:21,299 : Computing embedding for test
2019-02-13 06:22:28,276 : Computed test embeddings
2019-02-13 06:22:47,214 : Dev : Pearson 0.791600232068805
2019-02-13 06:22:47,214 : Test : Pearson 0.7878980212060073 Spearman 0.7127124050973181 MSE 0.3864142098810773                        for SICK Relatedness

2019-02-13 06:22:47,215 : 

***** Transfer task : STSBenchmark*****


2019-02-13 06:22:47,254 : loading BERT mode bert-base-uncased
2019-02-13 06:22:47,255 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:22:47,284 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:22:47,284 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbldlxqm1
2019-02-13 06:22:49,719 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:22:51,239 : Computing embedding for train
2019-02-13 06:23:02,095 : Computed train embeddings
2019-02-13 06:23:02,095 : Computing embedding for dev
2019-02-13 06:23:05,314 : Computed dev embeddings
2019-02-13 06:23:05,315 : Computing embedding for test
2019-02-13 06:23:08,149 : Computed test embeddings
2019-02-13 06:23:55,654 : Dev : Pearson 0.6560547998901856
2019-02-13 06:23:55,654 : Test : Pearson 0.6257516252582351 Spearman 0.6219055568350936 MSE 1.5263952258928792                        for SICK Relatedness

2019-02-13 06:23:55,654 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 06:23:55,974 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 06:23:55,984 : loading BERT mode bert-base-uncased
2019-02-13 06:23:55,984 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:23:56,007 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:23:56,007 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpls8pwbmk
2019-02-13 06:23:58,435 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:23:59,916 : Computing embeddings for train/dev/test
2019-02-13 06:26:05,353 : Computed embeddings
2019-02-13 06:26:05,353 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:27:12,425 : [('reg:1e-05', 61.21), ('reg:0.0001', 61.57), ('reg:0.001', 53.61), ('reg:0.01', 52.57)]
2019-02-13 06:27:12,425 : Validation : best param found is reg = 0.0001 with score             61.57
2019-02-13 06:27:12,425 : Evaluating...
2019-02-13 06:27:32,650 : 
Dev acc : 61.6 Test acc : 60.9 for LENGTH classification

2019-02-13 06:27:32,660 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 06:27:33,011 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 06:27:33,057 : loading BERT mode bert-base-uncased
2019-02-13 06:27:33,057 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:27:33,085 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:27:33,085 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv_swccxb
2019-02-13 06:27:35,514 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:27:37,000 : Computing embeddings for train/dev/test
2019-02-13 06:29:27,534 : Computed embeddings
2019-02-13 06:29:27,534 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:30:41,217 : [('reg:1e-05', 12.63), ('reg:0.0001', 4.32), ('reg:0.001', 0.53), ('reg:0.01', 0.19)]
2019-02-13 06:30:41,217 : Validation : best param found is reg = 1e-05 with score             12.63
2019-02-13 06:30:41,217 : Evaluating...
2019-02-13 06:31:04,646 : 
Dev acc : 12.6 Test acc : 12.5 for WORDCONTENT classification

2019-02-13 06:31:04,656 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 06:31:05,004 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 06:31:05,068 : loading BERT mode bert-base-uncased
2019-02-13 06:31:05,068 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:31:05,164 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:31:05,164 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx9khduyb
2019-02-13 06:31:07,620 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:31:09,094 : Computing embeddings for train/dev/test
2019-02-13 06:32:55,521 : Computed embeddings
2019-02-13 06:32:55,521 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:33:23,017 : [('reg:1e-05', 28.33), ('reg:0.0001', 27.12), ('reg:0.001', 23.05), ('reg:0.01', 22.28)]
2019-02-13 06:33:23,017 : Validation : best param found is reg = 1e-05 with score             28.33
2019-02-13 06:33:23,017 : Evaluating...
2019-02-13 06:33:29,475 : 
Dev acc : 28.3 Test acc : 27.9 for DEPTH classification

2019-02-13 06:33:29,485 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 06:33:30,047 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 06:33:30,109 : loading BERT mode bert-base-uncased
2019-02-13 06:33:30,109 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:33:30,137 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:33:30,137 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxhj_45gh
2019-02-13 06:33:32,565 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:33:33,964 : Computing embeddings for train/dev/test
2019-02-13 06:34:41,024 : Computed embeddings
2019-02-13 06:34:41,024 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:35:11,100 : [('reg:1e-05', 54.79), ('reg:0.0001', 53.72), ('reg:0.001', 45.32), ('reg:0.01', 34.56)]
2019-02-13 06:35:11,100 : Validation : best param found is reg = 1e-05 with score             54.79
2019-02-13 06:35:11,100 : Evaluating...
2019-02-13 06:35:18,843 : 
Dev acc : 54.8 Test acc : 54.8 for TOPCONSTITUENTS classification

2019-02-13 06:35:18,853 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 06:35:19,196 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 06:35:19,261 : loading BERT mode bert-base-uncased
2019-02-13 06:35:19,261 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:35:19,381 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:35:19,381 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1zzyn9tt
2019-02-13 06:35:21,861 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:35:23,269 : Computing embeddings for train/dev/test
2019-02-13 06:36:35,726 : Computed embeddings
2019-02-13 06:36:35,726 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:37:05,601 : [('reg:1e-05', 85.2), ('reg:0.0001', 85.16), ('reg:0.001', 85.08), ('reg:0.01', 84.38)]
2019-02-13 06:37:05,601 : Validation : best param found is reg = 1e-05 with score             85.2
2019-02-13 06:37:05,601 : Evaluating...
2019-02-13 06:37:13,084 : 
Dev acc : 85.2 Test acc : 84.5 for BIGRAMSHIFT classification

2019-02-13 06:37:13,094 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 06:37:13,651 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 06:37:13,716 : loading BERT mode bert-base-uncased
2019-02-13 06:37:13,716 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:37:13,746 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:37:13,746 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp17ynfz8x
2019-02-13 06:37:16,179 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:37:17,566 : Computing embeddings for train/dev/test
2019-02-13 06:38:28,282 : Computed embeddings
2019-02-13 06:38:28,282 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:38:54,229 : [('reg:1e-05', 89.52), ('reg:0.0001', 89.55), ('reg:0.001', 89.59), ('reg:0.01', 89.62)]
2019-02-13 06:38:54,229 : Validation : best param found is reg = 0.01 with score             89.62
2019-02-13 06:38:54,229 : Evaluating...
2019-02-13 06:39:00,750 : 
Dev acc : 89.6 Test acc : 88.3 for TENSE classification

2019-02-13 06:39:00,761 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 06:39:01,345 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 06:39:01,410 : loading BERT mode bert-base-uncased
2019-02-13 06:39:01,410 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:39:01,440 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:39:01,440 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgkrmpgpf
2019-02-13 06:39:03,845 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:39:05,253 : Computing embeddings for train/dev/test
2019-02-13 06:40:20,550 : Computed embeddings
2019-02-13 06:40:20,550 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:40:46,524 : [('reg:1e-05', 76.74), ('reg:0.0001', 76.74), ('reg:0.001', 77.69), ('reg:0.01', 78.0)]
2019-02-13 06:40:46,524 : Validation : best param found is reg = 0.01 with score             78.0
2019-02-13 06:40:46,524 : Evaluating...
2019-02-13 06:40:52,925 : 
Dev acc : 78.0 Test acc : 77.4 for SUBJNUMBER classification

2019-02-13 06:40:52,935 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 06:40:53,364 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 06:40:53,431 : loading BERT mode bert-base-uncased
2019-02-13 06:40:53,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:40:53,458 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:40:53,458 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvn5ic4_d
2019-02-13 06:40:55,872 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:40:57,267 : Computing embeddings for train/dev/test
2019-02-13 06:42:11,212 : Computed embeddings
2019-02-13 06:42:11,212 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:42:39,120 : [('reg:1e-05', 73.68), ('reg:0.0001', 73.7), ('reg:0.001', 74.17), ('reg:0.01', 75.4)]
2019-02-13 06:42:39,120 : Validation : best param found is reg = 0.01 with score             75.4
2019-02-13 06:42:39,120 : Evaluating...
2019-02-13 06:42:46,726 : 
Dev acc : 75.4 Test acc : 76.5 for OBJNUMBER classification

2019-02-13 06:42:46,736 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 06:42:47,126 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 06:42:47,195 : loading BERT mode bert-base-uncased
2019-02-13 06:42:47,195 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:42:47,316 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:42:47,317 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb5pnla93
2019-02-13 06:42:49,741 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:42:51,118 : Computing embeddings for train/dev/test
2019-02-13 06:44:15,838 : Computed embeddings
2019-02-13 06:44:15,838 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:44:35,617 : [('reg:1e-05', 61.77), ('reg:0.0001', 61.74), ('reg:0.001', 61.59), ('reg:0.01', 60.5)]
2019-02-13 06:44:35,617 : Validation : best param found is reg = 1e-05 with score             61.77
2019-02-13 06:44:35,617 : Evaluating...
2019-02-13 06:44:39,441 : 
Dev acc : 61.8 Test acc : 61.0 for ODDMANOUT classification

2019-02-13 06:44:39,450 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 06:44:39,845 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 06:44:39,921 : loading BERT mode bert-base-uncased
2019-02-13 06:44:39,921 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:44:40,048 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:44:40,048 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg_n05ljv
2019-02-13 06:44:42,466 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:44:43,853 : Computing embeddings for train/dev/test
2019-02-13 06:46:08,376 : Computed embeddings
2019-02-13 06:46:08,376 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:46:32,583 : [('reg:1e-05', 57.73), ('reg:0.0001', 57.72), ('reg:0.001', 57.35), ('reg:0.01', 54.74)]
2019-02-13 06:46:32,583 : Validation : best param found is reg = 1e-05 with score             57.73
2019-02-13 06:46:32,584 : Evaluating...
2019-02-13 06:46:37,161 : 
Dev acc : 57.7 Test acc : 57.2 for COORDINATIONINVERSION classification

2019-02-13 06:46:37,172 : {'STS12': {'MSRpar': {'pearson': (0.3292701396650404, 2.0082899770610228e-20), 'spearman': SpearmanrResult(correlation=0.3645991496615272, pvalue=5.384063356572047e-25), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4813987758111209, 9.109705035724574e-45), 'spearman': SpearmanrResult(correlation=0.4985232629312539, pvalue=2.277015508998401e-48), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48329305308622844, 3.063134918867712e-28), 'spearman': SpearmanrResult(correlation=0.5694600433425483, pvalue=8.228919429330057e-41), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5281831031648189, 4.1063695769363556e-55), 'spearman': SpearmanrResult(correlation=0.5468906077520795, pvalue=1.0079396415939168e-59), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5667964669073319, 2.7537967741220236e-35), 'spearman': SpearmanrResult(correlation=0.5071083002786961, pvalue=1.8489724027715693e-27), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4777883077269081, 'wmean': 0.4672208222790667}, 'spearman': {'mean': 0.497316272793221, 'wmean': 0.48945564252383367}}}, 'STS13': {'FNWN': {'pearson': (0.2547090729651854, 0.0004047635384451121), 'spearman': SpearmanrResult(correlation=0.2773233967734777, pvalue=0.000111853258612407), 'nsamples': 189}, 'headlines': {'pearson': (0.6675186688813312, 6.840119875134273e-98), 'spearman': SpearmanrResult(correlation=0.647096319780582, pvalue=3.383716129959606e-90), 'nsamples': 750}, 'OnWN': {'pearson': (0.480093688367714, 1.0991639033884098e-33), 'spearman': SpearmanrResult(correlation=0.480755410257724, pvalue=8.712364702157595e-34), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.46744047673807687, 'wmean': 0.545407717083804}, 'spearman': {'mean': 0.4683917089372613, 'wmean': 0.538293431320138}}}, 'STS14': {'deft-forum': {'pearson': (0.35035529508364155, 1.926069375099467e-14), 'spearman': SpearmanrResult(correlation=0.34548276624588536, pvalue=4.632417035518106e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7528417437911562, 4.545910599647106e-56), 'spearman': SpearmanrResult(correlation=0.7213971454836918, pvalue=1.795140763506984e-49), 'nsamples': 300}, 'headlines': {'pearson': (0.6161442294743605, 1.3324271949755748e-79), 'spearman': SpearmanrResult(correlation=0.572742414916247, pvalue=1.360275788455499e-66), 'nsamples': 750}, 'images': {'pearson': (0.43993237471562596, 7.60251458257844e-37), 'spearman': SpearmanrResult(correlation=0.4380484553158856, pvalue=1.642820461183141e-36), 'nsamples': 750}, 'OnWN': {'pearson': (0.6271625534438446, 3.0906491341853625e-83), 'spearman': SpearmanrResult(correlation=0.649525712451253, pvalue=4.4173498249072936e-91), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6140234168920401, 6.423658931609656e-79), 'spearman': SpearmanrResult(correlation=0.5609741339349115, pvalue=2.1674248283443142e-63), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5667432689001115, 'wmean': 0.5617224898185038}, 'spearman': {'mean': 0.5480284380579791, 'wmean': 0.543427846911861}}}, 'STS15': {'answers-forums': {'pearson': (0.5327909656289264, 6.96265572121149e-29), 'spearman': SpearmanrResult(correlation=0.5148010431291269, pvalue=9.148976077560888e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.5997667450283266, 1.8541957111838487e-74), 'spearman': SpearmanrResult(correlation=0.6083388479983969, pvalue=4.106282026000495e-77), 'nsamples': 750}, 'belief': {'pearson': (0.6005899199295938, 3.9634138212959947e-38), 'spearman': SpearmanrResult(correlation=0.6128790328719657, pvalue=4.760569352510402e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6795381727297876, 1.0311233246974753e-102), 'spearman': SpearmanrResult(correlation=0.6762720984904763, pvalue=2.2195080252420848e-101), 'nsamples': 750}, 'images': {'pearson': (0.6061257288995658, 2.025744543180625e-76), 'spearman': SpearmanrResult(correlation=0.6144108422285541, pvalue=4.823656557651322e-79), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.60376230644324, 'wmean': 0.613030272359235}, 'spearman': {'mean': 0.605340372943704, 'wmean': 0.6157154566794933}}}, 'STS16': {'answer-answer': {'pearson': (0.5266905972550545, 1.5697263075627333e-19), 'spearman': SpearmanrResult(correlation=0.5140943645052517, pvalue=1.548617135385278e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6548133333694409, 7.110732956322236e-32), 'spearman': SpearmanrResult(correlation=0.6600701117726094, pvalue=1.5686656550484838e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7288117716779764, 2.2612265035682826e-39), 'spearman': SpearmanrResult(correlation=0.7331262358203738, pvalue=4.7997697084648476e-40), 'nsamples': 230}, 'postediting': {'pearson': (0.781548413835633, 1.674918238970018e-51), 'spearman': SpearmanrResult(correlation=0.8026926099052625, pvalue=3.0403669935606535e-56), 'nsamples': 244}, 'question-question': {'pearson': (0.48956001899413354, 5.383213758533612e-14), 'spearman': SpearmanrResult(correlation=0.4825648195680615, pvalue=1.371784143823528e-13), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6362848270264477, 'wmean': 0.6386766409303353}, 'spearman': {'mean': 0.6385096283143118, 'wmean': 0.6410366819232787}}}, 'MR': {'devacc': 73.63, 'acc': 74.68, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 77.57, 'acc': 73.32, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.29, 'acc': 86.33, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.63, 'acc': 93.42, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.03, 'acc': 81.71, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.78, 'acc': 42.62, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.61, 'acc': 81.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.01, 'acc': 72.12, 'f1': 81.92, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.4, 'acc': 75.42, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.791600232068805, 'pearson': 0.7878980212060073, 'spearman': 0.7127124050973181, 'mse': 0.3864142098810773, 'yhat': array([2.5979958 , 4.19721743, 2.47941798, ..., 3.33426063, 4.26706675,
       4.87567802]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6560547998901856, 'pearson': 0.6257516252582351, 'spearman': 0.6219055568350936, 'mse': 1.5263952258928792, 'yhat': array([2.08333733, 1.79652376, 1.90633487, ..., 3.95513819, 3.66242626,
       3.20695483]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 61.57, 'acc': 60.94, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 12.63, 'acc': 12.5, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.33, 'acc': 27.88, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.79, 'acc': 54.76, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.2, 'acc': 84.55, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.62, 'acc': 88.34, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 78.0, 'acc': 77.42, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.4, 'acc': 76.55, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 61.77, 'acc': 60.98, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.73, 'acc': 57.19, 'ndev': 10002, 'ntest': 10002}}
