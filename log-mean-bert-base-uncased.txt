2019-02-12 17:42:24,700 : ********************************************************************************
2019-02-12 17:42:24,700 : ********************************************************************************
2019-02-12 17:42:24,700 : ********************************************************************************
2019-02-12 17:42:24,700 : layer 0
2019-02-12 17:42:24,702 : ********************************************************************************
2019-02-12 17:42:24,702 : ********************************************************************************
2019-02-12 17:42:24,702 : ********************************************************************************
2019-02-12 17:42:24,702 : ***** Transfer task : STS12 *****


2019-02-12 17:42:24,792 : loading BERT mode bert-base-uncased
2019-02-12 17:42:24,792 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:42:24,829 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:42:24,829 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4v670c7t
2019-02-12 17:42:27,202 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:42:33,338 : MSRpar : pearson = 0.3761, spearman = 0.4191
2019-02-12 17:42:33,981 : MSRvid : pearson = 0.5699, spearman = 0.5753
2019-02-12 17:42:34,519 : SMTeuroparl : pearson = 0.4763, spearman = 0.5888
2019-02-12 17:42:35,476 : surprise.OnWN : pearson = 0.6554, spearman = 0.6771
2019-02-12 17:42:36,012 : surprise.SMTnews : pearson = 0.4940, spearman = 0.4405
2019-02-12 17:42:36,012 : ALL (weighted average) : Pearson = 0.5202,             Spearman = 0.5469
2019-02-12 17:42:36,012 : ALL (average) : Pearson = 0.5143,             Spearman = 0.5402

2019-02-12 17:42:36,012 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 17:42:36,021 : loading BERT mode bert-base-uncased
2019-02-12 17:42:36,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:42:36,039 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:42:36,039 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd5thtzth
2019-02-12 17:42:38,325 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:42:40,283 : FNWN : pearson = 0.3676, spearman = 0.3646
2019-02-12 17:42:41,014 : headlines : pearson = 0.6577, spearman = 0.6415
2019-02-12 17:42:41,564 : OnWN : pearson = 0.4054, spearman = 0.4594
2019-02-12 17:42:41,564 : ALL (weighted average) : Pearson = 0.5268,             Spearman = 0.5385
2019-02-12 17:42:41,564 : ALL (average) : Pearson = 0.4769,             Spearman = 0.4885

2019-02-12 17:42:41,564 : ***** Transfer task : STS14 *****


2019-02-12 17:42:41,584 : loading BERT mode bert-base-uncased
2019-02-12 17:42:41,584 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:42:41,632 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:42:41,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc6pdhmq7
2019-02-12 17:42:43,913 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:42:45,818 : deft-forum : pearson = 0.3369, spearman = 0.3553
2019-02-12 17:42:46,405 : deft-news : pearson = 0.6840, spearman = 0.6730
2019-02-12 17:42:47,224 : headlines : pearson = 0.6223, spearman = 0.5877
2019-02-12 17:42:48,008 : images : pearson = 0.5857, spearman = 0.5889
2019-02-12 17:42:48,801 : OnWN : pearson = 0.5576, spearman = 0.6217
2019-02-12 17:42:49,840 : tweet-news : pearson = 0.5937, spearman = 0.5864
2019-02-12 17:42:49,840 : ALL (weighted average) : Pearson = 0.5670,             Spearman = 0.5734
2019-02-12 17:42:49,840 : ALL (average) : Pearson = 0.5634,             Spearman = 0.5688

2019-02-12 17:42:49,840 : ***** Transfer task : STS15 *****


2019-02-12 17:42:49,876 : loading BERT mode bert-base-uncased
2019-02-12 17:42:49,877 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:42:49,894 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:42:49,894 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpatqfubgt
2019-02-12 17:42:52,171 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:42:54,230 : answers-forums : pearson = 0.4531, spearman = 0.4482
2019-02-12 17:42:55,013 : answers-students : pearson = 0.6952, spearman = 0.7051
2019-02-12 17:42:55,725 : belief : pearson = 0.5276, spearman = 0.5268
2019-02-12 17:42:56,563 : headlines : pearson = 0.6782, spearman = 0.6724
2019-02-12 17:42:57,364 : images : pearson = 0.6894, spearman = 0.7051
2019-02-12 17:42:57,364 : ALL (weighted average) : Pearson = 0.6383,             Spearman = 0.6425
2019-02-12 17:42:57,364 : ALL (average) : Pearson = 0.6087,             Spearman = 0.6115

2019-02-12 17:42:57,364 : ***** Transfer task : STS16 *****


2019-02-12 17:42:57,479 : loading BERT mode bert-base-uncased
2019-02-12 17:42:57,479 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:42:57,495 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:42:57,495 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi34e_5xf
2019-02-12 17:42:59,771 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:43:01,474 : answer-answer : pearson = 0.4588, spearman = 0.5082
2019-02-12 17:43:01,731 : headlines : pearson = 0.6884, spearman = 0.6927
2019-02-12 17:43:02,058 : plagiarism : pearson = 0.6704, spearman = 0.6758
2019-02-12 17:43:02,576 : postediting : pearson = 0.7656, spearman = 0.7897
2019-02-12 17:43:02,811 : question-question : pearson = 0.4425, spearman = 0.4461
2019-02-12 17:43:02,811 : ALL (weighted average) : Pearson = 0.6083,             Spearman = 0.6264
2019-02-12 17:43:02,811 : ALL (average) : Pearson = 0.6051,             Spearman = 0.6225

2019-02-12 17:43:02,811 : ***** Transfer task : MR *****


2019-02-12 17:43:02,870 : loading BERT mode bert-base-uncased
2019-02-12 17:43:02,870 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:43:02,889 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:43:02,889 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzchf95cr
2019-02-12 17:43:05,203 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:43:06,603 : Generating sentence embeddings
2019-02-12 17:43:17,744 : Generated sentence embeddings
2019-02-12 17:43:17,745 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:43:33,996 : Best param found at split 1: l2reg = 0.0001                 with score 75.26
2019-02-12 17:43:49,075 : Best param found at split 2: l2reg = 0.0001                 with score 75.05
2019-02-12 17:44:01,267 : Best param found at split 3: l2reg = 0.0001                 with score 75.13
2019-02-12 17:44:15,555 : Best param found at split 4: l2reg = 0.001                 with score 74.74
2019-02-12 17:44:28,482 : Best param found at split 5: l2reg = 1e-05                 with score 74.69
2019-02-12 17:44:30,010 : Dev acc : 74.97 Test acc : 74.47

2019-02-12 17:44:30,011 : ***** Transfer task : CR *****


2019-02-12 17:44:30,054 : loading BERT mode bert-base-uncased
2019-02-12 17:44:30,054 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:44:30,090 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:44:30,090 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp388b8o4i
2019-02-12 17:44:32,449 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:44:33,825 : Generating sentence embeddings
2019-02-12 17:44:36,933 : Generated sentence embeddings
2019-02-12 17:44:36,933 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:44:40,538 : Best param found at split 1: l2reg = 0.001                 with score 78.3
2019-02-12 17:44:46,856 : Best param found at split 2: l2reg = 0.001                 with score 79.2
2019-02-12 17:44:52,028 : Best param found at split 3: l2reg = 0.0001                 with score 79.21
2019-02-12 17:44:57,250 : Best param found at split 4: l2reg = 0.001                 with score 78.55
2019-02-12 17:45:02,788 : Best param found at split 5: l2reg = 1e-05                 with score 79.05
2019-02-12 17:45:02,990 : Dev acc : 78.86 Test acc : 76.18

2019-02-12 17:45:02,990 : ***** Transfer task : MPQA *****


2019-02-12 17:45:03,005 : loading BERT mode bert-base-uncased
2019-02-12 17:45:03,005 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:45:03,034 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:45:03,034 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmo2_wsdx
2019-02-12 17:45:05,376 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:45:06,781 : Generating sentence embeddings
2019-02-12 17:45:09,897 : Generated sentence embeddings
2019-02-12 17:45:09,897 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:45:19,160 : Best param found at split 1: l2reg = 0.001                 with score 87.38
2019-02-12 17:45:26,781 : Best param found at split 2: l2reg = 0.001                 with score 88.34
2019-02-12 17:45:37,829 : Best param found at split 3: l2reg = 0.001                 with score 88.46
2019-02-12 17:45:52,420 : Best param found at split 4: l2reg = 0.001                 with score 88.3
2019-02-12 17:46:07,154 : Best param found at split 5: l2reg = 0.001                 with score 87.92
2019-02-12 17:46:07,756 : Dev acc : 88.08 Test acc : 87.68

2019-02-12 17:46:07,757 : ***** Transfer task : SUBJ *****


2019-02-12 17:46:07,808 : loading BERT mode bert-base-uncased
2019-02-12 17:46:07,808 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:46:07,832 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:46:07,833 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpweooxg07
2019-02-12 17:46:10,142 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:46:11,531 : Generating sentence embeddings
2019-02-12 17:46:23,035 : Generated sentence embeddings
2019-02-12 17:46:23,036 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 17:46:35,229 : Best param found at split 1: l2reg = 1e-05                 with score 91.57
2019-02-12 17:46:50,766 : Best param found at split 2: l2reg = 0.001                 with score 91.82
2019-02-12 17:47:04,219 : Best param found at split 3: l2reg = 0.001                 with score 91.6
2019-02-12 17:47:18,061 : Best param found at split 4: l2reg = 1e-05                 with score 92.04
2019-02-12 17:47:29,042 : Best param found at split 5: l2reg = 0.0001                 with score 91.41
2019-02-12 17:47:29,560 : Dev acc : 91.69 Test acc : 91.65

2019-02-12 17:47:29,561 : ***** Transfer task : SST Binary classification *****


2019-02-12 17:47:29,737 : loading BERT mode bert-base-uncased
2019-02-12 17:47:29,737 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:47:29,763 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:47:29,763 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzrjyix2r
2019-02-12 17:47:32,070 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:47:33,409 : Computing embedding for train
2019-02-12 17:48:10,674 : Computed train embeddings
2019-02-12 17:48:10,674 : Computing embedding for dev
2019-02-12 17:48:11,530 : Computed dev embeddings
2019-02-12 17:48:11,530 : Computing embedding for test
2019-02-12 17:48:13,187 : Computed test embeddings
2019-02-12 17:48:13,187 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 17:48:51,572 : [('reg:1e-05', 79.47), ('reg:0.0001', 79.82), ('reg:0.001', 79.7), ('reg:0.01', 78.78)]
2019-02-12 17:48:51,572 : Validation : best param found is reg = 0.0001 with score             79.82
2019-02-12 17:48:51,572 : Evaluating...
2019-02-12 17:48:59,004 : 
Dev acc : 79.82 Test acc : 80.29 for             SST Binary classification

2019-02-12 17:48:59,010 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 17:48:59,074 : loading BERT mode bert-base-uncased
2019-02-12 17:48:59,075 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:48:59,095 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:48:59,096 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyoaund5b
2019-02-12 17:49:01,463 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:49:02,917 : Computing embedding for train
2019-02-12 17:49:13,962 : Computed train embeddings
2019-02-12 17:49:13,962 : Computing embedding for dev
2019-02-12 17:49:15,005 : Computed dev embeddings
2019-02-12 17:49:15,006 : Computing embedding for test
2019-02-12 17:49:17,706 : Computed test embeddings
2019-02-12 17:49:17,706 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 17:49:22,483 : [('reg:1e-05', 40.24), ('reg:0.0001', 40.15), ('reg:0.001', 39.24), ('reg:0.01', 38.87)]
2019-02-12 17:49:22,483 : Validation : best param found is reg = 1e-05 with score             40.24
2019-02-12 17:49:22,483 : Evaluating...
2019-02-12 17:49:23,119 : 
Dev acc : 40.24 Test acc : 42.4 for             SST Fine-Grained classification

2019-02-12 17:49:23,120 : ***** Transfer task : TREC *****


2019-02-12 17:49:23,147 : loading BERT mode bert-base-uncased
2019-02-12 17:49:23,148 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:49:23,169 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:49:23,170 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyxkde3jo
2019-02-12 17:49:25,516 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:49:30,186 : Computed train embeddings
2019-02-12 17:49:30,410 : Computed test embeddings
2019-02-12 17:49:30,411 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 17:49:46,350 : [('reg:1e-05', 76.76), ('reg:0.0001', 76.65), ('reg:0.001', 74.78), ('reg:0.01', 65.48)]
2019-02-12 17:49:46,350 : Cross-validation : best param found is reg = 1e-05             with score 76.76
2019-02-12 17:49:46,350 : Evaluating...
2019-02-12 17:49:46,842 : 
Dev acc : 76.76 Test acc : 83.2             for TREC

2019-02-12 17:49:46,843 : ***** Transfer task : MRPC *****


2019-02-12 17:49:46,882 : loading BERT mode bert-base-uncased
2019-02-12 17:49:46,882 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:49:46,904 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:49:46,904 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2wjbh6ma
2019-02-12 17:49:49,337 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:49:50,902 : Computing embedding for train
2019-02-12 17:50:01,320 : Computed train embeddings
2019-02-12 17:50:01,320 : Computing embedding for test
2019-02-12 17:50:05,943 : Computed test embeddings
2019-02-12 17:50:05,959 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 17:50:16,320 : [('reg:1e-05', 74.41), ('reg:0.0001', 74.29), ('reg:0.001', 74.12), ('reg:0.01', 73.09)]
2019-02-12 17:50:16,320 : Cross-validation : best param found is reg = 1e-05             with score 74.41
2019-02-12 17:50:16,320 : Evaluating...
2019-02-12 17:50:16,614 : Dev acc : 74.41 Test acc 70.72; Test F1 77.24 for MRPC.

2019-02-12 17:50:16,614 : ***** Transfer task : SICK-Entailment*****


2019-02-12 17:50:16,644 : loading BERT mode bert-base-uncased
2019-02-12 17:50:16,644 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:50:16,699 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:50:16,699 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpujdi4btg
2019-02-12 17:50:19,131 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:50:20,580 : Computing embedding for train
2019-02-12 17:50:28,323 : Computed train embeddings
2019-02-12 17:50:28,323 : Computing embedding for dev
2019-02-12 17:50:29,651 : Computed dev embeddings
2019-02-12 17:50:29,651 : Computing embedding for test
2019-02-12 17:50:38,798 : Computed test embeddings
2019-02-12 17:50:38,826 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 17:50:42,758 : [('reg:1e-05', 80.0), ('reg:0.0001', 80.2), ('reg:0.001', 81.6), ('reg:0.01', 78.0)]
2019-02-12 17:50:42,758 : Validation : best param found is reg = 0.001 with score             81.6
2019-02-12 17:50:42,758 : Evaluating...
2019-02-12 17:50:43,703 : 
Dev acc : 81.6 Test acc : 81.49 for                        SICK entailment

2019-02-12 17:50:43,704 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 17:50:43,730 : loading BERT mode bert-base-uncased
2019-02-12 17:50:43,730 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:50:43,750 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:50:43,750 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkg1282vs
2019-02-12 17:50:46,191 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:50:47,601 : Computing embedding for train
2019-02-12 17:50:55,298 : Computed train embeddings
2019-02-12 17:50:55,299 : Computing embedding for dev
2019-02-12 17:50:56,166 : Computed dev embeddings
2019-02-12 17:50:56,166 : Computing embedding for test
2019-02-12 17:51:11,304 : Computed test embeddings
2019-02-12 17:51:47,437 : Dev : Pearson 0.807462851994446
2019-02-12 17:51:47,437 : Test : Pearson 0.8245061073607127 Spearman 0.7552315731337067 MSE 0.32676883589578126                        for SICK Relatedness

2019-02-12 17:51:47,438 : 

***** Transfer task : STSBenchmark*****


2019-02-12 17:51:47,519 : loading BERT mode bert-base-uncased
2019-02-12 17:51:47,519 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:51:47,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:51:47,539 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzxp0oduh
2019-02-12 17:51:49,972 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:51:51,438 : Computing embedding for train
2019-02-12 17:52:03,513 : Computed train embeddings
2019-02-12 17:52:03,513 : Computing embedding for dev
2019-02-12 17:52:06,775 : Computed dev embeddings
2019-02-12 17:52:06,775 : Computing embedding for test
2019-02-12 17:52:09,511 : Computed test embeddings
2019-02-12 17:52:43,584 : Dev : Pearson 0.7550856779166432
2019-02-12 17:52:43,584 : Test : Pearson 0.7023578299620669 Spearman 0.7027351035990721 MSE 1.3971707294445477                        for SICK Relatedness

2019-02-12 17:52:43,585 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 17:52:43,865 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 17:52:43,875 : loading BERT mode bert-base-uncased
2019-02-12 17:52:43,875 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:52:43,967 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:52:43,967 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjrqq5leo
2019-02-12 17:52:46,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:52:47,812 : Computing embeddings for train/dev/test
2019-02-12 17:55:01,131 : Computed embeddings
2019-02-12 17:55:01,131 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 17:55:58,446 : [('reg:1e-05', 96.27), ('reg:0.0001', 92.04), ('reg:0.001', 89.19), ('reg:0.01', 84.78)]
2019-02-12 17:55:58,446 : Validation : best param found is reg = 1e-05 with score             96.27
2019-02-12 17:55:58,446 : Evaluating...
2019-02-12 17:56:28,039 : 
Dev acc : 96.3 Test acc : 96.8 for LENGTH classification

2019-02-12 17:56:28,047 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 17:56:28,554 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 17:56:28,599 : loading BERT mode bert-base-uncased
2019-02-12 17:56:28,599 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 17:56:28,629 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 17:56:28,629 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplacio3id
2019-02-12 17:56:31,090 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 17:56:32,501 : Computing embeddings for train/dev/test
2019-02-12 17:58:48,926 : Computed embeddings
2019-02-12 17:58:48,926 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:00:18,444 : [('reg:1e-05', 82.59), ('reg:0.0001', 26.88), ('reg:0.001', 1.44), ('reg:0.01', 0.59)]
2019-02-12 18:00:18,445 : Validation : best param found is reg = 1e-05 with score             82.59
2019-02-12 18:00:18,445 : Evaluating...
2019-02-12 18:00:31,092 : 
Dev acc : 82.6 Test acc : 82.3 for WORDCONTENT classification

2019-02-12 18:00:31,102 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 18:00:31,671 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 18:00:31,737 : loading BERT mode bert-base-uncased
2019-02-12 18:00:31,737 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:00:31,765 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:00:31,766 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa5p3__b3
2019-02-12 18:00:34,235 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:00:35,675 : Computing embeddings for train/dev/test
2019-02-12 18:03:05,593 : Computed embeddings
2019-02-12 18:03:05,593 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:04:14,747 : [('reg:1e-05', 32.53), ('reg:0.0001', 32.27), ('reg:0.001', 30.94), ('reg:0.01', 26.68)]
2019-02-12 18:04:14,747 : Validation : best param found is reg = 1e-05 with score             32.53
2019-02-12 18:04:14,747 : Evaluating...
2019-02-12 18:04:31,725 : 
Dev acc : 32.5 Test acc : 32.2 for DEPTH classification

2019-02-12 18:04:31,733 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 18:04:32,198 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 18:04:32,259 : loading BERT mode bert-base-uncased
2019-02-12 18:04:32,259 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:04:32,284 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:04:32,284 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpznl2sgz9
2019-02-12 18:04:34,667 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:04:36,227 : Computing embeddings for train/dev/test
2019-02-12 18:06:48,557 : Computed embeddings
2019-02-12 18:06:48,557 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:08:49,820 : [('reg:1e-05', 54.57), ('reg:0.0001', 53.08), ('reg:0.001', 46.73), ('reg:0.01', 34.52)]
2019-02-12 18:08:49,820 : Validation : best param found is reg = 1e-05 with score             54.57
2019-02-12 18:08:49,820 : Evaluating...
2019-02-12 18:09:10,950 : 
Dev acc : 54.6 Test acc : 53.6 for TOPCONSTITUENTS classification

2019-02-12 18:09:10,959 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 18:09:11,330 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 18:09:11,395 : loading BERT mode bert-base-uncased
2019-02-12 18:09:11,395 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:09:11,512 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:09:11,512 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3prrq8uh
2019-02-12 18:09:13,950 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:09:15,369 : Computing embeddings for train/dev/test
2019-02-12 18:11:37,178 : Computed embeddings
2019-02-12 18:11:37,179 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:13:01,605 : [('reg:1e-05', 50.69), ('reg:0.0001', 50.6), ('reg:0.001', 50.65), ('reg:0.01', 50.42)]
2019-02-12 18:13:01,605 : Validation : best param found is reg = 1e-05 with score             50.69
2019-02-12 18:13:01,605 : Evaluating...
2019-02-12 18:13:18,244 : 
Dev acc : 50.7 Test acc : 49.5 for BIGRAMSHIFT classification

2019-02-12 18:13:18,252 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 18:13:18,658 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 18:13:18,723 : loading BERT mode bert-base-uncased
2019-02-12 18:13:18,723 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:13:18,836 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:13:18,836 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdmblknb3
2019-02-12 18:13:21,253 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:13:22,732 : Computing embeddings for train/dev/test
2019-02-12 18:15:38,974 : Computed embeddings
2019-02-12 18:15:38,975 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:16:53,710 : [('reg:1e-05', 86.24), ('reg:0.0001', 86.44), ('reg:0.001', 86.33), ('reg:0.01', 84.54)]
2019-02-12 18:16:53,710 : Validation : best param found is reg = 0.0001 with score             86.44
2019-02-12 18:16:53,710 : Evaluating...
2019-02-12 18:17:09,480 : 
Dev acc : 86.4 Test acc : 84.5 for TENSE classification

2019-02-12 18:17:09,488 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 18:17:10,077 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 18:17:10,138 : loading BERT mode bert-base-uncased
2019-02-12 18:17:10,139 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:17:10,165 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:17:10,166 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk69xo8m9
2019-02-12 18:17:12,605 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:17:14,074 : Computing embeddings for train/dev/test
2019-02-12 18:19:58,125 : Computed embeddings
2019-02-12 18:19:58,125 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:21:01,585 : [('reg:1e-05', 80.78), ('reg:0.0001', 80.71), ('reg:0.001', 80.93), ('reg:0.01', 77.83)]
2019-02-12 18:21:01,585 : Validation : best param found is reg = 0.001 with score             80.93
2019-02-12 18:21:01,585 : Evaluating...
2019-02-12 18:21:15,531 : 
Dev acc : 80.9 Test acc : 79.9 for SUBJNUMBER classification

2019-02-12 18:21:15,539 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 18:21:15,975 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 18:21:16,041 : loading BERT mode bert-base-uncased
2019-02-12 18:21:16,041 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:21:16,069 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:21:16,069 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph1_wysuy
2019-02-12 18:21:18,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:21:19,953 : Computing embeddings for train/dev/test
2019-02-12 18:24:06,397 : Computed embeddings
2019-02-12 18:24:06,397 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:25:00,746 : [('reg:1e-05', 79.73), ('reg:0.0001', 79.76), ('reg:0.001', 80.21), ('reg:0.01', 78.75)]
2019-02-12 18:25:00,746 : Validation : best param found is reg = 0.001 with score             80.21
2019-02-12 18:25:00,747 : Evaluating...
2019-02-12 18:25:12,864 : 
Dev acc : 80.2 Test acc : 81.0 for OBJNUMBER classification

2019-02-12 18:25:12,872 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 18:25:13,293 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 18:25:13,360 : loading BERT mode bert-base-uncased
2019-02-12 18:25:13,361 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:25:13,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:25:13,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjjdz60wy
2019-02-12 18:25:15,811 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:25:17,264 : Computing embeddings for train/dev/test
2019-02-12 18:28:28,124 : Computed embeddings
2019-02-12 18:28:28,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:29:26,521 : [('reg:1e-05', 50.2), ('reg:0.0001', 50.2), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-02-12 18:29:26,521 : Validation : best param found is reg = 1e-05 with score             50.2
2019-02-12 18:29:26,521 : Evaluating...
2019-02-12 18:29:40,097 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-02-12 18:29:40,105 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 18:29:40,540 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 18:29:40,615 : loading BERT mode bert-base-uncased
2019-02-12 18:29:40,615 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:29:40,736 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:29:40,736 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe0d9b6na
2019-02-12 18:29:43,159 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:29:44,605 : Computing embeddings for train/dev/test
2019-02-12 18:33:04,351 : Computed embeddings
2019-02-12 18:33:04,351 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:33:47,530 : [('reg:1e-05', 50.06), ('reg:0.0001', 50.15), ('reg:0.001', 50.41), ('reg:0.01', 50.16)]
2019-02-12 18:33:47,530 : Validation : best param found is reg = 0.001 with score             50.41
2019-02-12 18:33:47,530 : Evaluating...
2019-02-12 18:33:57,371 : 
Dev acc : 50.4 Test acc : 50.2 for COORDINATIONINVERSION classification

2019-02-12 18:33:57,380 : {'STS12': {'MSRpar': {'pearson': (0.37608910341210966, 1.305449361113563e-26), 'spearman': SpearmanrResult(correlation=0.4190600708400811, pvalue=2.973455788524349e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5699373467373037, 8.103574084921277e-66), 'spearman': SpearmanrResult(correlation=0.5753441509054089, pvalue=2.5588776681847767e-67), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4763309328096535, 2.2576438655411545e-27), 'spearman': SpearmanrResult(correlation=0.5887682326789692, pvalue=3.640580160602892e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.655365515674495, 3.0625287614551527e-93), 'spearman': SpearmanrResult(correlation=0.6771461948143819, pvalue=9.799753750931137e-102), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4939590153135734, 6.199535817945441e-26), 'spearman': SpearmanrResult(correlation=0.4404915403911901, pvalue=2.2799315637405728e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5143363827894271, 'wmean': 0.520196113139536}, 'spearman': {'mean': 0.5401620379260063, 'wmean': 0.5468672959574117}}}, 'STS13': {'FNWN': {'pearson': (0.3675728689317999, 1.9602074059305347e-07), 'spearman': SpearmanrResult(correlation=0.36460726440830216, pvalue=2.496879825345467e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6577456469075137, 3.910586594304102e-94), 'spearman': SpearmanrResult(correlation=0.6415388251047481, pvalue=3.3272464573637005e-88), 'nsamples': 750}, 'OnWN': {'pearson': (0.405358828264975, 1.340119901632524e-23), 'spearman': SpearmanrResult(correlation=0.45944123278771665, pvalue=1.2098069795384704e-30), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4768924480347629, 'wmean': 0.5267912067102642}, 'spearman': {'mean': 0.48852910743358896, 'wmean': 0.5385409489304261}}}, 'STS14': {'deft-forum': {'pearson': (0.3368883651023242, 2.0992121173197024e-13), 'spearman': SpearmanrResult(correlation=0.3553104682416424, pvalue=7.766715138294184e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.6840271733037607, 1.0036651236134263e-42), 'spearman': SpearmanrResult(correlation=0.6730380425574104, pvalue=6.268303716640072e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.6223013097943583, 1.2927931577194085e-81), 'spearman': SpearmanrResult(correlation=0.5876882779979462, pvalue=7.502578890201231e-71), 'nsamples': 750}, 'images': {'pearson': (0.5856500162079894, 2.9446079126518624e-70), 'spearman': SpearmanrResult(correlation=0.5888728253102319, pvalue=3.374182831798912e-71), 'nsamples': 750}, 'OnWN': {'pearson': (0.5576104973919013, 1.6898764913426272e-62), 'spearman': SpearmanrResult(correlation=0.6216878388199086, pvalue=2.0612260983420593e-81), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5936557343924328, 1.2948072497284076e-72), 'spearman': SpearmanrResult(correlation=0.5864121785000831, pvalue=1.767974057606539e-70), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5633555160321279, 'wmean': 0.566992289233916}, 'spearman': {'mean': 0.5688349385712038, 'wmean': 0.5734125237192239}}}, 'STS15': {'answers-forums': {'pearson': (0.4530994709025457, 2.204019314100139e-20), 'spearman': SpearmanrResult(correlation=0.448218517167679, pvalue=6.236500081011466e-20), 'nsamples': 375}, 'answers-students': {'pearson': (0.6952283215630497, 2.2906005961765553e-109), 'spearman': SpearmanrResult(correlation=0.7051249557537382, pvalue=8.65837353428262e-114), 'nsamples': 750}, 'belief': {'pearson': (0.5276254709396421, 2.9113467062348843e-28), 'spearman': SpearmanrResult(correlation=0.526845891894358, pvalue=3.605291797068943e-28), 'nsamples': 375}, 'headlines': {'pearson': (0.6781822014087495, 3.705187024361473e-102), 'spearman': SpearmanrResult(correlation=0.6724094474534524, pvalue=7.953902314224241e-100), 'nsamples': 750}, 'images': {'pearson': (0.6893929757542225, 7.65653314262036e-107), 'spearman': SpearmanrResult(correlation=0.7051025954506968, pvalue=8.8641333566024e-114), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6087056881136419, 'wmean': 0.6382914924117788}, 'spearman': {'mean': 0.611540281543985, 'wmean': 0.6425423007972265}}}, 'STS16': {'answer-answer': {'pearson': (0.4588464988240848, 1.248930303173984e-14), 'spearman': SpearmanrResult(correlation=0.5082104051298644, pvalue=4.369034288251477e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6883578657783531, 2.6535380218994988e-36), 'spearman': SpearmanrResult(correlation=0.6926994792644391, pvalue=6.403482331441476e-37), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6703933064785108, 2.222114107480498e-31), 'spearman': SpearmanrResult(correlation=0.6758228188546684, pvalue=4.804490939185053e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.7656333390720964, 2.862029131465676e-48), 'spearman': SpearmanrResult(correlation=0.7897413873655879, pvalue=2.8305091607538467e-53), 'nsamples': 244}, 'question-question': {'pearson': (0.4425016087832092, 1.9683991165745585e-11), 'spearman': SpearmanrResult(correlation=0.44613451406985594, pvalue=1.287989913917734e-11), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6051465237872509, 'wmean': 0.6082933817364816}, 'spearman': {'mean': 0.6225217209368831, 'wmean': 0.626429910231204}}}, 'MR': {'devacc': 74.97, 'acc': 74.47, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 78.86, 'acc': 76.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.08, 'acc': 87.68, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.69, 'acc': 91.65, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.82, 'acc': 80.29, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.24, 'acc': 42.4, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 76.76, 'acc': 83.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.41, 'acc': 70.72, 'f1': 77.24, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 81.6, 'acc': 81.49, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.807462851994446, 'pearson': 0.8245061073607127, 'spearman': 0.7552315731337067, 'mse': 0.32676883589578126, 'yhat': array([3.81480848, 4.23778118, 1.4352887 , ..., 3.26990125, 4.42546991,
       4.58557633]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7550856779166432, 'pearson': 0.7023578299620669, 'spearman': 0.7027351035990721, 'mse': 1.3971707294445477, 'yhat': array([1.49286679, 1.30544915, 2.05201972, ..., 3.82437181, 3.74310898,
       3.10179553]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 96.27, 'acc': 96.76, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 82.59, 'acc': 82.27, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.53, 'acc': 32.25, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.57, 'acc': 53.56, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.69, 'acc': 49.46, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.44, 'acc': 84.52, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.93, 'acc': 79.86, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.21, 'acc': 81.05, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.2, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.41, 'acc': 50.22, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 18:33:57,380 : ********************************************************************************
2019-02-12 18:33:57,380 : ********************************************************************************
2019-02-12 18:33:57,380 : ********************************************************************************
2019-02-12 18:33:57,380 : layer 1
2019-02-12 18:33:57,380 : ********************************************************************************
2019-02-12 18:33:57,380 : ********************************************************************************
2019-02-12 18:33:57,380 : ********************************************************************************
2019-02-12 18:33:57,468 : ***** Transfer task : STS12 *****


2019-02-12 18:33:57,481 : loading BERT mode bert-base-uncased
2019-02-12 18:33:57,481 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:33:57,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:33:57,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_sntuan6
2019-02-12 18:33:59,930 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:34:03,556 : MSRpar : pearson = 0.4116, spearman = 0.4495
2019-02-12 18:34:04,606 : MSRvid : pearson = 0.6776, spearman = 0.6797
2019-02-12 18:34:05,377 : SMTeuroparl : pearson = 0.5043, spearman = 0.6039
2019-02-12 18:34:06,735 : surprise.OnWN : pearson = 0.7067, spearman = 0.6945
2019-02-12 18:34:07,484 : surprise.SMTnews : pearson = 0.5447, spearman = 0.5010
2019-02-12 18:34:07,485 : ALL (weighted average) : Pearson = 0.5778,             Spearman = 0.5936
2019-02-12 18:34:07,485 : ALL (average) : Pearson = 0.5690,             Spearman = 0.5857

2019-02-12 18:34:07,485 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 18:34:07,495 : loading BERT mode bert-base-uncased
2019-02-12 18:34:07,495 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:34:07,513 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:34:07,513 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph2yh7ybm
2019-02-12 18:34:09,941 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:34:12,104 : FNWN : pearson = 0.4569, spearman = 0.4717
2019-02-12 18:34:13,391 : headlines : pearson = 0.6849, spearman = 0.6622
2019-02-12 18:34:14,432 : OnWN : pearson = 0.5627, spearman = 0.5900
2019-02-12 18:34:14,432 : ALL (weighted average) : Pearson = 0.6105,             Spearman = 0.6112
2019-02-12 18:34:14,432 : ALL (average) : Pearson = 0.5681,             Spearman = 0.5746

2019-02-12 18:34:14,432 : ***** Transfer task : STS14 *****


2019-02-12 18:34:14,448 : loading BERT mode bert-base-uncased
2019-02-12 18:34:14,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:34:14,465 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:34:14,465 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprk8dy0ps
2019-02-12 18:34:16,899 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:34:19,853 : deft-forum : pearson = 0.3706, spearman = 0.3778
2019-02-12 18:34:21,068 : deft-news : pearson = 0.7257, spearman = 0.6912
2019-02-12 18:34:23,255 : headlines : pearson = 0.6490, spearman = 0.6007
2019-02-12 18:34:25,408 : images : pearson = 0.6885, spearman = 0.6666
2019-02-12 18:34:27,598 : OnWN : pearson = 0.6819, spearman = 0.7213
2019-02-12 18:34:30,006 : tweet-news : pearson = 0.7007, spearman = 0.6655
2019-02-12 18:34:30,006 : ALL (weighted average) : Pearson = 0.6466,             Spearman = 0.6315
2019-02-12 18:34:30,006 : ALL (average) : Pearson = 0.6361,             Spearman = 0.6205

2019-02-12 18:34:30,006 : ***** Transfer task : STS15 *****


2019-02-12 18:34:30,068 : loading BERT mode bert-base-uncased
2019-02-12 18:34:30,068 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:34:30,086 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:34:30,086 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzgpey8u5
2019-02-12 18:34:32,489 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:34:35,440 : answers-forums : pearson = 0.5669, spearman = 0.5520
2019-02-12 18:34:37,690 : answers-students : pearson = 0.7242, spearman = 0.7330
2019-02-12 18:34:39,240 : belief : pearson = 0.6536, spearman = 0.6670
2019-02-12 18:34:41,650 : headlines : pearson = 0.7103, spearman = 0.6994
2019-02-12 18:34:44,017 : images : pearson = 0.7584, spearman = 0.7661
2019-02-12 18:34:44,017 : ALL (weighted average) : Pearson = 0.7008,             Spearman = 0.7020
2019-02-12 18:34:44,017 : ALL (average) : Pearson = 0.6827,             Spearman = 0.6835

2019-02-12 18:34:44,017 : ***** Transfer task : STS16 *****


2019-02-12 18:34:44,086 : loading BERT mode bert-base-uncased
2019-02-12 18:34:44,086 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:34:44,104 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:34:44,104 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyvryzod2
2019-02-12 18:34:46,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:34:48,879 : answer-answer : pearson = 0.4951, spearman = 0.5277
2019-02-12 18:34:49,674 : headlines : pearson = 0.7121, spearman = 0.7129
2019-02-12 18:34:50,525 : plagiarism : pearson = 0.7685, spearman = 0.7704
2019-02-12 18:34:51,608 : postediting : pearson = 0.8227, spearman = 0.8362
2019-02-12 18:34:52,335 : question-question : pearson = 0.4694, spearman = 0.4658
2019-02-12 18:34:52,335 : ALL (weighted average) : Pearson = 0.6566,             Spearman = 0.6662
2019-02-12 18:34:52,335 : ALL (average) : Pearson = 0.6536,             Spearman = 0.6626

2019-02-12 18:34:52,335 : ***** Transfer task : MR *****


2019-02-12 18:34:52,352 : loading BERT mode bert-base-uncased
2019-02-12 18:34:52,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:34:52,372 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:34:52,372 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv3je4tgc
2019-02-12 18:34:54,760 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:34:56,353 : Generating sentence embeddings
2019-02-12 18:35:20,421 : Generated sentence embeddings
2019-02-12 18:35:20,421 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:35:58,069 : Best param found at split 1: l2reg = 0.001                 with score 75.49
2019-02-12 18:36:37,191 : Best param found at split 2: l2reg = 0.001                 with score 74.87
2019-02-12 18:37:11,144 : Best param found at split 3: l2reg = 0.001                 with score 75.29
2019-02-12 18:37:28,743 : Best param found at split 4: l2reg = 0.001                 with score 74.94
2019-02-12 18:37:41,989 : Best param found at split 5: l2reg = 0.001                 with score 75.24
2019-02-12 18:37:42,564 : Dev acc : 75.17 Test acc : 74.71

2019-02-12 18:37:42,565 : ***** Transfer task : CR *****


2019-02-12 18:37:42,573 : loading BERT mode bert-base-uncased
2019-02-12 18:37:42,573 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:37:42,592 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:37:42,592 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe3d790b2
2019-02-12 18:37:45,036 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:37:46,470 : Generating sentence embeddings
2019-02-12 18:37:51,212 : Generated sentence embeddings
2019-02-12 18:37:51,212 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:37:55,070 : Best param found at split 1: l2reg = 0.0001                 with score 79.17
2019-02-12 18:38:01,793 : Best param found at split 2: l2reg = 0.0001                 with score 79.36
2019-02-12 18:38:11,678 : Best param found at split 3: l2reg = 0.01                 with score 80.56
2019-02-12 18:38:20,428 : Best param found at split 4: l2reg = 0.01                 with score 79.28
2019-02-12 18:38:29,681 : Best param found at split 5: l2reg = 0.001                 with score 79.54
2019-02-12 18:38:30,221 : Dev acc : 79.58 Test acc : 78.01

2019-02-12 18:38:30,222 : ***** Transfer task : MPQA *****


2019-02-12 18:38:30,227 : loading BERT mode bert-base-uncased
2019-02-12 18:38:30,227 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:38:30,246 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:38:30,247 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1v12mxoj
2019-02-12 18:38:32,647 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:38:34,184 : Generating sentence embeddings
2019-02-12 18:38:47,703 : Generated sentence embeddings
2019-02-12 18:38:47,703 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:39:28,681 : Best param found at split 1: l2reg = 0.01                 with score 87.33
2019-02-12 18:40:14,505 : Best param found at split 2: l2reg = 0.001                 with score 87.97
2019-02-12 18:40:45,443 : Best param found at split 3: l2reg = 0.001                 with score 88.26
2019-02-12 18:41:09,402 : Best param found at split 4: l2reg = 0.001                 with score 87.98
2019-02-12 18:41:27,175 : Best param found at split 5: l2reg = 0.01                 with score 87.84
2019-02-12 18:41:28,020 : Dev acc : 87.88 Test acc : 87.88

2019-02-12 18:41:28,021 : ***** Transfer task : SUBJ *****


2019-02-12 18:41:28,035 : loading BERT mode bert-base-uncased
2019-02-12 18:41:28,035 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:41:28,056 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:41:28,057 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl0wfl4lw
2019-02-12 18:41:30,486 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:41:32,035 : Generating sentence embeddings
2019-02-12 18:41:49,002 : Generated sentence embeddings
2019-02-12 18:41:49,002 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 18:42:07,178 : Best param found at split 1: l2reg = 0.0001                 with score 92.91
2019-02-12 18:42:26,917 : Best param found at split 2: l2reg = 0.001                 with score 93.22
2019-02-12 18:42:50,643 : Best param found at split 3: l2reg = 1e-05                 with score 92.65
2019-02-12 18:43:16,379 : Best param found at split 4: l2reg = 1e-05                 with score 93.14
2019-02-12 18:43:39,285 : Best param found at split 5: l2reg = 0.0001                 with score 92.88
2019-02-12 18:43:40,347 : Dev acc : 92.96 Test acc : 92.45

2019-02-12 18:43:40,347 : ***** Transfer task : SST Binary classification *****


2019-02-12 18:43:40,474 : loading BERT mode bert-base-uncased
2019-02-12 18:43:40,474 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:43:40,497 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:43:40,497 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4xjcwkrj
2019-02-12 18:43:42,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:43:44,410 : Computing embedding for train
2019-02-12 18:45:13,391 : Computed train embeddings
2019-02-12 18:45:13,392 : Computing embedding for dev
2019-02-12 18:45:14,589 : Computed dev embeddings
2019-02-12 18:45:14,589 : Computing embedding for test
2019-02-12 18:45:16,867 : Computed test embeddings
2019-02-12 18:45:16,867 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:45:45,208 : [('reg:1e-05', 77.98), ('reg:0.0001', 78.67), ('reg:0.001', 77.98), ('reg:0.01', 77.52)]
2019-02-12 18:45:45,208 : Validation : best param found is reg = 0.0001 with score             78.67
2019-02-12 18:45:45,208 : Evaluating...
2019-02-12 18:45:49,918 : 
Dev acc : 78.67 Test acc : 76.55 for             SST Binary classification

2019-02-12 18:45:49,919 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 18:45:49,972 : loading BERT mode bert-base-uncased
2019-02-12 18:45:49,972 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:45:49,992 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:45:49,992 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyjn34hc6
2019-02-12 18:45:52,468 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:45:53,941 : Computing embedding for train
2019-02-12 18:46:07,769 : Computed train embeddings
2019-02-12 18:46:07,770 : Computing embedding for dev
2019-02-12 18:46:09,505 : Computed dev embeddings
2019-02-12 18:46:09,505 : Computing embedding for test
2019-02-12 18:46:13,009 : Computed test embeddings
2019-02-12 18:46:13,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:46:18,448 : [('reg:1e-05', 40.05), ('reg:0.0001', 39.87), ('reg:0.001', 39.78), ('reg:0.01', 39.87)]
2019-02-12 18:46:18,448 : Validation : best param found is reg = 1e-05 with score             40.05
2019-02-12 18:46:18,448 : Evaluating...
2019-02-12 18:46:19,794 : 
Dev acc : 40.05 Test acc : 41.22 for             SST Fine-Grained classification

2019-02-12 18:46:19,795 : ***** Transfer task : TREC *****


2019-02-12 18:46:19,808 : loading BERT mode bert-base-uncased
2019-02-12 18:46:19,808 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:46:19,826 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:46:19,826 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa4xsxqn9
2019-02-12 18:46:22,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:46:30,638 : Computed train embeddings
2019-02-12 18:46:31,235 : Computed test embeddings
2019-02-12 18:46:31,236 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 18:46:49,869 : [('reg:1e-05', 79.0), ('reg:0.0001', 79.18), ('reg:0.001', 78.98), ('reg:0.01', 69.88)]
2019-02-12 18:46:49,870 : Cross-validation : best param found is reg = 0.0001             with score 79.18
2019-02-12 18:46:49,870 : Evaluating...
2019-02-12 18:46:50,984 : 
Dev acc : 79.18 Test acc : 90.0             for TREC

2019-02-12 18:46:50,984 : ***** Transfer task : MRPC *****


2019-02-12 18:46:51,006 : loading BERT mode bert-base-uncased
2019-02-12 18:46:51,006 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:46:51,026 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:46:51,026 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_ad54ohz
2019-02-12 18:46:53,482 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:46:55,018 : Computing embedding for train
2019-02-12 18:47:09,997 : Computed train embeddings
2019-02-12 18:47:09,997 : Computing embedding for test
2019-02-12 18:47:16,595 : Computed test embeddings
2019-02-12 18:47:16,611 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 18:47:29,185 : [('reg:1e-05', 73.85), ('reg:0.0001', 73.75), ('reg:0.001', 73.75), ('reg:0.01', 73.04)]
2019-02-12 18:47:29,185 : Cross-validation : best param found is reg = 1e-05             with score 73.85
2019-02-12 18:47:29,185 : Evaluating...
2019-02-12 18:47:29,983 : Dev acc : 73.85 Test acc 73.22; Test F1 81.31 for MRPC.

2019-02-12 18:47:29,983 : ***** Transfer task : SICK-Entailment*****


2019-02-12 18:47:30,046 : loading BERT mode bert-base-uncased
2019-02-12 18:47:30,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:47:30,065 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:47:30,065 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx5f4uruv
2019-02-12 18:47:32,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:47:33,983 : Computing embedding for train
2019-02-12 18:47:46,908 : Computed train embeddings
2019-02-12 18:47:46,908 : Computing embedding for dev
2019-02-12 18:47:48,506 : Computed dev embeddings
2019-02-12 18:47:48,506 : Computing embedding for test
2019-02-12 18:48:03,029 : Computed test embeddings
2019-02-12 18:48:03,056 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:48:06,712 : [('reg:1e-05', 80.6), ('reg:0.0001', 80.6), ('reg:0.001', 80.4), ('reg:0.01', 81.2)]
2019-02-12 18:48:06,712 : Validation : best param found is reg = 0.01 with score             81.2
2019-02-12 18:48:06,712 : Evaluating...
2019-02-12 18:48:07,711 : 
Dev acc : 81.2 Test acc : 79.56 for                        SICK entailment

2019-02-12 18:48:07,711 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 18:48:07,738 : loading BERT mode bert-base-uncased
2019-02-12 18:48:07,738 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:48:07,757 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:48:07,757 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxv6i_e4n
2019-02-12 18:48:10,180 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:48:11,680 : Computing embedding for train
2019-02-12 18:48:22,979 : Computed train embeddings
2019-02-12 18:48:22,979 : Computing embedding for dev
2019-02-12 18:48:24,533 : Computed dev embeddings
2019-02-12 18:48:24,533 : Computing embedding for test
2019-02-12 18:48:33,886 : Computed test embeddings
2019-02-12 18:49:09,204 : Dev : Pearson 0.8237443352877583
2019-02-12 18:49:09,204 : Test : Pearson 0.8329372932713901 Spearman 0.7594390801065386 MSE 0.3134001024837591                        for SICK Relatedness

2019-02-12 18:49:09,205 : 

***** Transfer task : STSBenchmark*****


2019-02-12 18:49:09,244 : loading BERT mode bert-base-uncased
2019-02-12 18:49:09,244 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:49:09,273 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:49:09,273 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjt2q24w0
2019-02-12 18:49:11,707 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:49:13,143 : Computing embedding for train
2019-02-12 18:49:24,079 : Computed train embeddings
2019-02-12 18:49:24,079 : Computing embedding for dev
2019-02-12 18:49:28,618 : Computed dev embeddings
2019-02-12 18:49:28,618 : Computing embedding for test
2019-02-12 18:49:32,610 : Computed test embeddings
2019-02-12 18:50:10,209 : Dev : Pearson 0.7559527821468942
2019-02-12 18:50:10,209 : Test : Pearson 0.7058771519466621 Spearman 0.703691280303891 MSE 1.3937725223639443                        for SICK Relatedness

2019-02-12 18:50:10,209 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 18:50:10,564 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 18:50:10,574 : loading BERT mode bert-base-uncased
2019-02-12 18:50:10,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:50:10,601 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:50:10,601 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppxf_az0v
2019-02-12 18:50:13,038 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:50:14,517 : Computing embeddings for train/dev/test
2019-02-12 18:52:42,613 : Computed embeddings
2019-02-12 18:52:42,613 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:53:36,022 : [('reg:1e-05', 95.3), ('reg:0.0001', 93.41), ('reg:0.001', 89.03), ('reg:0.01', 80.01)]
2019-02-12 18:53:36,022 : Validation : best param found is reg = 1e-05 with score             95.3
2019-02-12 18:53:36,022 : Evaluating...
2019-02-12 18:53:50,509 : 
Dev acc : 95.3 Test acc : 95.3 for LENGTH classification

2019-02-12 18:53:50,519 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 18:53:50,865 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 18:53:50,909 : loading BERT mode bert-base-uncased
2019-02-12 18:53:50,910 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:53:50,938 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:53:50,939 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsfzm0q9a
2019-02-12 18:53:53,400 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:53:54,813 : Computing embeddings for train/dev/test
2019-02-12 18:56:12,883 : Computed embeddings
2019-02-12 18:56:12,884 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 18:57:09,218 : [('reg:1e-05', 89.99), ('reg:0.0001', 62.0), ('reg:0.001', 5.4), ('reg:0.01', 0.99)]
2019-02-12 18:57:09,218 : Validation : best param found is reg = 1e-05 with score             89.99
2019-02-12 18:57:09,218 : Evaluating...
2019-02-12 18:57:21,175 : 
Dev acc : 90.0 Test acc : 89.9 for WORDCONTENT classification

2019-02-12 18:57:21,185 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 18:57:21,531 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 18:57:21,595 : loading BERT mode bert-base-uncased
2019-02-12 18:57:21,595 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 18:57:21,690 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 18:57:21,690 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpee1zpnq1
2019-02-12 18:57:24,133 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 18:57:25,546 : Computing embeddings for train/dev/test
2019-02-12 18:59:25,891 : Computed embeddings
2019-02-12 18:59:25,891 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:00:35,147 : [('reg:1e-05', 33.7), ('reg:0.0001', 33.24), ('reg:0.001', 31.32), ('reg:0.01', 26.41)]
2019-02-12 19:00:35,148 : Validation : best param found is reg = 1e-05 with score             33.7
2019-02-12 19:00:35,148 : Evaluating...
2019-02-12 19:01:04,341 : 
Dev acc : 33.7 Test acc : 34.0 for DEPTH classification

2019-02-12 19:01:04,350 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 19:01:04,709 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 19:01:04,770 : loading BERT mode bert-base-uncased
2019-02-12 19:01:04,770 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:01:04,874 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:01:04,874 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp05m5y41p
2019-02-12 19:01:07,299 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:01:08,873 : Computing embeddings for train/dev/test
2019-02-12 19:03:37,405 : Computed embeddings
2019-02-12 19:03:37,405 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:05:05,713 : [('reg:1e-05', 63.66), ('reg:0.0001', 60.64), ('reg:0.001', 51.21), ('reg:0.01', 41.27)]
2019-02-12 19:05:05,713 : Validation : best param found is reg = 1e-05 with score             63.66
2019-02-12 19:05:05,714 : Evaluating...
2019-02-12 19:05:33,452 : 
Dev acc : 63.7 Test acc : 64.4 for TOPCONSTITUENTS classification

2019-02-12 19:05:33,454 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 19:05:33,965 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 19:05:34,030 : loading BERT mode bert-base-uncased
2019-02-12 19:05:34,030 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:05:34,061 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:05:34,061 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp16i7cln5
2019-02-12 19:05:36,489 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:05:37,937 : Computing embeddings for train/dev/test
2019-02-12 19:07:57,664 : Computed embeddings
2019-02-12 19:07:57,665 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:09:53,090 : [('reg:1e-05', 55.18), ('reg:0.0001', 55.07), ('reg:0.001', 54.86), ('reg:0.01', 52.2)]
2019-02-12 19:09:53,090 : Validation : best param found is reg = 1e-05 with score             55.18
2019-02-12 19:09:53,090 : Evaluating...
2019-02-12 19:10:26,482 : 
Dev acc : 55.2 Test acc : 54.7 for BIGRAMSHIFT classification

2019-02-12 19:10:26,490 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 19:10:26,882 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 19:10:26,946 : loading BERT mode bert-base-uncased
2019-02-12 19:10:26,946 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:10:26,976 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:10:26,976 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnybxaecq
2019-02-12 19:10:29,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:10:30,889 : Computing embeddings for train/dev/test
2019-02-12 19:12:36,692 : Computed embeddings
2019-02-12 19:12:36,692 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:14:24,649 : [('reg:1e-05', 86.72), ('reg:0.0001', 86.64), ('reg:0.001', 86.32), ('reg:0.01', 86.32)]
2019-02-12 19:14:24,649 : Validation : best param found is reg = 1e-05 with score             86.72
2019-02-12 19:14:24,649 : Evaluating...
2019-02-12 19:14:43,973 : 
Dev acc : 86.7 Test acc : 84.9 for TENSE classification

2019-02-12 19:14:43,981 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 19:14:44,390 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 19:14:44,452 : loading BERT mode bert-base-uncased
2019-02-12 19:14:44,452 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:14:44,478 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:14:44,478 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg05oxjx6
2019-02-12 19:14:46,892 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:14:48,309 : Computing embeddings for train/dev/test
2019-02-12 19:17:06,460 : Computed embeddings
2019-02-12 19:17:06,460 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:18:21,340 : [('reg:1e-05', 81.56), ('reg:0.0001', 81.64), ('reg:0.001', 81.59), ('reg:0.01', 79.84)]
2019-02-12 19:18:21,340 : Validation : best param found is reg = 0.0001 with score             81.64
2019-02-12 19:18:21,340 : Evaluating...
2019-02-12 19:18:40,704 : 
Dev acc : 81.6 Test acc : 80.6 for SUBJNUMBER classification

2019-02-12 19:18:40,712 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 19:18:41,114 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 19:18:41,180 : loading BERT mode bert-base-uncased
2019-02-12 19:18:41,180 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:18:41,293 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:18:41,293 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1w9hkrao
2019-02-12 19:18:43,678 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:18:45,159 : Computing embeddings for train/dev/test
2019-02-12 19:21:16,930 : Computed embeddings
2019-02-12 19:21:16,930 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:22:24,765 : [('reg:1e-05', 80.65), ('reg:0.0001', 80.66), ('reg:0.001', 80.42), ('reg:0.01', 78.41)]
2019-02-12 19:22:24,765 : Validation : best param found is reg = 0.0001 with score             80.66
2019-02-12 19:22:24,765 : Evaluating...
2019-02-12 19:22:41,403 : 
Dev acc : 80.7 Test acc : 81.3 for OBJNUMBER classification

2019-02-12 19:22:41,411 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 19:22:41,785 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 19:22:41,853 : loading BERT mode bert-base-uncased
2019-02-12 19:22:41,853 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:22:41,974 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:22:41,974 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph0gs2h88
2019-02-12 19:22:44,397 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:22:45,850 : Computing embeddings for train/dev/test
2019-02-12 19:25:26,244 : Computed embeddings
2019-02-12 19:25:26,245 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:26:31,480 : [('reg:1e-05', 52.19), ('reg:0.0001', 52.17), ('reg:0.001', 52.17), ('reg:0.01', 51.52)]
2019-02-12 19:26:31,480 : Validation : best param found is reg = 1e-05 with score             52.19
2019-02-12 19:26:31,480 : Evaluating...
2019-02-12 19:26:56,890 : 
Dev acc : 52.2 Test acc : 51.9 for ODDMANOUT classification

2019-02-12 19:26:56,898 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 19:26:57,478 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 19:26:57,553 : loading BERT mode bert-base-uncased
2019-02-12 19:26:57,553 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:26:57,582 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:26:57,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcjxaswof
2019-02-12 19:26:59,980 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:27:01,510 : Computing embeddings for train/dev/test
2019-02-12 19:29:49,956 : Computed embeddings
2019-02-12 19:29:49,956 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:31:23,349 : [('reg:1e-05', 53.87), ('reg:0.0001', 53.81), ('reg:0.001', 53.39), ('reg:0.01', 51.77)]
2019-02-12 19:31:23,349 : Validation : best param found is reg = 1e-05 with score             53.87
2019-02-12 19:31:23,349 : Evaluating...
2019-02-12 19:31:48,912 : 
Dev acc : 53.9 Test acc : 54.4 for COORDINATIONINVERSION classification

2019-02-12 19:31:48,921 : {'STS12': {'MSRpar': {'pearson': (0.4115698730785093, 5.037196737842292e-32), 'spearman': SpearmanrResult(correlation=0.4495488118834225, pvalue=1.3795272049822265e-38), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6776352699462084, 6.1947534732058175e-102), 'spearman': SpearmanrResult(correlation=0.6797159941043616, pvalue=8.714446384967709e-103), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5042994973169879, 5.556967587205416e-31), 'spearman': SpearmanrResult(correlation=0.603904918359693, pvalue=5.883444019880835e-47), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.706654728174429, 1.7271051687514914e-114), 'spearman': SpearmanrResult(correlation=0.6944707535211025, pvalue=4.909743040333514e-109), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5447132988295186, 3.306977455407203e-32), 'spearman': SpearmanrResult(correlation=0.5010169970962378, pvalue=9.589475296576469e-27), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5689745334691306, 'wmean': 0.5777699417312855}, 'spearman': {'mean': 0.5857314949929635, 'wmean': 0.5935971071429096}}}, 'STS13': {'FNWN': {'pearson': (0.456854546871803, 3.903171796935881e-11), 'spearman': SpearmanrResult(correlation=0.4717088553428003, pvalue=7.312691372453695e-12), 'nsamples': 189}, 'headlines': {'pearson': (0.6849128810333198, 6.04762930898994e-105), 'spearman': SpearmanrResult(correlation=0.6621610206172825, pvalue=8.17199058363817e-96), 'nsamples': 750}, 'OnWN': {'pearson': (0.5626528641183649, 3.7296084174228235e-48), 'spearman': SpearmanrResult(correlation=0.5900119464093035, pvalue=6.55693073112438e-54), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5681400973411627, 'wmean': 0.6104522846027756}, 'spearman': {'mean': 0.5746272741231288, 'wmean': 0.6111802940389136}}}, 'STS14': {'deft-forum': {'pearson': (0.3706466345753305, 4.216284966600407e-16), 'spearman': SpearmanrResult(correlation=0.3778186581508171, pvalue=1.022318892363268e-16), 'nsamples': 450}, 'deft-news': {'pearson': (0.7256974962813679, 2.549129810625089e-50), 'spearman': SpearmanrResult(correlation=0.6912153567054475, pvalue=6.076064894471612e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.6490164124552856, 6.779609518406149e-91), 'spearman': SpearmanrResult(correlation=0.6007184774219916, pvalue=9.492344143059358e-75), 'nsamples': 750}, 'images': {'pearson': (0.6885331838229208, 1.7818725636718904e-106), 'spearman': SpearmanrResult(correlation=0.6665817108687678, pvalue=1.5902479784666192e-97), 'nsamples': 750}, 'OnWN': {'pearson': (0.6818791198540003, 1.1147919154679959e-103), 'spearman': SpearmanrResult(correlation=0.7212699129911014, pvalue=2.0534850176300252e-121), 'nsamples': 750}, 'tweet-news': {'pearson': (0.7007430231853985, 8.28175878673569e-112), 'spearman': SpearmanrResult(correlation=0.6655428944276202, pvalue=4.037755165667666e-97), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.636085978362384, 'wmean': 0.6465677437150701}, 'spearman': {'mean': 0.6205245017609576, 'wmean': 0.6314580666564301}}}, 'STS15': {'answers-forums': {'pearson': (0.5669477516524087, 2.8331697406099518e-33), 'spearman': SpearmanrResult(correlation=0.551974653843101, pvalue=2.74906856069651e-31), 'nsamples': 375}, 'answers-students': {'pearson': (0.724199278993236, 7.427639684655011e-123), 'spearman': SpearmanrResult(correlation=0.7330307681545996, pvalue=2.566044398122116e-127), 'nsamples': 750}, 'belief': {'pearson': (0.6536142959461023, 4.622192058847672e-47), 'spearman': SpearmanrResult(correlation=0.6670153733112454, pvalue=1.3003853422191857e-49), 'nsamples': 375}, 'headlines': {'pearson': (0.7103387328288516, 3.408382438164426e-116), 'spearman': SpearmanrResult(correlation=0.6993656203781194, pvalue=3.413999303827497e-111), 'nsamples': 750}, 'images': {'pearson': (0.7583617989300914, 3.6220526574568447e-141), 'spearman': SpearmanrResult(correlation=0.7661159458479145, pvalue=9.36607391781379e-146), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.682692371670138, 'wmean': 0.7007952086378585}, 'spearman': {'mean': 0.683500472306996, 'wmean': 0.7020018369894517}}}, 'STS16': {'answer-answer': {'pearson': (0.4951157582298667, 4.094327798317896e-17), 'spearman': SpearmanrResult(correlation=0.5276593234670046, pvalue=1.3111293149757351e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.7121472433808903, 7.95745265203215e-40), 'spearman': SpearmanrResult(correlation=0.7129035672165338, pvalue=6.065927543550842e-40), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7684931510491422, 4.1939265215603935e-46), 'spearman': SpearmanrResult(correlation=0.7703667937765707, pvalue=1.86930642696349e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.8227194233465925, 2.6827771055389183e-61), 'spearman': SpearmanrResult(correlation=0.8362344162937558, pvalue=4.392741298360782e-65), 'nsamples': 244}, 'question-question': {'pearson': (0.4693857011896893, 7.551090483266258e-13), 'spearman': SpearmanrResult(correlation=0.4657959407762339, pvalue=1.1866802239318185e-12), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6535722554392362, 'wmean': 0.656562092562179}, 'spearman': {'mean': 0.6625920083060197, 'wmean': 0.6662018281315822}}}, 'MR': {'devacc': 75.17, 'acc': 74.71, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.58, 'acc': 78.01, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.88, 'acc': 87.88, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.96, 'acc': 92.45, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.67, 'acc': 76.55, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.05, 'acc': 41.22, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.18, 'acc': 90.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.85, 'acc': 73.22, 'f1': 81.31, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 81.2, 'acc': 79.56, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8237443352877583, 'pearson': 0.8329372932713901, 'spearman': 0.7594390801065386, 'mse': 0.3134001024837591, 'yhat': array([3.95354949, 4.19727119, 1.23979649, ..., 3.421574  , 4.46573708,
       4.32375173]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7559527821468942, 'pearson': 0.7058771519466621, 'spearman': 0.703691280303891, 'mse': 1.3937725223639443, 'yhat': array([1.44652761, 1.59174253, 1.84302307, ..., 3.64178831, 3.70489176,
       3.40347257]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 95.3, 'acc': 95.35, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 89.99, 'acc': 89.92, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 33.7, 'acc': 33.96, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 63.66, 'acc': 64.39, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 55.18, 'acc': 54.73, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.72, 'acc': 84.93, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.64, 'acc': 80.6, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.66, 'acc': 81.34, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 52.19, 'acc': 51.9, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.87, 'acc': 54.36, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 19:31:48,921 : ********************************************************************************
2019-02-12 19:31:48,921 : ********************************************************************************
2019-02-12 19:31:48,921 : ********************************************************************************
2019-02-12 19:31:48,921 : layer 2
2019-02-12 19:31:48,921 : ********************************************************************************
2019-02-12 19:31:48,921 : ********************************************************************************
2019-02-12 19:31:48,921 : ********************************************************************************
2019-02-12 19:31:49,004 : ***** Transfer task : STS12 *****


2019-02-12 19:31:49,016 : loading BERT mode bert-base-uncased
2019-02-12 19:31:49,016 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:31:49,033 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:31:49,034 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi2aaall2
2019-02-12 19:31:51,433 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:31:55,477 : MSRpar : pearson = 0.4183, spearman = 0.4531
2019-02-12 19:31:57,278 : MSRvid : pearson = 0.6548, spearman = 0.6577
2019-02-12 19:31:58,589 : SMTeuroparl : pearson = 0.5035, spearman = 0.6006
2019-02-12 19:32:00,819 : surprise.OnWN : pearson = 0.7034, spearman = 0.6899
2019-02-12 19:32:02,015 : surprise.SMTnews : pearson = 0.5523, spearman = 0.5146
2019-02-12 19:32:02,016 : ALL (weighted average) : Pearson = 0.5740,             Spearman = 0.5893
2019-02-12 19:32:02,016 : ALL (average) : Pearson = 0.5665,             Spearman = 0.5832

2019-02-12 19:32:02,016 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 19:32:02,023 : loading BERT mode bert-base-uncased
2019-02-12 19:32:02,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:32:02,041 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:32:02,041 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk55p4xwl
2019-02-12 19:32:04,469 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:32:06,850 : FNWN : pearson = 0.4663, spearman = 0.4824
2019-02-12 19:32:08,832 : headlines : pearson = 0.6743, spearman = 0.6529
2019-02-12 19:32:10,346 : OnWN : pearson = 0.5408, spearman = 0.5667
2019-02-12 19:32:10,347 : ALL (weighted average) : Pearson = 0.5982,             Spearman = 0.5992
2019-02-12 19:32:10,347 : ALL (average) : Pearson = 0.5605,             Spearman = 0.5673

2019-02-12 19:32:10,347 : ***** Transfer task : STS14 *****


2019-02-12 19:32:10,366 : loading BERT mode bert-base-uncased
2019-02-12 19:32:10,367 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:32:10,384 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:32:10,384 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz8nsyyn8
2019-02-12 19:32:12,809 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:32:15,582 : deft-forum : pearson = 0.3585, spearman = 0.3760
2019-02-12 19:32:16,808 : deft-news : pearson = 0.7372, spearman = 0.7022
2019-02-12 19:32:18,886 : headlines : pearson = 0.6352, spearman = 0.5915
2019-02-12 19:32:21,067 : images : pearson = 0.6663, spearman = 0.6472
2019-02-12 19:32:23,254 : OnWN : pearson = 0.6771, spearman = 0.7180
2019-02-12 19:32:25,741 : tweet-news : pearson = 0.6738, spearman = 0.6446
2019-02-12 19:32:25,741 : ALL (weighted average) : Pearson = 0.6325,             Spearman = 0.6215
2019-02-12 19:32:25,741 : ALL (average) : Pearson = 0.6247,             Spearman = 0.6132

2019-02-12 19:32:25,741 : ***** Transfer task : STS15 *****


2019-02-12 19:32:25,773 : loading BERT mode bert-base-uncased
2019-02-12 19:32:25,773 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:32:25,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:32:25,791 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqgdz_mxr
2019-02-12 19:32:28,196 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:32:31,152 : answers-forums : pearson = 0.5400, spearman = 0.5348
2019-02-12 19:32:33,416 : answers-students : pearson = 0.7131, spearman = 0.7185
2019-02-12 19:32:34,922 : belief : pearson = 0.6510, spearman = 0.6739
2019-02-12 19:32:37,279 : headlines : pearson = 0.7007, spearman = 0.6893
2019-02-12 19:32:39,631 : images : pearson = 0.7447, spearman = 0.7547
2019-02-12 19:32:39,631 : ALL (weighted average) : Pearson = 0.6885,             Spearman = 0.6917
2019-02-12 19:32:39,631 : ALL (average) : Pearson = 0.6699,             Spearman = 0.6742

2019-02-12 19:32:39,631 : ***** Transfer task : STS16 *****


2019-02-12 19:32:39,703 : loading BERT mode bert-base-uncased
2019-02-12 19:32:39,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:32:39,721 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:32:39,721 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzn26k7v4
2019-02-12 19:32:42,137 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:32:44,406 : answer-answer : pearson = 0.4973, spearman = 0.5231
2019-02-12 19:32:45,212 : headlines : pearson = 0.7055, spearman = 0.7050
2019-02-12 19:32:46,110 : plagiarism : pearson = 0.7417, spearman = 0.7448
2019-02-12 19:32:47,235 : postediting : pearson = 0.8125, spearman = 0.8305
2019-02-12 19:32:47,928 : question-question : pearson = 0.4694, spearman = 0.4718
2019-02-12 19:32:47,928 : ALL (weighted average) : Pearson = 0.6483,             Spearman = 0.6585
2019-02-12 19:32:47,928 : ALL (average) : Pearson = 0.6453,             Spearman = 0.6550

2019-02-12 19:32:47,928 : ***** Transfer task : MR *****


2019-02-12 19:32:47,944 : loading BERT mode bert-base-uncased
2019-02-12 19:32:47,945 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:32:47,965 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:32:47,965 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzo7vo29f
2019-02-12 19:32:50,422 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:32:51,945 : Generating sentence embeddings
2019-02-12 19:33:11,924 : Generated sentence embeddings
2019-02-12 19:33:11,925 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:33:28,059 : Best param found at split 1: l2reg = 0.0001                 with score 75.16
2019-02-12 19:33:43,112 : Best param found at split 2: l2reg = 0.001                 with score 75.12
2019-02-12 19:34:05,994 : Best param found at split 3: l2reg = 0.0001                 with score 75.39
2019-02-12 19:34:31,413 : Best param found at split 4: l2reg = 0.001                 with score 74.99
2019-02-12 19:35:04,955 : Best param found at split 5: l2reg = 0.0001                 with score 74.89
2019-02-12 19:35:07,001 : Dev acc : 75.11 Test acc : 74.64

2019-02-12 19:35:07,002 : ***** Transfer task : CR *****


2019-02-12 19:35:07,010 : loading BERT mode bert-base-uncased
2019-02-12 19:35:07,010 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:35:07,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:35:07,029 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsy646rvs
2019-02-12 19:35:09,470 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:35:11,022 : Generating sentence embeddings
2019-02-12 19:35:18,831 : Generated sentence embeddings
2019-02-12 19:35:18,831 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:35:32,776 : Best param found at split 1: l2reg = 0.0001                 with score 79.5
2019-02-12 19:35:47,632 : Best param found at split 2: l2reg = 0.01                 with score 79.86
2019-02-12 19:36:00,345 : Best param found at split 3: l2reg = 0.01                 with score 80.76
2019-02-12 19:36:12,162 : Best param found at split 4: l2reg = 1e-05                 with score 80.07
2019-02-12 19:36:22,079 : Best param found at split 5: l2reg = 1e-05                 with score 80.3
2019-02-12 19:36:22,555 : Dev acc : 80.1 Test acc : 78.2

2019-02-12 19:36:22,555 : ***** Transfer task : MPQA *****


2019-02-12 19:36:22,561 : loading BERT mode bert-base-uncased
2019-02-12 19:36:22,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:36:22,580 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:36:22,580 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0o8tje4r
2019-02-12 19:36:25,000 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:36:26,488 : Generating sentence embeddings
2019-02-12 19:36:37,542 : Generated sentence embeddings
2019-02-12 19:36:37,543 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:37:04,485 : Best param found at split 1: l2reg = 0.001                 with score 87.07
2019-02-12 19:37:21,686 : Best param found at split 2: l2reg = 0.001                 with score 87.92
2019-02-12 19:37:49,631 : Best param found at split 3: l2reg = 0.001                 with score 88.17
2019-02-12 19:38:17,637 : Best param found at split 4: l2reg = 1e-05                 with score 87.54
2019-02-12 19:38:48,411 : Best param found at split 5: l2reg = 0.01                 with score 87.44
2019-02-12 19:38:49,657 : Dev acc : 87.63 Test acc : 87.92

2019-02-12 19:38:49,658 : ***** Transfer task : SUBJ *****


2019-02-12 19:38:49,672 : loading BERT mode bert-base-uncased
2019-02-12 19:38:49,672 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:38:49,694 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:38:49,694 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptppiwapr
2019-02-12 19:38:52,081 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:38:53,640 : Generating sentence embeddings
2019-02-12 19:39:17,881 : Generated sentence embeddings
2019-02-12 19:39:17,882 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 19:39:48,969 : Best param found at split 1: l2reg = 0.001                 with score 92.98
2019-02-12 19:40:14,527 : Best param found at split 2: l2reg = 0.001                 with score 93.61
2019-02-12 19:40:41,508 : Best param found at split 3: l2reg = 0.001                 with score 93.04
2019-02-12 19:41:10,339 : Best param found at split 4: l2reg = 0.001                 with score 93.44
2019-02-12 19:41:29,924 : Best param found at split 5: l2reg = 0.001                 with score 93.25
2019-02-12 19:41:30,983 : Dev acc : 93.26 Test acc : 92.78

2019-02-12 19:41:30,984 : ***** Transfer task : SST Binary classification *****


2019-02-12 19:41:31,124 : loading BERT mode bert-base-uncased
2019-02-12 19:41:31,124 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:41:31,150 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:41:31,150 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0phh0xk4
2019-02-12 19:41:33,720 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:41:35,266 : Computing embedding for train
2019-02-12 19:42:48,883 : Computed train embeddings
2019-02-12 19:42:48,883 : Computing embedding for dev
2019-02-12 19:42:49,930 : Computed dev embeddings
2019-02-12 19:42:49,930 : Computing embedding for test
2019-02-12 19:42:52,126 : Computed test embeddings
2019-02-12 19:42:52,126 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:43:32,112 : [('reg:1e-05', 79.01), ('reg:0.0001', 78.67), ('reg:0.001', 78.56), ('reg:0.01', 78.1)]
2019-02-12 19:43:32,112 : Validation : best param found is reg = 1e-05 with score             79.01
2019-02-12 19:43:32,112 : Evaluating...
2019-02-12 19:43:44,818 : 
Dev acc : 79.01 Test acc : 79.79 for             SST Binary classification

2019-02-12 19:43:44,824 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 19:43:44,875 : loading BERT mode bert-base-uncased
2019-02-12 19:43:44,875 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:43:44,895 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:43:44,895 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5vzn1_5c
2019-02-12 19:43:47,319 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:43:48,807 : Computing embedding for train
2019-02-12 19:44:04,365 : Computed train embeddings
2019-02-12 19:44:04,365 : Computing embedding for dev
2019-02-12 19:44:06,459 : Computed dev embeddings
2019-02-12 19:44:06,459 : Computing embedding for test
2019-02-12 19:44:10,630 : Computed test embeddings
2019-02-12 19:44:10,630 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:44:16,492 : [('reg:1e-05', 40.24), ('reg:0.0001', 40.24), ('reg:0.001', 40.6), ('reg:0.01', 40.78)]
2019-02-12 19:44:16,492 : Validation : best param found is reg = 0.01 with score             40.78
2019-02-12 19:44:16,492 : Evaluating...
2019-02-12 19:44:17,956 : 
Dev acc : 40.78 Test acc : 40.05 for             SST Fine-Grained classification

2019-02-12 19:44:17,956 : ***** Transfer task : TREC *****


2019-02-12 19:44:17,969 : loading BERT mode bert-base-uncased
2019-02-12 19:44:17,969 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:44:17,987 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:44:17,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7dhoebf_
2019-02-12 19:44:20,405 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:44:30,323 : Computed train embeddings
2019-02-12 19:44:31,066 : Computed test embeddings
2019-02-12 19:44:31,066 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 19:44:51,234 : [('reg:1e-05', 80.52), ('reg:0.0001', 80.5), ('reg:0.001', 79.57), ('reg:0.01', 71.57)]
2019-02-12 19:44:51,234 : Cross-validation : best param found is reg = 1e-05             with score 80.52
2019-02-12 19:44:51,235 : Evaluating...
2019-02-12 19:44:53,103 : 
Dev acc : 80.52 Test acc : 90.0             for TREC

2019-02-12 19:44:53,104 : ***** Transfer task : MRPC *****


2019-02-12 19:44:53,125 : loading BERT mode bert-base-uncased
2019-02-12 19:44:53,125 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:44:53,145 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:44:53,146 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7us5o9mi
2019-02-12 19:44:55,584 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:44:57,111 : Computing embedding for train
2019-02-12 19:45:11,912 : Computed train embeddings
2019-02-12 19:45:11,912 : Computing embedding for test
2019-02-12 19:45:18,857 : Computed test embeddings
2019-02-12 19:45:18,873 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 19:45:26,459 : [('reg:1e-05', 73.33), ('reg:0.0001', 73.38), ('reg:0.001', 73.38), ('reg:0.01', 72.99)]
2019-02-12 19:45:26,459 : Cross-validation : best param found is reg = 0.0001             with score 73.38
2019-02-12 19:45:26,459 : Evaluating...
2019-02-12 19:45:26,901 : Dev acc : 73.38 Test acc 73.8; Test F1 80.57 for MRPC.

2019-02-12 19:45:26,901 : ***** Transfer task : SICK-Entailment*****


2019-02-12 19:45:26,967 : loading BERT mode bert-base-uncased
2019-02-12 19:45:26,967 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:45:26,986 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:45:26,987 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk6liqw86
2019-02-12 19:45:29,431 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:45:30,873 : Computing embedding for train
2019-02-12 19:45:38,501 : Computed train embeddings
2019-02-12 19:45:38,501 : Computing embedding for dev
2019-02-12 19:45:39,313 : Computed dev embeddings
2019-02-12 19:45:39,313 : Computing embedding for test
2019-02-12 19:45:47,615 : Computed test embeddings
2019-02-12 19:45:47,643 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:45:51,204 : [('reg:1e-05', 80.4), ('reg:0.0001', 80.8), ('reg:0.001', 79.8), ('reg:0.01', 73.0)]
2019-02-12 19:45:51,204 : Validation : best param found is reg = 0.0001 with score             80.8
2019-02-12 19:45:51,204 : Evaluating...
2019-02-12 19:45:51,927 : 
Dev acc : 80.8 Test acc : 79.87 for                        SICK entailment

2019-02-12 19:45:51,927 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 19:45:51,954 : loading BERT mode bert-base-uncased
2019-02-12 19:45:51,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:45:52,011 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:45:52,011 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyb3zicbe
2019-02-12 19:45:54,485 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:45:55,894 : Computing embedding for train
2019-02-12 19:46:03,495 : Computed train embeddings
2019-02-12 19:46:03,496 : Computing embedding for dev
2019-02-12 19:46:04,455 : Computed dev embeddings
2019-02-12 19:46:04,456 : Computing embedding for test
2019-02-12 19:46:12,657 : Computed test embeddings
2019-02-12 19:46:49,274 : Dev : Pearson 0.8189028295858121
2019-02-12 19:46:49,274 : Test : Pearson 0.8291880187610355 Spearman 0.7553988012935134 MSE 0.31995341456434395                        for SICK Relatedness

2019-02-12 19:46:49,275 : 

***** Transfer task : STSBenchmark*****


2019-02-12 19:46:49,345 : loading BERT mode bert-base-uncased
2019-02-12 19:46:49,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:46:49,365 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:46:49,365 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkkf49uv6
2019-02-12 19:46:51,784 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:46:53,349 : Computing embedding for train
2019-02-12 19:47:09,169 : Computed train embeddings
2019-02-12 19:47:09,170 : Computing embedding for dev
2019-02-12 19:47:13,696 : Computed dev embeddings
2019-02-12 19:47:13,696 : Computing embedding for test
2019-02-12 19:47:17,676 : Computed test embeddings
2019-02-12 19:48:27,620 : Dev : Pearson 0.7464978427102179
2019-02-12 19:48:27,620 : Test : Pearson 0.702112468179167 Spearman 0.6994958784546036 MSE 1.3678795004455795                        for SICK Relatedness

2019-02-12 19:48:27,620 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 19:48:27,936 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 19:48:27,946 : loading BERT mode bert-base-uncased
2019-02-12 19:48:27,946 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:48:27,968 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:48:27,969 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpom7qx6h7
2019-02-12 19:48:30,393 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:48:31,889 : Computing embeddings for train/dev/test
2019-02-12 19:50:50,122 : Computed embeddings
2019-02-12 19:50:50,123 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:52:05,585 : [('reg:1e-05', 94.48), ('reg:0.0001', 92.61), ('reg:0.001', 88.45), ('reg:0.01', 79.35)]
2019-02-12 19:52:05,585 : Validation : best param found is reg = 1e-05 with score             94.48
2019-02-12 19:52:05,585 : Evaluating...
2019-02-12 19:52:22,706 : 
Dev acc : 94.5 Test acc : 94.3 for LENGTH classification

2019-02-12 19:52:22,714 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 19:52:23,057 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 19:52:23,102 : loading BERT mode bert-base-uncased
2019-02-12 19:52:23,102 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:52:23,130 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:52:23,131 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwv2wm4kd
2019-02-12 19:52:25,564 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:52:26,985 : Computing embeddings for train/dev/test
2019-02-12 19:54:38,663 : Computed embeddings
2019-02-12 19:54:38,663 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:55:48,512 : [('reg:1e-05', 88.15), ('reg:0.0001', 60.7), ('reg:0.001', 5.52), ('reg:0.01', 0.98)]
2019-02-12 19:55:48,512 : Validation : best param found is reg = 1e-05 with score             88.15
2019-02-12 19:55:48,512 : Evaluating...
2019-02-12 19:56:03,718 : 
Dev acc : 88.2 Test acc : 88.2 for WORDCONTENT classification

2019-02-12 19:56:03,728 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 19:56:04,077 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 19:56:04,142 : loading BERT mode bert-base-uncased
2019-02-12 19:56:04,142 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:56:04,240 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:56:04,240 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkou84tfp
2019-02-12 19:56:06,678 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:56:08,142 : Computing embeddings for train/dev/test
2019-02-12 19:58:15,596 : Computed embeddings
2019-02-12 19:58:15,596 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 19:59:07,276 : [('reg:1e-05', 35.34), ('reg:0.0001', 34.98), ('reg:0.001', 32.85), ('reg:0.01', 27.64)]
2019-02-12 19:59:07,276 : Validation : best param found is reg = 1e-05 with score             35.34
2019-02-12 19:59:07,276 : Evaluating...
2019-02-12 19:59:23,461 : 
Dev acc : 35.3 Test acc : 35.1 for DEPTH classification

2019-02-12 19:59:23,470 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 19:59:24,022 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 19:59:24,083 : loading BERT mode bert-base-uncased
2019-02-12 19:59:24,083 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 19:59:24,109 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 19:59:24,110 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnpsdt6k1
2019-02-12 19:59:26,545 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 19:59:28,019 : Computing embeddings for train/dev/test
2019-02-12 20:01:59,651 : Computed embeddings
2019-02-12 20:01:59,651 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:03:06,257 : [('reg:1e-05', 69.94), ('reg:0.0001', 66.75), ('reg:0.001', 57.05), ('reg:0.01', 44.3)]
2019-02-12 20:03:06,257 : Validation : best param found is reg = 1e-05 with score             69.94
2019-02-12 20:03:06,257 : Evaluating...
2019-02-12 20:03:30,393 : 
Dev acc : 69.9 Test acc : 70.0 for TOPCONSTITUENTS classification

2019-02-12 20:03:30,401 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 20:03:30,738 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 20:03:30,803 : loading BERT mode bert-base-uncased
2019-02-12 20:03:30,804 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:03:30,922 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:03:30,922 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0oi6vrfo
2019-02-12 20:03:33,338 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:03:34,902 : Computing embeddings for train/dev/test
2019-02-12 20:06:20,484 : Computed embeddings
2019-02-12 20:06:20,484 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:07:42,089 : [('reg:1e-05', 69.84), ('reg:0.0001', 69.85), ('reg:0.001', 68.85), ('reg:0.01', 64.67)]
2019-02-12 20:07:42,089 : Validation : best param found is reg = 0.0001 with score             69.85
2019-02-12 20:07:42,089 : Evaluating...
2019-02-12 20:08:07,847 : 
Dev acc : 69.8 Test acc : 69.5 for BIGRAMSHIFT classification

2019-02-12 20:08:07,855 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 20:08:08,408 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 20:08:08,473 : loading BERT mode bert-base-uncased
2019-02-12 20:08:08,473 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:08:08,503 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:08:08,503 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0duskmik
2019-02-12 20:08:10,917 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:08:12,381 : Computing embeddings for train/dev/test
2019-02-12 20:11:08,156 : Computed embeddings
2019-02-12 20:11:08,156 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:12:15,563 : [('reg:1e-05', 87.51), ('reg:0.0001', 87.57), ('reg:0.001', 87.76), ('reg:0.01', 87.67)]
2019-02-12 20:12:15,563 : Validation : best param found is reg = 0.001 with score             87.76
2019-02-12 20:12:15,563 : Evaluating...
2019-02-12 20:12:30,567 : 
Dev acc : 87.8 Test acc : 87.1 for TENSE classification

2019-02-12 20:12:30,575 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 20:12:31,142 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 20:12:31,204 : loading BERT mode bert-base-uncased
2019-02-12 20:12:31,204 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:12:31,232 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:12:31,232 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzsiptq06
2019-02-12 20:12:33,688 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:12:35,153 : Computing embeddings for train/dev/test
2019-02-12 20:15:15,810 : Computed embeddings
2019-02-12 20:15:15,811 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:16:19,520 : [('reg:1e-05', 81.55), ('reg:0.0001', 81.49), ('reg:0.001', 81.22), ('reg:0.01', 79.07)]
2019-02-12 20:16:19,520 : Validation : best param found is reg = 1e-05 with score             81.55
2019-02-12 20:16:19,520 : Evaluating...
2019-02-12 20:16:36,648 : 
Dev acc : 81.5 Test acc : 80.2 for SUBJNUMBER classification

2019-02-12 20:16:36,656 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 20:16:37,073 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 20:16:37,138 : loading BERT mode bert-base-uncased
2019-02-12 20:16:37,138 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:16:37,165 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:16:37,165 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn14hqr9z
2019-02-12 20:16:39,572 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:16:41,042 : Computing embeddings for train/dev/test
2019-02-12 20:19:30,044 : Computed embeddings
2019-02-12 20:19:30,045 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:20:37,419 : [('reg:1e-05', 81.13), ('reg:0.0001', 81.13), ('reg:0.001', 81.13), ('reg:0.01', 80.04)]
2019-02-12 20:20:37,420 : Validation : best param found is reg = 1e-05 with score             81.13
2019-02-12 20:20:37,420 : Evaluating...
2019-02-12 20:20:50,685 : 
Dev acc : 81.1 Test acc : 82.1 for OBJNUMBER classification

2019-02-12 20:20:50,693 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 20:20:51,080 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 20:20:51,148 : loading BERT mode bert-base-uncased
2019-02-12 20:20:51,149 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:20:51,270 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:20:51,270 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph81hx7k7
2019-02-12 20:20:53,696 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:20:55,178 : Computing embeddings for train/dev/test
2019-02-12 20:23:35,990 : Computed embeddings
2019-02-12 20:23:35,990 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:24:56,556 : [('reg:1e-05', 53.62), ('reg:0.0001', 53.7), ('reg:0.001', 53.43), ('reg:0.01', 52.53)]
2019-02-12 20:24:56,556 : Validation : best param found is reg = 0.0001 with score             53.7
2019-02-12 20:24:56,556 : Evaluating...
2019-02-12 20:25:16,832 : 
Dev acc : 53.7 Test acc : 53.1 for ODDMANOUT classification

2019-02-12 20:25:16,839 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 20:25:17,230 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 20:25:17,305 : loading BERT mode bert-base-uncased
2019-02-12 20:25:17,305 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:25:17,427 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:25:17,428 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9exss98p
2019-02-12 20:25:19,868 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:25:21,377 : Computing embeddings for train/dev/test
2019-02-12 20:28:03,968 : Computed embeddings
2019-02-12 20:28:03,968 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:29:33,652 : [('reg:1e-05', 56.68), ('reg:0.0001', 56.7), ('reg:0.001', 55.35), ('reg:0.01', 52.19)]
2019-02-12 20:29:33,653 : Validation : best param found is reg = 0.0001 with score             56.7
2019-02-12 20:29:33,653 : Evaluating...
2019-02-12 20:29:54,914 : 
Dev acc : 56.7 Test acc : 56.9 for COORDINATIONINVERSION classification

2019-02-12 20:29:54,924 : {'STS12': {'MSRpar': {'pearson': (0.4182776298763544, 4.009544040927623e-33), 'spearman': SpearmanrResult(correlation=0.4531390767686579, pvalue=2.987053443266199e-39), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6547588145686539, 5.159713227403249e-93), 'spearman': SpearmanrResult(correlation=0.6577489276214195, pvalue=3.899457927592248e-94), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5035465825045548, 7.021171962595966e-31), 'spearman': SpearmanrResult(correlation=0.6005500630230636, pvalue=2.5188878312361366e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.7034486168293742, 5.0057087724969825e-113), 'spearman': SpearmanrResult(correlation=0.6899093802430127, pvalue=4.603434101188696e-107), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5523038388810086, 3.0671000827197627e-33), 'spearman': SpearmanrResult(correlation=0.5146082245288787, pvalue=2.326907891944589e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5664670965319892, 'wmean': 0.5739578214410875}, 'spearman': {'mean': 0.5831911344370064, 'wmean': 0.5893111965860446}}}, 'STS13': {'FNWN': {'pearson': (0.4663138574665031, 1.3561283720130955e-11), 'spearman': SpearmanrResult(correlation=0.4824026214265076, pvalue=2.0813734335483648e-12), 'nsamples': 189}, 'headlines': {'pearson': (0.6742946517275877, 1.396170085842768e-100), 'spearman': SpearmanrResult(correlation=0.6529476788926748, pvalue=2.43113227188067e-92), 'nsamples': 750}, 'OnWN': {'pearson': (0.5408038648717477, 6.266680797501094e-44), 'spearman': SpearmanrResult(correlation=0.5666622621140254, pvalue=5.777279809397946e-49), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5604707913552794, 'wmean': 0.598163517366607}, 'spearman': {'mean': 0.5673375208110693, 'wmean': 0.5991882557767229}}}, 'STS14': {'deft-forum': {'pearson': (0.358526296480026, 4.2710104638203365e-15), 'spearman': SpearmanrResult(correlation=0.37597647117925004, pvalue=1.4760552109254407e-16), 'nsamples': 450}, 'deft-news': {'pearson': (0.7371679501825404, 1.1592913068246085e-52), 'spearman': SpearmanrResult(correlation=0.702178189706483, pvalue=7.174817238205543e-46), 'nsamples': 300}, 'headlines': {'pearson': (0.6351805589438418, 5.647216658005599e-86), 'spearman': SpearmanrResult(correlation=0.591457299001131, pvalue=5.83426199260668e-72), 'nsamples': 750}, 'images': {'pearson': (0.6662551223699058, 2.132381356039771e-97), 'spearman': SpearmanrResult(correlation=0.6471559783303287, pvalue=3.2194125472797167e-90), 'nsamples': 750}, 'OnWN': {'pearson': (0.6771000863892735, 1.0232324755047387e-101), 'spearman': SpearmanrResult(correlation=0.7180305121248204, pvalue=7.678853411345571e-120), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6737914231024885, 2.224313048234764e-100), 'spearman': SpearmanrResult(correlation=0.6446073778947741, pvalue=2.672748896762475e-89), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6246702395780127, 'wmean': 0.6324620297533082}, 'spearman': {'mean': 0.6132343047061312, 'wmean': 0.6215416651882394}}}, 'STS15': {'answers-forums': {'pearson': (0.5400388365746589, 8.969218644482813e-30), 'spearman': SpearmanrResult(correlation=0.534836115433142, pvalue=3.924743799460956e-29), 'nsamples': 375}, 'answers-students': {'pearson': (0.7131401618711889, 1.6524978252075517e-117), 'spearman': SpearmanrResult(correlation=0.7184863496963562, pvalue=4.627201647010825e-120), 'nsamples': 750}, 'belief': {'pearson': (0.651043782717649, 1.378419935592714e-46), 'spearman': SpearmanrResult(correlation=0.6738990913139672, pvalue=5.642580861866252e-51), 'nsamples': 375}, 'headlines': {'pearson': (0.7007304599115733, 8.389754352650724e-112), 'spearman': SpearmanrResult(correlation=0.6892560463809452, pvalue=8.760749636141584e-107), 'nsamples': 750}, 'images': {'pearson': (0.7447351001105315, 1.6363747771828698e-133), 'spearman': SpearmanrResult(correlation=0.754715774865417, pvalue=4.533122024172415e-139), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6699376682371203, 'wmean': 0.6885367578848619}, 'spearman': {'mean': 0.6742386755379656, 'wmean': 0.6917064435790683}}}, 'STS16': {'answer-answer': {'pearson': (0.49733128732383236, 2.8227680262699535e-17), 'spearman': SpearmanrResult(correlation=0.5230606102238952, pvalue=3.065891912514043e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.7055317794135227, 8.238092174042106e-39), 'spearman': SpearmanrResult(correlation=0.7050411877615843, pvalue=9.771859639016578e-39), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7416860171521443, 2.0238224937715924e-41), 'spearman': SpearmanrResult(correlation=0.7447615618247146, pvalue=6.291332960622266e-42), 'nsamples': 230}, 'postediting': {'pearson': (0.8124814584149181, 1.2254937195950086e-58), 'spearman': SpearmanrResult(correlation=0.8304927467333736, pvalue=1.958677586370302e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.46938291616388883, 7.553754686308985e-13), 'spearman': SpearmanrResult(correlation=0.4718380924718441, pvalue=5.52797247105356e-13), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6452826916936611, 'wmean': 0.6483422001102079}, 'spearman': {'mean': 0.6550388398030823, 'wmean': 0.6584841496616752}}}, 'MR': {'devacc': 75.11, 'acc': 74.64, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.1, 'acc': 78.2, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.63, 'acc': 87.92, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.26, 'acc': 92.78, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.01, 'acc': 79.79, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.78, 'acc': 40.05, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.52, 'acc': 90.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.38, 'acc': 73.8, 'f1': 80.57, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.8, 'acc': 79.87, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8189028295858121, 'pearson': 0.8291880187610355, 'spearman': 0.7553988012935134, 'mse': 0.31995341456434395, 'yhat': array([3.73129528, 4.39443017, 1.30646466, ..., 3.17756424, 4.48952683,
       4.19583398]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7464978427102179, 'pearson': 0.702112468179167, 'spearman': 0.6994958784546036, 'mse': 1.3678795004455795, 'yhat': array([1.43416875, 1.8825786 , 1.99594349, ..., 3.85884025, 3.90270777,
       3.49578379]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 94.48, 'acc': 94.26, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 88.15, 'acc': 88.25, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.34, 'acc': 35.13, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 69.94, 'acc': 70.03, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 69.85, 'acc': 69.46, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.76, 'acc': 87.14, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.55, 'acc': 80.24, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 81.13, 'acc': 82.12, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 53.7, 'acc': 53.13, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.7, 'acc': 56.88, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 20:29:54,924 : ********************************************************************************
2019-02-12 20:29:54,924 : ********************************************************************************
2019-02-12 20:29:54,924 : ********************************************************************************
2019-02-12 20:29:54,924 : layer 3
2019-02-12 20:29:54,924 : ********************************************************************************
2019-02-12 20:29:54,924 : ********************************************************************************
2019-02-12 20:29:54,924 : ********************************************************************************
2019-02-12 20:29:55,013 : ***** Transfer task : STS12 *****


2019-02-12 20:29:55,049 : loading BERT mode bert-base-uncased
2019-02-12 20:29:55,050 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:29:55,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:29:55,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4g3ropcx
2019-02-12 20:29:57,497 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:30:01,690 : MSRpar : pearson = 0.4122, spearman = 0.4495
2019-02-12 20:30:03,666 : MSRvid : pearson = 0.6125, spearman = 0.6165
2019-02-12 20:30:05,121 : SMTeuroparl : pearson = 0.4964, spearman = 0.5871
2019-02-12 20:30:07,507 : surprise.OnWN : pearson = 0.6903, spearman = 0.6824
2019-02-12 20:30:08,786 : surprise.SMTnews : pearson = 0.5818, spearman = 0.5174
2019-02-12 20:30:08,786 : ALL (weighted average) : Pearson = 0.5619,             Spearman = 0.5750
2019-02-12 20:30:08,787 : ALL (average) : Pearson = 0.5587,             Spearman = 0.5706

2019-02-12 20:30:08,787 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 20:30:08,795 : loading BERT mode bert-base-uncased
2019-02-12 20:30:08,795 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:30:08,812 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:30:08,812 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm3qjmnk5
2019-02-12 20:30:11,237 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:30:13,651 : FNWN : pearson = 0.4317, spearman = 0.4437
2019-02-12 20:30:15,738 : headlines : pearson = 0.6611, spearman = 0.6387
2019-02-12 20:30:17,397 : OnWN : pearson = 0.5134, spearman = 0.5429
2019-02-12 20:30:17,397 : ALL (weighted average) : Pearson = 0.5770,             Spearman = 0.5783
2019-02-12 20:30:17,397 : ALL (average) : Pearson = 0.5354,             Spearman = 0.5418

2019-02-12 20:30:17,397 : ***** Transfer task : STS14 *****


2019-02-12 20:30:17,417 : loading BERT mode bert-base-uncased
2019-02-12 20:30:17,417 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:30:17,465 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:30:17,465 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqciojn0h
2019-02-12 20:30:19,887 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:30:22,754 : deft-forum : pearson = 0.3486, spearman = 0.3706
2019-02-12 20:30:24,055 : deft-news : pearson = 0.7287, spearman = 0.6948
2019-02-12 20:30:26,302 : headlines : pearson = 0.6192, spearman = 0.5769
2019-02-12 20:30:28,634 : images : pearson = 0.6202, spearman = 0.6103
2019-02-12 20:30:31,024 : OnWN : pearson = 0.6598, spearman = 0.7035
2019-02-12 20:30:33,705 : tweet-news : pearson = 0.6601, spearman = 0.6312
2019-02-12 20:30:33,705 : ALL (weighted average) : Pearson = 0.6120,             Spearman = 0.6044
2019-02-12 20:30:33,706 : ALL (average) : Pearson = 0.6061,             Spearman = 0.5979

2019-02-12 20:30:33,706 : ***** Transfer task : STS15 *****


2019-02-12 20:30:33,753 : loading BERT mode bert-base-uncased
2019-02-12 20:30:33,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:30:33,771 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:30:33,771 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj3jt7bnh
2019-02-12 20:30:36,204 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:30:38,977 : answers-forums : pearson = 0.5330, spearman = 0.5291
2019-02-12 20:30:40,617 : answers-students : pearson = 0.7067, spearman = 0.7100
2019-02-12 20:30:41,859 : belief : pearson = 0.6371, spearman = 0.6525
2019-02-12 20:30:43,741 : headlines : pearson = 0.6794, spearman = 0.6692
2019-02-12 20:30:45,686 : images : pearson = 0.7290, spearman = 0.7401
2019-02-12 20:30:45,686 : ALL (weighted average) : Pearson = 0.6750,             Spearman = 0.6775
2019-02-12 20:30:45,687 : ALL (average) : Pearson = 0.6570,             Spearman = 0.6601

2019-02-12 20:30:45,687 : ***** Transfer task : STS16 *****


2019-02-12 20:30:45,755 : loading BERT mode bert-base-uncased
2019-02-12 20:30:45,755 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:30:45,772 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:30:45,773 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi_71y98c
2019-02-12 20:30:48,184 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:30:50,418 : answer-answer : pearson = 0.4821, spearman = 0.5125
2019-02-12 20:30:50,955 : headlines : pearson = 0.6921, spearman = 0.6954
2019-02-12 20:30:51,550 : plagiarism : pearson = 0.7368, spearman = 0.7445
2019-02-12 20:30:52,435 : postediting : pearson = 0.7986, spearman = 0.8268
2019-02-12 20:30:52,974 : question-question : pearson = 0.4169, spearman = 0.4174
2019-02-12 20:30:52,974 : ALL (weighted average) : Pearson = 0.6292,             Spearman = 0.6438
2019-02-12 20:30:52,974 : ALL (average) : Pearson = 0.6253,             Spearman = 0.6393

2019-02-12 20:30:52,974 : ***** Transfer task : MR *****


2019-02-12 20:30:52,990 : loading BERT mode bert-base-uncased
2019-02-12 20:30:52,990 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:30:53,010 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:30:53,011 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxg_6vhyg
2019-02-12 20:30:55,434 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:30:56,936 : Generating sentence embeddings
2019-02-12 20:31:15,025 : Generated sentence embeddings
2019-02-12 20:31:15,026 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:31:29,444 : Best param found at split 1: l2reg = 0.001                 with score 75.67
2019-02-12 20:31:44,883 : Best param found at split 2: l2reg = 0.001                 with score 75.66
2019-02-12 20:32:09,346 : Best param found at split 3: l2reg = 0.001                 with score 75.8
2019-02-12 20:32:38,069 : Best param found at split 4: l2reg = 1e-05                 with score 75.45
2019-02-12 20:33:14,147 : Best param found at split 5: l2reg = 0.0001                 with score 75.15
2019-02-12 20:33:15,922 : Dev acc : 75.55 Test acc : 74.53

2019-02-12 20:33:15,923 : ***** Transfer task : CR *****


2019-02-12 20:33:15,931 : loading BERT mode bert-base-uncased
2019-02-12 20:33:15,931 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:33:15,951 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:33:15,951 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmposm95099
2019-02-12 20:33:18,373 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:33:19,936 : Generating sentence embeddings
2019-02-12 20:33:27,773 : Generated sentence embeddings
2019-02-12 20:33:27,773 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:33:40,684 : Best param found at split 1: l2reg = 0.0001                 with score 79.89
2019-02-12 20:33:48,993 : Best param found at split 2: l2reg = 1e-05                 with score 80.23
2019-02-12 20:33:57,673 : Best param found at split 3: l2reg = 0.0001                 with score 80.76
2019-02-12 20:34:07,208 : Best param found at split 4: l2reg = 0.0001                 with score 80.67
2019-02-12 20:34:17,250 : Best param found at split 5: l2reg = 0.01                 with score 80.27
2019-02-12 20:34:17,777 : Dev acc : 80.36 Test acc : 78.57

2019-02-12 20:34:17,777 : ***** Transfer task : MPQA *****


2019-02-12 20:34:17,782 : loading BERT mode bert-base-uncased
2019-02-12 20:34:17,782 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:34:17,801 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:34:17,801 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfhpf3d6_
2019-02-12 20:34:20,227 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:34:21,701 : Generating sentence embeddings
2019-02-12 20:34:32,669 : Generated sentence embeddings
2019-02-12 20:34:32,670 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:35:01,016 : Best param found at split 1: l2reg = 0.0001                 with score 87.08
2019-02-12 20:35:31,862 : Best param found at split 2: l2reg = 0.0001                 with score 87.36
2019-02-12 20:36:01,210 : Best param found at split 3: l2reg = 0.001                 with score 88.06
2019-02-12 20:36:30,144 : Best param found at split 4: l2reg = 1e-05                 with score 87.63
2019-02-12 20:37:12,808 : Best param found at split 5: l2reg = 0.01                 with score 87.59
2019-02-12 20:37:14,325 : Dev acc : 87.54 Test acc : 87.72

2019-02-12 20:37:14,325 : ***** Transfer task : SUBJ *****


2019-02-12 20:37:14,341 : loading BERT mode bert-base-uncased
2019-02-12 20:37:14,342 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:37:14,360 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:37:14,360 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpianbknkp
2019-02-12 20:37:16,790 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:37:18,236 : Generating sentence embeddings
2019-02-12 20:37:37,290 : Generated sentence embeddings
2019-02-12 20:37:37,290 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 20:38:03,627 : Best param found at split 1: l2reg = 0.001                 with score 93.55
2019-02-12 20:38:30,643 : Best param found at split 2: l2reg = 0.001                 with score 93.84
2019-02-12 20:38:54,245 : Best param found at split 3: l2reg = 0.001                 with score 93.38
2019-02-12 20:39:15,540 : Best param found at split 4: l2reg = 0.001                 with score 93.65
2019-02-12 20:39:40,180 : Best param found at split 5: l2reg = 0.001                 with score 93.43
2019-02-12 20:39:41,615 : Dev acc : 93.57 Test acc : 93.16

2019-02-12 20:39:41,616 : ***** Transfer task : SST Binary classification *****


2019-02-12 20:39:41,748 : loading BERT mode bert-base-uncased
2019-02-12 20:39:41,748 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:39:41,770 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:39:41,770 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv8etei0u
2019-02-12 20:39:44,199 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:39:45,668 : Computing embedding for train
2019-02-12 20:41:21,098 : Computed train embeddings
2019-02-12 20:41:21,098 : Computing embedding for dev
2019-02-12 20:41:22,775 : Computed dev embeddings
2019-02-12 20:41:22,776 : Computing embedding for test
2019-02-12 20:41:25,863 : Computed test embeddings
2019-02-12 20:41:25,863 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:42:03,734 : [('reg:1e-05', 81.08), ('reg:0.0001', 81.08), ('reg:0.001', 80.39), ('reg:0.01', 79.13)]
2019-02-12 20:42:03,734 : Validation : best param found is reg = 1e-05 with score             81.08
2019-02-12 20:42:03,734 : Evaluating...
2019-02-12 20:42:13,323 : 
Dev acc : 81.08 Test acc : 79.9 for             SST Binary classification

2019-02-12 20:42:13,329 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 20:42:13,378 : loading BERT mode bert-base-uncased
2019-02-12 20:42:13,378 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:42:13,400 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:42:13,400 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa5fldogz
2019-02-12 20:42:15,831 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:42:17,251 : Computing embedding for train
2019-02-12 20:42:29,169 : Computed train embeddings
2019-02-12 20:42:29,169 : Computing embedding for dev
2019-02-12 20:42:30,718 : Computed dev embeddings
2019-02-12 20:42:30,718 : Computing embedding for test
2019-02-12 20:42:33,782 : Computed test embeddings
2019-02-12 20:42:33,782 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:42:37,188 : [('reg:1e-05', 40.87), ('reg:0.0001', 40.78), ('reg:0.001', 41.05), ('reg:0.01', 41.05)]
2019-02-12 20:42:37,188 : Validation : best param found is reg = 0.001 with score             41.05
2019-02-12 20:42:37,189 : Evaluating...
2019-02-12 20:42:38,042 : 
Dev acc : 41.05 Test acc : 41.58 for             SST Fine-Grained classification

2019-02-12 20:42:38,042 : ***** Transfer task : TREC *****


2019-02-12 20:42:38,055 : loading BERT mode bert-base-uncased
2019-02-12 20:42:38,056 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:42:38,074 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:42:38,075 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv__7ua56
2019-02-12 20:42:40,537 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:42:46,492 : Computed train embeddings
2019-02-12 20:42:46,824 : Computed test embeddings
2019-02-12 20:42:46,825 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 20:43:00,372 : [('reg:1e-05', 79.12), ('reg:0.0001', 79.07), ('reg:0.001', 78.3), ('reg:0.01', 70.08)]
2019-02-12 20:43:00,372 : Cross-validation : best param found is reg = 1e-05             with score 79.12
2019-02-12 20:43:00,372 : Evaluating...
2019-02-12 20:43:01,472 : 
Dev acc : 79.12 Test acc : 88.4             for TREC

2019-02-12 20:43:01,473 : ***** Transfer task : MRPC *****


2019-02-12 20:43:01,495 : loading BERT mode bert-base-uncased
2019-02-12 20:43:01,495 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:43:01,515 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:43:01,515 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqx9bifa4
2019-02-12 20:43:03,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:43:05,401 : Computing embedding for train
2019-02-12 20:43:18,961 : Computed train embeddings
2019-02-12 20:43:18,961 : Computing embedding for test
2019-02-12 20:43:25,010 : Computed test embeddings
2019-02-12 20:43:25,026 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 20:43:36,864 : [('reg:1e-05', 73.38), ('reg:0.0001', 73.43), ('reg:0.001', 73.33), ('reg:0.01', 73.11)]
2019-02-12 20:43:36,864 : Cross-validation : best param found is reg = 0.0001             with score 73.43
2019-02-12 20:43:36,864 : Evaluating...
2019-02-12 20:43:37,529 : Dev acc : 73.43 Test acc 71.54; Test F1 77.3 for MRPC.

2019-02-12 20:43:37,529 : ***** Transfer task : SICK-Entailment*****


2019-02-12 20:43:37,553 : loading BERT mode bert-base-uncased
2019-02-12 20:43:37,553 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:43:37,608 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:43:37,609 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp94spyieb
2019-02-12 20:43:40,036 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:43:41,510 : Computing embedding for train
2019-02-12 20:43:53,421 : Computed train embeddings
2019-02-12 20:43:53,421 : Computing embedding for dev
2019-02-12 20:43:54,905 : Computed dev embeddings
2019-02-12 20:43:54,906 : Computing embedding for test
2019-02-12 20:44:08,518 : Computed test embeddings
2019-02-12 20:44:08,546 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:44:12,416 : [('reg:1e-05', 81.2), ('reg:0.0001', 81.0), ('reg:0.001', 81.0), ('reg:0.01', 75.2)]
2019-02-12 20:44:12,417 : Validation : best param found is reg = 1e-05 with score             81.2
2019-02-12 20:44:12,417 : Evaluating...
2019-02-12 20:44:13,496 : 
Dev acc : 81.2 Test acc : 78.89 for                        SICK entailment

2019-02-12 20:44:13,496 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 20:44:13,523 : loading BERT mode bert-base-uncased
2019-02-12 20:44:13,523 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:44:13,558 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:44:13,559 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfn9nsr5p
2019-02-12 20:44:15,986 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:44:17,477 : Computing embedding for train
2019-02-12 20:44:30,519 : Computed train embeddings
2019-02-12 20:44:30,519 : Computing embedding for dev
2019-02-12 20:44:32,140 : Computed dev embeddings
2019-02-12 20:44:32,140 : Computing embedding for test
2019-02-12 20:44:46,563 : Computed test embeddings
2019-02-12 20:45:32,913 : Dev : Pearson 0.8214254889327501
2019-02-12 20:45:32,914 : Test : Pearson 0.8254965591046618 Spearman 0.7533282709130273 MSE 0.32435933615366774                        for SICK Relatedness

2019-02-12 20:45:32,914 : 

***** Transfer task : STSBenchmark*****


2019-02-12 20:45:32,983 : loading BERT mode bert-base-uncased
2019-02-12 20:45:32,984 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:45:33,003 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:45:33,003 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpidjprav5
2019-02-12 20:45:35,427 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:45:36,835 : Computing embedding for train
2019-02-12 20:45:48,327 : Computed train embeddings
2019-02-12 20:45:48,327 : Computing embedding for dev
2019-02-12 20:45:51,616 : Computed dev embeddings
2019-02-12 20:45:51,616 : Computing embedding for test
2019-02-12 20:45:54,378 : Computed test embeddings
2019-02-12 20:46:38,979 : Dev : Pearson 0.7541006643513408
2019-02-12 20:46:38,979 : Test : Pearson 0.6870015019307263 Spearman 0.6822661773333344 MSE 1.3792724835892651                        for SICK Relatedness

2019-02-12 20:46:38,979 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 20:46:39,229 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 20:46:39,239 : loading BERT mode bert-base-uncased
2019-02-12 20:46:39,239 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:46:39,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:46:39,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvb1yf001
2019-02-12 20:46:41,755 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:46:43,254 : Computing embeddings for train/dev/test
2019-02-12 20:49:39,630 : Computed embeddings
2019-02-12 20:49:39,630 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:50:30,339 : [('reg:1e-05', 94.5), ('reg:0.0001', 92.78), ('reg:0.001', 89.1), ('reg:0.01', 83.03)]
2019-02-12 20:50:30,339 : Validation : best param found is reg = 1e-05 with score             94.5
2019-02-12 20:50:30,339 : Evaluating...
2019-02-12 20:50:44,085 : 
Dev acc : 94.5 Test acc : 93.8 for LENGTH classification

2019-02-12 20:50:44,094 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 20:50:44,462 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 20:50:44,507 : loading BERT mode bert-base-uncased
2019-02-12 20:50:44,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:50:44,534 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:50:44,534 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjar6jb15
2019-02-12 20:50:46,979 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:50:48,494 : Computing embeddings for train/dev/test
2019-02-12 20:53:29,788 : Computed embeddings
2019-02-12 20:53:29,789 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:54:30,725 : [('reg:1e-05', 82.58), ('reg:0.0001', 45.23), ('reg:0.001', 2.87), ('reg:0.01', 0.86)]
2019-02-12 20:54:30,725 : Validation : best param found is reg = 1e-05 with score             82.58
2019-02-12 20:54:30,725 : Evaluating...
2019-02-12 20:54:47,397 : 
Dev acc : 82.6 Test acc : 83.2 for WORDCONTENT classification

2019-02-12 20:54:47,406 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 20:54:47,756 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 20:54:47,821 : loading BERT mode bert-base-uncased
2019-02-12 20:54:47,821 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:54:47,916 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:54:47,916 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn65t05s0
2019-02-12 20:54:50,343 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:54:51,815 : Computing embeddings for train/dev/test
2019-02-12 20:56:52,028 : Computed embeddings
2019-02-12 20:56:52,028 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 20:57:39,799 : [('reg:1e-05', 35.72), ('reg:0.0001', 35.26), ('reg:0.001', 33.49), ('reg:0.01', 27.99)]
2019-02-12 20:57:39,799 : Validation : best param found is reg = 1e-05 with score             35.72
2019-02-12 20:57:39,799 : Evaluating...
2019-02-12 20:57:54,248 : 
Dev acc : 35.7 Test acc : 36.4 for DEPTH classification

2019-02-12 20:57:54,257 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 20:57:54,627 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 20:57:54,688 : loading BERT mode bert-base-uncased
2019-02-12 20:57:54,688 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 20:57:54,796 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 20:57:54,796 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8112nkja
2019-02-12 20:57:57,226 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 20:57:58,639 : Computing embeddings for train/dev/test
2019-02-12 21:00:34,689 : Computed embeddings
2019-02-12 21:00:34,689 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:01:16,315 : [('reg:1e-05', 75.67), ('reg:0.0001', 68.54), ('reg:0.001', 61.76), ('reg:0.01', 47.36)]
2019-02-12 21:01:16,315 : Validation : best param found is reg = 1e-05 with score             75.67
2019-02-12 21:01:16,315 : Evaluating...
2019-02-12 21:01:32,013 : 
Dev acc : 75.7 Test acc : 76.2 for TOPCONSTITUENTS classification

2019-02-12 21:01:32,021 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 21:01:32,536 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 21:01:32,602 : loading BERT mode bert-base-uncased
2019-02-12 21:01:32,602 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:01:32,632 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:01:32,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo6lwa88q
2019-02-12 21:01:35,061 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:01:36,514 : Computing embeddings for train/dev/test
2019-02-12 21:04:30,303 : Computed embeddings
2019-02-12 21:04:30,303 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:05:37,790 : [('reg:1e-05', 83.06), ('reg:0.0001', 83.2), ('reg:0.001', 82.84), ('reg:0.01', 80.96)]
2019-02-12 21:05:37,790 : Validation : best param found is reg = 0.0001 with score             83.2
2019-02-12 21:05:37,790 : Evaluating...
2019-02-12 21:05:56,134 : 
Dev acc : 83.2 Test acc : 82.3 for BIGRAMSHIFT classification

2019-02-12 21:05:56,142 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 21:05:56,546 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 21:05:56,611 : loading BERT mode bert-base-uncased
2019-02-12 21:05:56,611 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:05:56,641 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:05:56,642 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq08_hzvs
2019-02-12 21:05:59,088 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:06:00,546 : Computing embeddings for train/dev/test
2019-02-12 21:08:40,786 : Computed embeddings
2019-02-12 21:08:40,786 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:09:29,102 : [('reg:1e-05', 88.76), ('reg:0.0001', 88.85), ('reg:0.001', 88.86), ('reg:0.01', 87.95)]
2019-02-12 21:09:29,102 : Validation : best param found is reg = 0.001 with score             88.86
2019-02-12 21:09:29,102 : Evaluating...
2019-02-12 21:09:44,466 : 
Dev acc : 88.9 Test acc : 87.5 for TENSE classification

2019-02-12 21:09:44,475 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 21:09:44,884 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 21:09:44,946 : loading BERT mode bert-base-uncased
2019-02-12 21:09:44,946 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:09:44,972 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:09:44,973 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp19or5nkc
2019-02-12 21:09:47,404 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:09:48,867 : Computing embeddings for train/dev/test
2019-02-12 21:12:59,933 : Computed embeddings
2019-02-12 21:12:59,933 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:13:48,456 : [('reg:1e-05', 83.22), ('reg:0.0001', 83.13), ('reg:0.001', 83.09), ('reg:0.01', 79.39)]
2019-02-12 21:13:48,456 : Validation : best param found is reg = 1e-05 with score             83.22
2019-02-12 21:13:48,456 : Evaluating...
2019-02-12 21:14:04,001 : 
Dev acc : 83.2 Test acc : 82.0 for SUBJNUMBER classification

2019-02-12 21:14:04,009 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 21:14:04,410 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 21:14:04,476 : loading BERT mode bert-base-uncased
2019-02-12 21:14:04,476 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:14:04,590 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:14:04,590 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdxmn573a
2019-02-12 21:14:06,999 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:14:08,469 : Computing embeddings for train/dev/test
2019-02-12 21:17:01,963 : Computed embeddings
2019-02-12 21:17:01,963 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:18:12,849 : [('reg:1e-05', 81.83), ('reg:0.0001', 81.9), ('reg:0.001', 82.14), ('reg:0.01', 81.05)]
2019-02-12 21:18:12,849 : Validation : best param found is reg = 0.001 with score             82.14
2019-02-12 21:18:12,849 : Evaluating...
2019-02-12 21:18:27,845 : 
Dev acc : 82.1 Test acc : 83.2 for OBJNUMBER classification

2019-02-12 21:18:27,853 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 21:18:28,228 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 21:18:28,297 : loading BERT mode bert-base-uncased
2019-02-12 21:18:28,297 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:18:28,419 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:18:28,419 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmwk61mpp
2019-02-12 21:18:30,833 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:18:32,304 : Computing embeddings for train/dev/test
2019-02-12 21:21:22,341 : Computed embeddings
2019-02-12 21:21:22,341 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:22:29,201 : [('reg:1e-05', 55.27), ('reg:0.0001', 55.36), ('reg:0.001', 55.43), ('reg:0.01', 54.08)]
2019-02-12 21:22:29,202 : Validation : best param found is reg = 0.001 with score             55.43
2019-02-12 21:22:29,202 : Evaluating...
2019-02-12 21:22:44,153 : 
Dev acc : 55.4 Test acc : 54.7 for ODDMANOUT classification

2019-02-12 21:22:44,161 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 21:22:44,746 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 21:22:44,821 : loading BERT mode bert-base-uncased
2019-02-12 21:22:44,822 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:22:44,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:22:44,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbx2vxez9
2019-02-12 21:22:47,286 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:22:48,740 : Computing embeddings for train/dev/test
2019-02-12 21:25:35,081 : Computed embeddings
2019-02-12 21:25:35,081 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:26:56,101 : [('reg:1e-05', 58.29), ('reg:0.0001', 58.25), ('reg:0.001', 56.99), ('reg:0.01', 53.33)]
2019-02-12 21:26:56,101 : Validation : best param found is reg = 1e-05 with score             58.29
2019-02-12 21:26:56,101 : Evaluating...
2019-02-12 21:27:21,949 : 
Dev acc : 58.3 Test acc : 57.5 for COORDINATIONINVERSION classification

2019-02-12 21:27:21,958 : {'STS12': {'MSRpar': {'pearson': (0.4122456455722078, 3.913682182910535e-32), 'spearman': SpearmanrResult(correlation=0.44948434961678857, pvalue=1.417714229712767e-38), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6124712561814787, 2.0158885430663785e-78), 'spearman': SpearmanrResult(correlation=0.6164935672784585, pvalue=1.0271103230884742e-79), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49642057409684104, 6.2422988180080164e-30), 'spearman': SpearmanrResult(correlation=0.5871412116024212, pvalue=7.120809986628736e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6903378428222363, 3.0158210394227256e-107), 'spearman': SpearmanrResult(correlation=0.6824265845644653, pvalue=6.606087513374995e-104), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5817961432689654, 1.6304216984223552e-37), 'spearman': SpearmanrResult(correlation=0.5174057064265033, pvalue=1.0599003884416308e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5586542923883459, 'wmean': 0.5618676843972682}, 'spearman': {'mean': 0.5705902838977274, 'wmean': 0.5750469977749261}}}, 'STS13': {'FNWN': {'pearson': (0.4316507453480641, 5.598682753760639e-10), 'spearman': SpearmanrResult(correlation=0.4437471917139449, pvalue=1.6023606043777242e-10), 'nsamples': 189}, 'headlines': {'pearson': (0.6611246440338685, 2.037999244436058e-95), 'spearman': SpearmanrResult(correlation=0.6387417278151295, pvalue=3.2320020256014613e-87), 'nsamples': 750}, 'OnWN': {'pearson': (0.5134248076237046, 4.708672346249424e-39), 'spearman': SpearmanrResult(correlation=0.5429412723938822, pvalue=2.4962400460884943e-44), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5354000656685457, 'wmean': 0.5769711939820559}, 'spearman': {'mean': 0.5418100639743189, 'wmean': 0.5783430459388337}}}, 'STS14': {'deft-forum': {'pearson': (0.3486173657342487, 2.6386129656458704e-14), 'spearman': SpearmanrResult(correlation=0.3706453066145085, pvalue=4.2173775416606534e-16), 'nsamples': 450}, 'deft-news': {'pearson': (0.7287466349706161, 6.244358588633391e-51), 'spearman': SpearmanrResult(correlation=0.6947878758913093, pvalue=1.4619414683567465e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.6191869945266408, 1.365775089146611e-80), 'spearman': SpearmanrResult(correlation=0.5769139131989853, pvalue=9.270785723823293e-68), 'nsamples': 750}, 'images': {'pearson': (0.6201959076933803, 6.381981787844762e-81), 'spearman': SpearmanrResult(correlation=0.6103094472889243, pvalue=9.809603321409968e-78), 'nsamples': 750}, 'OnWN': {'pearson': (0.6597832033785311, 6.615405102783912e-95), 'spearman': SpearmanrResult(correlation=0.7034703483671217, pvalue=4.8935180133109334e-113), 'nsamples': 750}, 'tweet-news': {'pearson': (0.660140886254614, 4.835784131323053e-95), 'spearman': SpearmanrResult(correlation=0.6312319533242494, pvalue=1.2894139388531029e-84), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6061118320930051, 'wmean': 0.6119952130563924}, 'spearman': {'mean': 0.5978931407808498, 'wmean': 0.6044455993009019}}}, 'STS15': {'answers-forums': {'pearson': (0.5329998576134684, 6.56785346123051e-29), 'spearman': SpearmanrResult(correlation=0.5290520583234161, pvalue=1.9659201019738108e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.7067028451159878, 1.6414362694803704e-114), 'spearman': SpearmanrResult(correlation=0.7099606083283496, pvalue=5.113969434638802e-116), 'nsamples': 750}, 'belief': {'pearson': (0.6371198478202044, 4.282945720000134e-44), 'spearman': SpearmanrResult(correlation=0.6524788349262951, pvalue=7.49947698664825e-47), 'nsamples': 375}, 'headlines': {'pearson': (0.6794238809261226, 1.1488103416518632e-102), 'spearman': SpearmanrResult(correlation=0.6691812858040742, pvalue=1.519101160478686e-98), 'nsamples': 750}, 'images': {'pearson': (0.7290030236103133, 2.9236125248392767e-125), 'spearman': SpearmanrResult(correlation=0.7400708065099588, pvalue=5.283412422472168e-131), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6570498910172193, 'wmean': 0.675047400592315}, 'spearman': {'mean': 0.6601487187784187, 'wmean': 0.6774945368168095}}}, 'STS16': {'answer-answer': {'pearson': (0.482130172613554, 3.4327851414151413e-16), 'spearman': SpearmanrResult(correlation=0.5125017445871457, pvalue=2.054595247973864e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.692121084301449, 7.749956558181332e-37), 'spearman': SpearmanrResult(correlation=0.6953879439364248, pvalue=2.621679372764277e-37), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7367920637883043, 1.255773939825235e-40), 'spearman': SpearmanrResult(correlation=0.7444714927716893, pvalue=7.029288584890972e-42), 'nsamples': 230}, 'postediting': {'pearson': (0.7986098287909918, 2.76851707150746e-55), 'spearman': SpearmanrResult(correlation=0.8268180111880888, pvalue=2.066868467502132e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.41685188052738614, 3.4218850489924195e-10), 'spearman': SpearmanrResult(correlation=0.4174292728933146, pvalue=3.2172162055461497e-10), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.625301006004337, 'wmean': 0.6292109863081276}, 'spearman': {'mean': 0.6393216930753326, 'wmean': 0.6437957818443422}}}, 'MR': {'devacc': 75.55, 'acc': 74.53, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.36, 'acc': 78.57, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.54, 'acc': 87.72, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.57, 'acc': 93.16, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.08, 'acc': 79.9, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.05, 'acc': 41.58, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.12, 'acc': 88.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.43, 'acc': 71.54, 'f1': 77.3, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 81.2, 'acc': 78.89, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8214254889327501, 'pearson': 0.8254965591046618, 'spearman': 0.7533282709130273, 'mse': 0.32435933615366774, 'yhat': array([3.61022094, 4.2984634 , 1.16710751, ..., 3.11644899, 4.49683737,
       4.31626656]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7541006643513408, 'pearson': 0.6870015019307263, 'spearman': 0.6822661773333344, 'mse': 1.3792724835892651, 'yhat': array([1.2243143 , 1.49555881, 1.69258163, ..., 3.8669485 , 3.9621369 ,
       3.56594004]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 94.5, 'acc': 93.77, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 82.58, 'acc': 83.23, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.72, 'acc': 36.38, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 75.67, 'acc': 76.19, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 83.2, 'acc': 82.26, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.86, 'acc': 87.48, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 83.22, 'acc': 81.97, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 82.14, 'acc': 83.16, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 55.43, 'acc': 54.74, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.29, 'acc': 57.51, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 21:27:21,958 : ********************************************************************************
2019-02-12 21:27:21,958 : ********************************************************************************
2019-02-12 21:27:21,958 : ********************************************************************************
2019-02-12 21:27:21,958 : layer 4
2019-02-12 21:27:21,958 : ********************************************************************************
2019-02-12 21:27:21,958 : ********************************************************************************
2019-02-12 21:27:21,958 : ********************************************************************************
2019-02-12 21:27:22,045 : ***** Transfer task : STS12 *****


2019-02-12 21:27:22,057 : loading BERT mode bert-base-uncased
2019-02-12 21:27:22,057 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:27:22,074 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:27:22,074 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbaadl_w2
2019-02-12 21:27:24,503 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:27:28,389 : MSRpar : pearson = 0.4053, spearman = 0.4401
2019-02-12 21:27:30,112 : MSRvid : pearson = 0.5781, spearman = 0.5837
2019-02-12 21:27:31,330 : SMTeuroparl : pearson = 0.5028, spearman = 0.5862
2019-02-12 21:27:33,406 : surprise.OnWN : pearson = 0.6740, spearman = 0.6744
2019-02-12 21:27:34,548 : surprise.SMTnews : pearson = 0.6073, spearman = 0.5229
2019-02-12 21:27:34,548 : ALL (weighted average) : Pearson = 0.5522,             Spearman = 0.5635
2019-02-12 21:27:34,548 : ALL (average) : Pearson = 0.5535,             Spearman = 0.5615

2019-02-12 21:27:34,548 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 21:27:34,556 : loading BERT mode bert-base-uncased
2019-02-12 21:27:34,556 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:27:34,573 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:27:34,574 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpymkbrgwv
2019-02-12 21:27:36,990 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:27:39,396 : FNWN : pearson = 0.3721, spearman = 0.3791
2019-02-12 21:27:41,270 : headlines : pearson = 0.6481, spearman = 0.6271
2019-02-12 21:27:42,670 : OnWN : pearson = 0.4799, spearman = 0.5121
2019-02-12 21:27:42,670 : ALL (weighted average) : Pearson = 0.5504,             Spearman = 0.5528
2019-02-12 21:27:42,670 : ALL (average) : Pearson = 0.5000,             Spearman = 0.5061

2019-02-12 21:27:42,670 : ***** Transfer task : STS14 *****


2019-02-12 21:27:42,692 : loading BERT mode bert-base-uncased
2019-02-12 21:27:42,692 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:27:42,710 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:27:42,710 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpce7e9kq6
2019-02-12 21:27:45,171 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:27:47,907 : deft-forum : pearson = 0.3297, spearman = 0.3628
2019-02-12 21:27:49,001 : deft-news : pearson = 0.7316, spearman = 0.7011
2019-02-12 21:27:51,006 : headlines : pearson = 0.6097, spearman = 0.5688
2019-02-12 21:27:53,013 : images : pearson = 0.5920, spearman = 0.5876
2019-02-12 21:27:55,032 : OnWN : pearson = 0.6403, spearman = 0.6846
2019-02-12 21:27:57,316 : tweet-news : pearson = 0.6551, spearman = 0.6215
2019-02-12 21:27:57,316 : ALL (weighted average) : Pearson = 0.5975,             Spearman = 0.5921
2019-02-12 21:27:57,316 : ALL (average) : Pearson = 0.5931,             Spearman = 0.5877

2019-02-12 21:27:57,316 : ***** Transfer task : STS15 *****


2019-02-12 21:27:57,347 : loading BERT mode bert-base-uncased
2019-02-12 21:27:57,347 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:27:57,365 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:27:57,365 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmvqb5ue3
2019-02-12 21:27:59,781 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:28:02,678 : answers-forums : pearson = 0.5174, spearman = 0.5263
2019-02-12 21:28:04,735 : answers-students : pearson = 0.6866, spearman = 0.6903
2019-02-12 21:28:06,131 : belief : pearson = 0.6272, spearman = 0.6531
2019-02-12 21:28:08,262 : headlines : pearson = 0.6671, spearman = 0.6585
2019-02-12 21:28:10,403 : images : pearson = 0.7074, spearman = 0.7206
2019-02-12 21:28:10,403 : ALL (weighted average) : Pearson = 0.6584,             Spearman = 0.6648
2019-02-12 21:28:10,403 : ALL (average) : Pearson = 0.6412,             Spearman = 0.6498

2019-02-12 21:28:10,404 : ***** Transfer task : STS16 *****


2019-02-12 21:28:10,475 : loading BERT mode bert-base-uncased
2019-02-12 21:28:10,475 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:28:10,493 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:28:10,493 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk_iavq7y
2019-02-12 21:28:12,917 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:28:15,082 : answer-answer : pearson = 0.4763, spearman = 0.5058
2019-02-12 21:28:15,795 : headlines : pearson = 0.6773, spearman = 0.6804
2019-02-12 21:28:16,594 : plagiarism : pearson = 0.7214, spearman = 0.7256
2019-02-12 21:28:17,595 : postediting : pearson = 0.7849, spearman = 0.8222
2019-02-12 21:28:18,274 : question-question : pearson = 0.3540, spearman = 0.3487
2019-02-12 21:28:18,274 : ALL (weighted average) : Pearson = 0.6080,             Spearman = 0.6225
2019-02-12 21:28:18,274 : ALL (average) : Pearson = 0.6028,             Spearman = 0.6165

2019-02-12 21:28:18,274 : ***** Transfer task : MR *****


2019-02-12 21:28:18,291 : loading BERT mode bert-base-uncased
2019-02-12 21:28:18,291 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:28:18,311 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:28:18,311 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvldvfwxd
2019-02-12 21:28:20,732 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:28:22,238 : Generating sentence embeddings
2019-02-12 21:28:44,010 : Generated sentence embeddings
2019-02-12 21:28:44,011 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:29:07,169 : Best param found at split 1: l2reg = 0.001                 with score 76.17
2019-02-12 21:29:26,197 : Best param found at split 2: l2reg = 0.001                 with score 76.11
2019-02-12 21:29:51,175 : Best param found at split 3: l2reg = 0.001                 with score 76.72
2019-02-12 21:30:18,661 : Best param found at split 4: l2reg = 1e-05                 with score 76.05
2019-02-12 21:30:52,163 : Best param found at split 5: l2reg = 0.001                 with score 76.38
2019-02-12 21:30:55,204 : Dev acc : 76.29 Test acc : 75.63

2019-02-12 21:30:55,205 : ***** Transfer task : CR *****


2019-02-12 21:30:55,213 : loading BERT mode bert-base-uncased
2019-02-12 21:30:55,213 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:30:55,232 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:30:55,232 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd3qpl1bg
2019-02-12 21:30:57,633 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:30:59,163 : Generating sentence embeddings
2019-02-12 21:31:06,330 : Generated sentence embeddings
2019-02-12 21:31:06,331 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:31:19,759 : Best param found at split 1: l2reg = 1e-05                 with score 80.33
2019-02-12 21:31:29,494 : Best param found at split 2: l2reg = 0.001                 with score 80.92
2019-02-12 21:31:39,267 : Best param found at split 3: l2reg = 1e-05                 with score 80.86
2019-02-12 21:31:49,297 : Best param found at split 4: l2reg = 0.001                 with score 81.03
2019-02-12 21:31:59,116 : Best param found at split 5: l2reg = 0.0001                 with score 80.54
2019-02-12 21:31:59,588 : Dev acc : 80.74 Test acc : 80.19

2019-02-12 21:31:59,588 : ***** Transfer task : MPQA *****


2019-02-12 21:31:59,593 : loading BERT mode bert-base-uncased
2019-02-12 21:31:59,593 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:31:59,612 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:31:59,613 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5kbkw_3t
2019-02-12 21:32:02,085 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:32:03,572 : Generating sentence embeddings
2019-02-12 21:32:14,544 : Generated sentence embeddings
2019-02-12 21:32:14,545 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:32:41,541 : Best param found at split 1: l2reg = 0.001                 with score 87.87
2019-02-12 21:33:01,886 : Best param found at split 2: l2reg = 0.001                 with score 87.8
2019-02-12 21:33:27,943 : Best param found at split 3: l2reg = 0.001                 with score 88.39
2019-02-12 21:33:55,578 : Best param found at split 4: l2reg = 0.001                 with score 88.17
2019-02-12 21:34:33,581 : Best param found at split 5: l2reg = 0.01                 with score 87.93
2019-02-12 21:34:36,660 : Dev acc : 88.03 Test acc : 87.36

2019-02-12 21:34:36,661 : ***** Transfer task : SUBJ *****


2019-02-12 21:34:36,675 : loading BERT mode bert-base-uncased
2019-02-12 21:34:36,675 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:34:36,697 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:34:36,697 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_vtboqup
2019-02-12 21:34:39,143 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:34:40,712 : Generating sentence embeddings
2019-02-12 21:35:05,559 : Generated sentence embeddings
2019-02-12 21:35:05,559 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 21:35:37,286 : Best param found at split 1: l2reg = 0.001                 with score 94.16
2019-02-12 21:36:06,625 : Best param found at split 2: l2reg = 0.001                 with score 94.4
2019-02-12 21:36:33,202 : Best param found at split 3: l2reg = 0.0001                 with score 94.07
2019-02-12 21:36:55,045 : Best param found at split 4: l2reg = 0.001                 with score 94.4
2019-02-12 21:37:19,586 : Best param found at split 5: l2reg = 0.001                 with score 93.98
2019-02-12 21:37:20,983 : Dev acc : 94.2 Test acc : 93.85

2019-02-12 21:37:20,984 : ***** Transfer task : SST Binary classification *****


2019-02-12 21:37:21,110 : loading BERT mode bert-base-uncased
2019-02-12 21:37:21,110 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:37:21,131 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:37:21,132 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp80d1bvka
2019-02-12 21:37:23,552 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:37:25,022 : Computing embedding for train
2019-02-12 21:39:02,711 : Computed train embeddings
2019-02-12 21:39:02,711 : Computing embedding for dev
2019-02-12 21:39:04,332 : Computed dev embeddings
2019-02-12 21:39:04,332 : Computing embedding for test
2019-02-12 21:39:07,785 : Computed test embeddings
2019-02-12 21:39:07,785 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:39:58,478 : [('reg:1e-05', 81.65), ('reg:0.0001', 81.42), ('reg:0.001', 81.65), ('reg:0.01', 79.82)]
2019-02-12 21:39:58,478 : Validation : best param found is reg = 1e-05 with score             81.65
2019-02-12 21:39:58,478 : Evaluating...
2019-02-12 21:40:10,886 : 
Dev acc : 81.65 Test acc : 80.18 for             SST Binary classification

2019-02-12 21:40:10,891 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 21:40:10,940 : loading BERT mode bert-base-uncased
2019-02-12 21:40:10,941 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:40:10,961 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:40:10,961 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvb3mccgc
2019-02-12 21:40:13,413 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:40:14,880 : Computing embedding for train
2019-02-12 21:40:31,001 : Computed train embeddings
2019-02-12 21:40:31,002 : Computing embedding for dev
2019-02-12 21:40:32,896 : Computed dev embeddings
2019-02-12 21:40:32,896 : Computing embedding for test
2019-02-12 21:40:36,819 : Computed test embeddings
2019-02-12 21:40:36,819 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:40:46,046 : [('reg:1e-05', 42.14), ('reg:0.0001', 42.14), ('reg:0.001', 42.05), ('reg:0.01', 41.96)]
2019-02-12 21:40:46,046 : Validation : best param found is reg = 1e-05 with score             42.14
2019-02-12 21:40:46,046 : Evaluating...
2019-02-12 21:40:47,616 : 
Dev acc : 42.14 Test acc : 42.58 for             SST Fine-Grained classification

2019-02-12 21:40:47,616 : ***** Transfer task : TREC *****


2019-02-12 21:40:47,629 : loading BERT mode bert-base-uncased
2019-02-12 21:40:47,629 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:40:47,647 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:40:47,648 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8jy02_64
2019-02-12 21:40:50,078 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:40:58,738 : Computed train embeddings
2019-02-12 21:40:59,451 : Computed test embeddings
2019-02-12 21:40:59,451 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 21:41:20,799 : [('reg:1e-05', 82.06), ('reg:0.0001', 81.86), ('reg:0.001', 80.46), ('reg:0.01', 72.67)]
2019-02-12 21:41:20,799 : Cross-validation : best param found is reg = 1e-05             with score 82.06
2019-02-12 21:41:20,799 : Evaluating...
2019-02-12 21:41:21,301 : 
Dev acc : 82.06 Test acc : 90.8             for TREC

2019-02-12 21:41:21,302 : ***** Transfer task : MRPC *****


2019-02-12 21:41:21,324 : loading BERT mode bert-base-uncased
2019-02-12 21:41:21,324 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:41:21,344 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:41:21,344 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpckzb_6p1
2019-02-12 21:41:23,786 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:41:25,309 : Computing embedding for train
2019-02-12 21:41:38,963 : Computed train embeddings
2019-02-12 21:41:38,963 : Computing embedding for test
2019-02-12 21:41:47,092 : Computed test embeddings
2019-02-12 21:41:47,109 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 21:41:57,773 : [('reg:1e-05', 73.6), ('reg:0.0001', 73.26), ('reg:0.001', 73.45), ('reg:0.01', 73.6)]
2019-02-12 21:41:57,773 : Cross-validation : best param found is reg = 1e-05             with score 73.6
2019-02-12 21:41:57,773 : Evaluating...
2019-02-12 21:41:58,264 : Dev acc : 73.6 Test acc 74.09; Test F1 80.4 for MRPC.

2019-02-12 21:41:58,264 : ***** Transfer task : SICK-Entailment*****


2019-02-12 21:41:58,327 : loading BERT mode bert-base-uncased
2019-02-12 21:41:58,327 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:41:58,346 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:41:58,346 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfhy57rfw
2019-02-12 21:42:00,781 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:42:02,200 : Computing embedding for train
2019-02-12 21:42:09,869 : Computed train embeddings
2019-02-12 21:42:09,869 : Computing embedding for dev
2019-02-12 21:42:10,830 : Computed dev embeddings
2019-02-12 21:42:10,830 : Computing embedding for test
2019-02-12 21:42:19,071 : Computed test embeddings
2019-02-12 21:42:19,098 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:42:21,234 : [('reg:1e-05', 78.8), ('reg:0.0001', 79.0), ('reg:0.001', 78.6), ('reg:0.01', 71.0)]
2019-02-12 21:42:21,234 : Validation : best param found is reg = 0.0001 with score             79.0
2019-02-12 21:42:21,234 : Evaluating...
2019-02-12 21:42:21,802 : 
Dev acc : 79.0 Test acc : 79.32 for                        SICK entailment

2019-02-12 21:42:21,802 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 21:42:21,829 : loading BERT mode bert-base-uncased
2019-02-12 21:42:21,829 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:42:21,885 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:42:21,885 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptijr8fu4
2019-02-12 21:42:24,325 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:42:25,742 : Computing embedding for train
2019-02-12 21:42:33,337 : Computed train embeddings
2019-02-12 21:42:33,337 : Computing embedding for dev
2019-02-12 21:42:34,297 : Computed dev embeddings
2019-02-12 21:42:34,297 : Computing embedding for test
2019-02-12 21:42:42,524 : Computed test embeddings
2019-02-12 21:43:22,425 : Dev : Pearson 0.8081018837451983
2019-02-12 21:43:22,425 : Test : Pearson 0.8210946306785972 Spearman 0.7497532476361182 MSE 0.33389116918705375                        for SICK Relatedness

2019-02-12 21:43:22,426 : 

***** Transfer task : STSBenchmark*****


2019-02-12 21:43:22,496 : loading BERT mode bert-base-uncased
2019-02-12 21:43:22,497 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:43:22,515 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:43:22,515 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp70z4o1p9
2019-02-12 21:43:24,943 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:43:26,477 : Computing embedding for train
2019-02-12 21:43:42,845 : Computed train embeddings
2019-02-12 21:43:42,845 : Computing embedding for dev
2019-02-12 21:43:47,531 : Computed dev embeddings
2019-02-12 21:43:47,531 : Computing embedding for test
2019-02-12 21:43:51,712 : Computed test embeddings
2019-02-12 21:44:57,314 : Dev : Pearson 0.7464539187484868
2019-02-12 21:44:57,314 : Test : Pearson 0.6798474619844024 Spearman 0.6748435995029207 MSE 1.4105237914184845                        for SICK Relatedness

2019-02-12 21:44:57,314 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 21:44:57,629 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 21:44:57,639 : loading BERT mode bert-base-uncased
2019-02-12 21:44:57,639 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:44:57,662 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:44:57,663 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9pp3524n
2019-02-12 21:45:00,090 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:45:01,569 : Computing embeddings for train/dev/test
2019-02-12 21:47:18,125 : Computed embeddings
2019-02-12 21:47:18,125 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:48:39,671 : [('reg:1e-05', 93.54), ('reg:0.0001', 92.74), ('reg:0.001', 87.19), ('reg:0.01', 81.66)]
2019-02-12 21:48:39,671 : Validation : best param found is reg = 1e-05 with score             93.54
2019-02-12 21:48:39,671 : Evaluating...
2019-02-12 21:48:57,609 : 
Dev acc : 93.5 Test acc : 94.3 for LENGTH classification

2019-02-12 21:48:57,618 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 21:48:57,958 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 21:48:58,002 : loading BERT mode bert-base-uncased
2019-02-12 21:48:58,002 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:48:58,031 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:48:58,031 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphb3bezol
2019-02-12 21:49:00,450 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:49:01,920 : Computing embeddings for train/dev/test
2019-02-12 21:51:10,902 : Computed embeddings
2019-02-12 21:51:10,902 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:52:32,819 : [('reg:1e-05', 75.5), ('reg:0.0001', 37.35), ('reg:0.001', 2.5), ('reg:0.01', 0.83)]
2019-02-12 21:52:32,819 : Validation : best param found is reg = 1e-05 with score             75.5
2019-02-12 21:52:32,820 : Evaluating...
2019-02-12 21:52:45,966 : 
Dev acc : 75.5 Test acc : 75.7 for WORDCONTENT classification

2019-02-12 21:52:45,975 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 21:52:46,328 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 21:52:46,393 : loading BERT mode bert-base-uncased
2019-02-12 21:52:46,393 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:52:46,418 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:52:46,418 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuf7dwg4d
2019-02-12 21:52:48,862 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:52:50,341 : Computing embeddings for train/dev/test
2019-02-12 21:55:08,085 : Computed embeddings
2019-02-12 21:55:08,085 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:55:53,014 : [('reg:1e-05', 37.56), ('reg:0.0001', 37.13), ('reg:0.001', 34.79), ('reg:0.01', 29.14)]
2019-02-12 21:55:53,014 : Validation : best param found is reg = 1e-05 with score             37.56
2019-02-12 21:55:53,014 : Evaluating...
2019-02-12 21:56:02,813 : 
Dev acc : 37.6 Test acc : 37.3 for DEPTH classification

2019-02-12 21:56:02,823 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 21:56:03,197 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 21:56:03,258 : loading BERT mode bert-base-uncased
2019-02-12 21:56:03,258 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:56:03,390 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:56:03,390 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxr6uwayp
2019-02-12 21:56:05,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:56:07,306 : Computing embeddings for train/dev/test
2019-02-12 21:58:17,024 : Computed embeddings
2019-02-12 21:58:17,024 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 21:59:19,420 : [('reg:1e-05', 75.1), ('reg:0.0001', 73.22), ('reg:0.001', 66.73), ('reg:0.01', 49.65)]
2019-02-12 21:59:19,420 : Validation : best param found is reg = 1e-05 with score             75.1
2019-02-12 21:59:19,420 : Evaluating...
2019-02-12 21:59:33,598 : 
Dev acc : 75.1 Test acc : 75.7 for TOPCONSTITUENTS classification

2019-02-12 21:59:33,607 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 21:59:33,940 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 21:59:34,005 : loading BERT mode bert-base-uncased
2019-02-12 21:59:34,005 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 21:59:34,123 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 21:59:34,123 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprd50g0xt
2019-02-12 21:59:36,546 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 21:59:38,011 : Computing embeddings for train/dev/test
2019-02-12 22:01:49,948 : Computed embeddings
2019-02-12 22:01:49,948 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:03:32,969 : [('reg:1e-05', 86.15), ('reg:0.0001', 86.17), ('reg:0.001', 85.91), ('reg:0.01', 84.67)]
2019-02-12 22:03:32,970 : Validation : best param found is reg = 0.0001 with score             86.17
2019-02-12 22:03:32,970 : Evaluating...
2019-02-12 22:03:55,684 : 
Dev acc : 86.2 Test acc : 85.2 for BIGRAMSHIFT classification

2019-02-12 22:03:55,693 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 22:03:56,259 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 22:03:56,325 : loading BERT mode bert-base-uncased
2019-02-12 22:03:56,326 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:03:56,356 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:03:56,356 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy49j0bcu
2019-02-12 22:03:58,783 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:04:00,286 : Computing embeddings for train/dev/test
2019-02-12 22:06:28,526 : Computed embeddings
2019-02-12 22:06:28,527 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:07:18,856 : [('reg:1e-05', 88.77), ('reg:0.0001', 88.77), ('reg:0.001', 88.94), ('reg:0.01', 89.18)]
2019-02-12 22:07:18,856 : Validation : best param found is reg = 0.01 with score             89.18
2019-02-12 22:07:18,856 : Evaluating...
2019-02-12 22:07:34,366 : 
Dev acc : 89.2 Test acc : 87.9 for TENSE classification

2019-02-12 22:07:34,376 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 22:07:34,978 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 22:07:35,042 : loading BERT mode bert-base-uncased
2019-02-12 22:07:35,043 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:07:35,067 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:07:35,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplfw6sw2u
2019-02-12 22:07:37,499 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:07:39,071 : Computing embeddings for train/dev/test
2019-02-12 22:10:16,682 : Computed embeddings
2019-02-12 22:10:16,682 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:11:36,128 : [('reg:1e-05', 84.37), ('reg:0.0001', 84.22), ('reg:0.001', 84.18), ('reg:0.01', 81.42)]
2019-02-12 22:11:36,128 : Validation : best param found is reg = 1e-05 with score             84.37
2019-02-12 22:11:36,128 : Evaluating...
2019-02-12 22:11:55,372 : 
Dev acc : 84.4 Test acc : 82.5 for SUBJNUMBER classification

2019-02-12 22:11:55,379 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 22:11:55,801 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 22:11:55,867 : loading BERT mode bert-base-uncased
2019-02-12 22:11:55,867 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:11:55,893 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:11:55,893 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq1sc9yes
2019-02-12 22:11:58,279 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:11:59,849 : Computing embeddings for train/dev/test
2019-02-12 22:14:25,085 : Computed embeddings
2019-02-12 22:14:25,085 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:15:33,412 : [('reg:1e-05', 83.09), ('reg:0.0001', 83.13), ('reg:0.001', 83.02), ('reg:0.01', 82.37)]
2019-02-12 22:15:33,413 : Validation : best param found is reg = 0.0001 with score             83.13
2019-02-12 22:15:33,413 : Evaluating...
2019-02-12 22:15:53,771 : 
Dev acc : 83.1 Test acc : 83.6 for OBJNUMBER classification

2019-02-12 22:15:53,779 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 22:15:54,161 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 22:15:54,229 : loading BERT mode bert-base-uncased
2019-02-12 22:15:54,229 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:15:54,349 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:15:54,349 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1j62obd9
2019-02-12 22:15:56,743 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:15:58,303 : Computing embeddings for train/dev/test
2019-02-12 22:18:54,497 : Computed embeddings
2019-02-12 22:18:54,497 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:19:55,713 : [('reg:1e-05', 58.44), ('reg:0.0001', 58.35), ('reg:0.001', 57.95), ('reg:0.01', 57.04)]
2019-02-12 22:19:55,714 : Validation : best param found is reg = 1e-05 with score             58.44
2019-02-12 22:19:55,714 : Evaluating...
2019-02-12 22:20:18,106 : 
Dev acc : 58.4 Test acc : 59.6 for ODDMANOUT classification

2019-02-12 22:20:18,114 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 22:20:18,713 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 22:20:18,788 : loading BERT mode bert-base-uncased
2019-02-12 22:20:18,788 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:20:18,818 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:20:18,818 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_u1sylcs
2019-02-12 22:20:21,232 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:20:22,801 : Computing embeddings for train/dev/test
2019-02-12 22:23:24,241 : Computed embeddings
2019-02-12 22:23:24,241 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:24:24,119 : [('reg:1e-05', 58.07), ('reg:0.0001', 58.04), ('reg:0.001', 57.87), ('reg:0.01', 54.91)]
2019-02-12 22:24:24,120 : Validation : best param found is reg = 1e-05 with score             58.07
2019-02-12 22:24:24,120 : Evaluating...
2019-02-12 22:24:42,927 : 
Dev acc : 58.1 Test acc : 58.2 for COORDINATIONINVERSION classification

2019-02-12 22:24:42,936 : {'STS12': {'MSRpar': {'pearson': (0.4052528229448153, 5.184459789409608e-31), 'spearman': SpearmanrResult(correlation=0.4400593881107694, pvalue=7.2164011687994715e-37), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5781198381657037, 4.234063823707794e-68), 'spearman': SpearmanrResult(correlation=0.5836619905624902, pvalue=1.1070533954923591e-69), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5028136618251258, 8.811336708865506e-31), 'spearman': SpearmanrResult(correlation=0.5862438845437978, pvalue=1.029235710476498e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6739990337448989, 1.8357000764257304e-100), 'spearman': SpearmanrResult(correlation=0.6743744579876841, pvalue=1.29665873698275e-100), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6072765076987535, 1.4199422666845886e-41), 'spearman': SpearmanrResult(correlation=0.5229234273429784, pvalue=2.1994797382015973e-29), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5534923728758594, 'wmean': 0.5521633103253214}, 'spearman': {'mean': 0.5614526297095439, 'wmean': 0.5634827117153023}}}, 'STS13': {'FNWN': {'pearson': (0.372052938326867, 1.3536997629773848e-07), 'spearman': SpearmanrResult(correlation=0.37907053180836076, pvalue=7.494541225597373e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6480739546601892, 1.4946342365735147e-90), 'spearman': SpearmanrResult(correlation=0.6270650187950015, pvalue=3.333233309079082e-83), 'nsamples': 750}, 'OnWN': {'pearson': (0.4799013404642696, 1.1758670471369962e-33), 'spearman': SpearmanrResult(correlation=0.5121057629576942, pvalue=7.884516133825847e-39), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.500009411150442, 'wmean': 0.5503987488929166}, 'spearman': {'mean': 0.5060804378536855, 'wmean': 0.5528229517515318}}}, 'STS14': {'deft-forum': {'pearson': (0.3297163897194047, 7.149623797714282e-13), 'spearman': SpearmanrResult(correlation=0.36278776982239624, pvalue=1.913451362224233e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.7316315992034294, 1.620960515455753e-51), 'spearman': SpearmanrResult(correlation=0.7010947043537464, pvalue=1.1226882586341086e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.6096815694875363, 1.549690787976233e-77), 'spearman': SpearmanrResult(correlation=0.5687851153263359, pvalue=1.67836905301294e-65), 'nsamples': 750}, 'images': {'pearson': (0.591974697686206, 4.09800000733299e-72), 'spearman': SpearmanrResult(correlation=0.587642212053968, pvalue=7.738874210426582e-71), 'nsamples': 750}, 'OnWN': {'pearson': (0.6403114394717663, 9.049464380856232e-88), 'spearman': SpearmanrResult(correlation=0.6846258207900822, pvalue=7.979986687577142e-105), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6550704230979825, 3.947614578733825e-93), 'spearman': SpearmanrResult(correlation=0.6214669372400929, pvalue=2.4376274745663605e-81), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5930643531110542, 'wmean': 0.5975041206513011}, 'spearman': {'mean': 0.5877337599311037, 'wmean': 0.5921261258090831}}}, 'STS15': {'answers-forums': {'pearson': (0.5173888238590508, 4.61682608729839e-27), 'spearman': SpearmanrResult(correlation=0.5262692414717844, pvalue=4.22147237319601e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.6866462683166453, 1.1258635488865303e-105), 'spearman': SpearmanrResult(correlation=0.6903386065150742, pvalue=3.013546484299612e-107), 'nsamples': 750}, 'belief': {'pearson': (0.6272192956018731, 2.1250343371543077e-42), 'spearman': SpearmanrResult(correlation=0.6531186102084776, pvalue=5.711032913433934e-47), 'nsamples': 375}, 'headlines': {'pearson': (0.6671394152527296, 9.627940695541496e-98), 'spearman': SpearmanrResult(correlation=0.6585473295372856, pvalue=1.946881284978932e-94), 'nsamples': 750}, 'images': {'pearson': (0.7074201564527084, 7.679068079133998e-115), 'spearman': SpearmanrResult(correlation=0.7205976358327468, pvalue=4.3725201690880265e-121), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6411627918966014, 'wmean': 0.6583774749381364}, 'spearman': {'mean': 0.6497742847130737, 'wmean': 0.6647943744313094}}}, 'STS16': {'answer-answer': {'pearson': (0.4762949438543628, 8.670321108450283e-16), 'spearman': SpearmanrResult(correlation=0.5057588483084345, pvalue=6.6912905917088936e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6772714026658299, 8.95968090179346e-35), 'spearman': SpearmanrResult(correlation=0.6804121707855832, pvalue=3.358356003305362e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7213897471078284, 3.0374011440175776e-38), 'spearman': SpearmanrResult(correlation=0.7256435564145253, pvalue=6.925401333608475e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.7848939179183156, 3.2340557437920288e-52), 'spearman': SpearmanrResult(correlation=0.8221923351859188, pvalue=3.712524094429376e-61), 'nsamples': 244}, 'question-question': {'pearson': (0.35401062850828596, 1.4526489564571263e-07), 'spearman': SpearmanrResult(correlation=0.34867486159786426, pvalue=2.2939403507917073e-07), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6027721280109246, 'wmean': 0.607960770799242}, 'spearman': {'mean': 0.6165363544584652, 'wmean': 0.62248850913205}}}, 'MR': {'devacc': 76.29, 'acc': 75.63, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.74, 'acc': 80.19, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.03, 'acc': 87.36, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.2, 'acc': 93.85, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.65, 'acc': 80.18, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.14, 'acc': 42.58, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 82.06, 'acc': 90.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.6, 'acc': 74.09, 'f1': 80.4, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.0, 'acc': 79.32, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8081018837451983, 'pearson': 0.8210946306785972, 'spearman': 0.7497532476361182, 'mse': 0.33389116918705375, 'yhat': array([3.79825653, 4.24169418, 1.33630719, ..., 3.09608245, 4.51618008,
       4.46230123]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7464539187484868, 'pearson': 0.6798474619844024, 'spearman': 0.6748435995029207, 'mse': 1.4105237914184845, 'yhat': array([1.45598282, 1.54532538, 1.98314118, ..., 3.90942835, 3.81228405,
       3.45631741]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 93.54, 'acc': 94.31, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 75.5, 'acc': 75.74, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 37.56, 'acc': 37.32, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 75.1, 'acc': 75.7, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.17, 'acc': 85.16, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.18, 'acc': 87.92, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.37, 'acc': 82.48, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.13, 'acc': 83.57, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.44, 'acc': 59.63, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.07, 'acc': 58.21, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 22:24:42,936 : ********************************************************************************
2019-02-12 22:24:42,936 : ********************************************************************************
2019-02-12 22:24:42,936 : ********************************************************************************
2019-02-12 22:24:42,936 : layer 5
2019-02-12 22:24:42,936 : ********************************************************************************
2019-02-12 22:24:42,936 : ********************************************************************************
2019-02-12 22:24:42,936 : ********************************************************************************
2019-02-12 22:24:43,023 : ***** Transfer task : STS12 *****


2019-02-12 22:24:43,036 : loading BERT mode bert-base-uncased
2019-02-12 22:24:43,036 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:24:43,053 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:24:43,053 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpng_gtbln
2019-02-12 22:24:45,440 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:24:50,260 : MSRpar : pearson = 0.4167, spearman = 0.4487
2019-02-12 22:24:52,798 : MSRvid : pearson = 0.5432, spearman = 0.5497
2019-02-12 22:24:54,586 : SMTeuroparl : pearson = 0.4812, spearman = 0.5806
2019-02-12 22:24:57,577 : surprise.OnWN : pearson = 0.6512, spearman = 0.6550
2019-02-12 22:24:59,174 : surprise.SMTnews : pearson = 0.6241, spearman = 0.5168
2019-02-12 22:24:59,174 : ALL (weighted average) : Pearson = 0.5400,             Spearman = 0.5511
2019-02-12 22:24:59,174 : ALL (average) : Pearson = 0.5433,             Spearman = 0.5502

2019-02-12 22:24:59,174 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 22:24:59,182 : loading BERT mode bert-base-uncased
2019-02-12 22:24:59,182 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:24:59,199 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:24:59,199 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt7r6ldj5
2019-02-12 22:25:01,608 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:25:04,312 : FNWN : pearson = 0.3742, spearman = 0.3874
2019-02-12 22:25:07,076 : headlines : pearson = 0.6364, spearman = 0.6133
2019-02-12 22:25:09,217 : OnWN : pearson = 0.4712, spearman = 0.5008
2019-02-12 22:25:09,217 : ALL (weighted average) : Pearson = 0.5416,             Spearman = 0.5428
2019-02-12 22:25:09,217 : ALL (average) : Pearson = 0.4939,             Spearman = 0.5005

2019-02-12 22:25:09,218 : ***** Transfer task : STS14 *****


2019-02-12 22:25:09,237 : loading BERT mode bert-base-uncased
2019-02-12 22:25:09,238 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:25:09,284 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:25:09,284 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1w25eqg7
2019-02-12 22:25:11,674 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:25:15,004 : deft-forum : pearson = 0.3202, spearman = 0.3497
2019-02-12 22:25:16,510 : deft-news : pearson = 0.7425, spearman = 0.7111
2019-02-12 22:25:19,607 : headlines : pearson = 0.5986, spearman = 0.5515
2019-02-12 22:25:22,736 : images : pearson = 0.5862, spearman = 0.5811
2019-02-12 22:25:25,891 : OnWN : pearson = 0.6462, spearman = 0.6825
2019-02-12 22:25:29,348 : tweet-news : pearson = 0.6661, spearman = 0.6246
2019-02-12 22:25:29,348 : ALL (weighted average) : Pearson = 0.5973,             Spearman = 0.5868
2019-02-12 22:25:29,348 : ALL (average) : Pearson = 0.5933,             Spearman = 0.5834

2019-02-12 22:25:29,348 : ***** Transfer task : STS15 *****


2019-02-12 22:25:29,379 : loading BERT mode bert-base-uncased
2019-02-12 22:25:29,379 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:25:29,396 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:25:29,397 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmky8vvpg
2019-02-12 22:25:31,793 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:25:35,258 : answers-forums : pearson = 0.5297, spearman = 0.5239
2019-02-12 22:25:38,611 : answers-students : pearson = 0.6772, spearman = 0.6842
2019-02-12 22:25:40,686 : belief : pearson = 0.6507, spearman = 0.6778
2019-02-12 22:25:43,267 : headlines : pearson = 0.6557, spearman = 0.6453
2019-02-12 22:25:46,014 : images : pearson = 0.7158, spearman = 0.7263
2019-02-12 22:25:46,014 : ALL (weighted average) : Pearson = 0.6597,             Spearman = 0.6642
2019-02-12 22:25:46,014 : ALL (average) : Pearson = 0.6458,             Spearman = 0.6515

2019-02-12 22:25:46,014 : ***** Transfer task : STS16 *****


2019-02-12 22:25:46,082 : loading BERT mode bert-base-uncased
2019-02-12 22:25:46,082 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:25:46,100 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:25:46,100 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9vtslzr5
2019-02-12 22:25:48,492 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:25:50,973 : answer-answer : pearson = 0.5104, spearman = 0.5336
2019-02-12 22:25:51,933 : headlines : pearson = 0.6597, spearman = 0.6599
2019-02-12 22:25:53,032 : plagiarism : pearson = 0.7512, spearman = 0.7620
2019-02-12 22:25:54,313 : postediting : pearson = 0.7949, spearman = 0.8308
2019-02-12 22:25:55,066 : question-question : pearson = 0.3393, spearman = 0.3394
2019-02-12 22:25:55,066 : ALL (weighted average) : Pearson = 0.6168,             Spearman = 0.6313
2019-02-12 22:25:55,066 : ALL (average) : Pearson = 0.6111,             Spearman = 0.6252

2019-02-12 22:25:55,066 : ***** Transfer task : MR *****


2019-02-12 22:25:55,083 : loading BERT mode bert-base-uncased
2019-02-12 22:25:55,083 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:25:55,103 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:25:55,103 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw5f77ne2
2019-02-12 22:25:57,491 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:25:59,058 : Generating sentence embeddings
2019-02-12 22:26:21,974 : Generated sentence embeddings
2019-02-12 22:26:21,975 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:26:38,954 : Best param found at split 1: l2reg = 0.01                 with score 77.12
2019-02-12 22:26:55,156 : Best param found at split 2: l2reg = 0.001                 with score 76.73
2019-02-12 22:27:09,306 : Best param found at split 3: l2reg = 0.001                 with score 77.27
2019-02-12 22:27:34,917 : Best param found at split 4: l2reg = 0.001                 with score 76.87
2019-02-12 22:28:15,060 : Best param found at split 5: l2reg = 0.0001                 with score 77.23
2019-02-12 22:28:17,027 : Dev acc : 77.04 Test acc : 76.63

2019-02-12 22:28:17,028 : ***** Transfer task : CR *****


2019-02-12 22:28:17,035 : loading BERT mode bert-base-uncased
2019-02-12 22:28:17,036 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:28:17,054 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:28:17,055 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa3ciql2q
2019-02-12 22:28:19,451 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:28:20,993 : Generating sentence embeddings
2019-02-12 22:28:28,659 : Generated sentence embeddings
2019-02-12 22:28:28,659 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:28:40,014 : Best param found at split 1: l2reg = 0.0001                 with score 80.29
2019-02-12 22:28:55,374 : Best param found at split 2: l2reg = 0.001                 with score 81.05
2019-02-12 22:29:11,094 : Best param found at split 3: l2reg = 1e-05                 with score 81.49
2019-02-12 22:29:25,909 : Best param found at split 4: l2reg = 0.001                 with score 80.67
2019-02-12 22:29:40,602 : Best param found at split 5: l2reg = 1e-05                 with score 80.6
2019-02-12 22:29:41,380 : Dev acc : 80.82 Test acc : 80.45

2019-02-12 22:29:41,381 : ***** Transfer task : MPQA *****


2019-02-12 22:29:41,386 : loading BERT mode bert-base-uncased
2019-02-12 22:29:41,386 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:29:41,405 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:29:41,405 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphpstx3me
2019-02-12 22:29:43,822 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:29:45,318 : Generating sentence embeddings
2019-02-12 22:29:55,107 : Generated sentence embeddings
2019-02-12 22:29:55,107 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:30:11,600 : Best param found at split 1: l2reg = 0.001                 with score 87.52
2019-02-12 22:30:25,014 : Best param found at split 2: l2reg = 0.001                 with score 87.81
2019-02-12 22:30:41,509 : Best param found at split 3: l2reg = 0.01                 with score 87.25
2019-02-12 22:30:57,582 : Best param found at split 4: l2reg = 0.0001                 with score 87.86
2019-02-12 22:31:22,895 : Best param found at split 5: l2reg = 0.001                 with score 87.51
2019-02-12 22:31:25,148 : Dev acc : 87.59 Test acc : 88.03

2019-02-12 22:31:25,149 : ***** Transfer task : SUBJ *****


2019-02-12 22:31:25,163 : loading BERT mode bert-base-uncased
2019-02-12 22:31:25,163 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:31:25,184 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:31:25,184 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgjq4w54r
2019-02-12 22:31:27,587 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:31:29,102 : Generating sentence embeddings
2019-02-12 22:31:51,264 : Generated sentence embeddings
2019-02-12 22:31:51,264 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 22:32:27,937 : Best param found at split 1: l2reg = 1e-05                 with score 94.59
2019-02-12 22:33:12,298 : Best param found at split 2: l2reg = 1e-05                 with score 94.9
2019-02-12 22:33:52,299 : Best param found at split 3: l2reg = 0.001                 with score 94.39
2019-02-12 22:34:10,867 : Best param found at split 4: l2reg = 0.001                 with score 94.91
2019-02-12 22:34:26,527 : Best param found at split 5: l2reg = 0.001                 with score 94.45
2019-02-12 22:34:27,407 : Dev acc : 94.65 Test acc : 94.29

2019-02-12 22:34:27,408 : ***** Transfer task : SST Binary classification *****


2019-02-12 22:34:27,537 : loading BERT mode bert-base-uncased
2019-02-12 22:34:27,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:34:27,560 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:34:27,561 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu0x879nc
2019-02-12 22:34:29,994 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:34:31,432 : Computing embedding for train
2019-02-12 22:35:46,506 : Computed train embeddings
2019-02-12 22:35:46,506 : Computing embedding for dev
2019-02-12 22:35:48,147 : Computed dev embeddings
2019-02-12 22:35:48,148 : Computing embedding for test
2019-02-12 22:35:51,693 : Computed test embeddings
2019-02-12 22:35:51,693 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:37:03,510 : [('reg:1e-05', 80.62), ('reg:0.0001', 80.73), ('reg:0.001', 81.31), ('reg:0.01', 80.73)]
2019-02-12 22:37:03,510 : Validation : best param found is reg = 0.001 with score             81.31
2019-02-12 22:37:03,510 : Evaluating...
2019-02-12 22:37:22,426 : 
Dev acc : 81.31 Test acc : 82.04 for             SST Binary classification

2019-02-12 22:37:22,426 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 22:37:22,475 : loading BERT mode bert-base-uncased
2019-02-12 22:37:22,476 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:37:22,496 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:37:22,496 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp3o2v2jh
2019-02-12 22:37:24,887 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:37:26,418 : Computing embedding for train
2019-02-12 22:37:46,633 : Computed train embeddings
2019-02-12 22:37:46,634 : Computing embedding for dev
2019-02-12 22:37:49,185 : Computed dev embeddings
2019-02-12 22:37:49,185 : Computing embedding for test
2019-02-12 22:37:54,657 : Computed test embeddings
2019-02-12 22:37:54,657 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:38:02,292 : [('reg:1e-05', 41.05), ('reg:0.0001', 40.96), ('reg:0.001', 41.05), ('reg:0.01', 41.33)]
2019-02-12 22:38:02,292 : Validation : best param found is reg = 0.01 with score             41.33
2019-02-12 22:38:02,292 : Evaluating...
2019-02-12 22:38:03,317 : 
Dev acc : 41.33 Test acc : 42.35 for             SST Fine-Grained classification

2019-02-12 22:38:03,318 : ***** Transfer task : TREC *****


2019-02-12 22:38:03,331 : loading BERT mode bert-base-uncased
2019-02-12 22:38:03,331 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:38:03,349 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:38:03,349 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi9_vofn9
2019-02-12 22:38:05,802 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:38:14,329 : Computed train embeddings
2019-02-12 22:38:14,891 : Computed test embeddings
2019-02-12 22:38:14,891 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 22:38:27,499 : [('reg:1e-05', 83.05), ('reg:0.0001', 83.09), ('reg:0.001', 81.64), ('reg:0.01', 75.74)]
2019-02-12 22:38:27,499 : Cross-validation : best param found is reg = 0.0001             with score 83.09
2019-02-12 22:38:27,499 : Evaluating...
2019-02-12 22:38:28,182 : 
Dev acc : 83.09 Test acc : 91.0             for TREC

2019-02-12 22:38:28,183 : ***** Transfer task : MRPC *****


2019-02-12 22:38:28,205 : loading BERT mode bert-base-uncased
2019-02-12 22:38:28,205 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:38:28,225 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:38:28,225 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp14c32zr9
2019-02-12 22:38:30,662 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:38:32,124 : Computing embedding for train
2019-02-12 22:38:44,168 : Computed train embeddings
2019-02-12 22:38:44,168 : Computing embedding for test
2019-02-12 22:38:49,372 : Computed test embeddings
2019-02-12 22:38:49,388 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 22:38:57,605 : [('reg:1e-05', 73.87), ('reg:0.0001', 73.92), ('reg:0.001', 73.82), ('reg:0.01', 73.53)]
2019-02-12 22:38:57,605 : Cross-validation : best param found is reg = 0.0001             with score 73.92
2019-02-12 22:38:57,605 : Evaluating...
2019-02-12 22:38:58,118 : Dev acc : 73.92 Test acc 74.14; Test F1 80.34 for MRPC.

2019-02-12 22:38:58,118 : ***** Transfer task : SICK-Entailment*****


2019-02-12 22:38:58,181 : loading BERT mode bert-base-uncased
2019-02-12 22:38:58,181 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:38:58,199 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:38:58,200 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpehwbltj2
2019-02-12 22:39:00,629 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:39:02,028 : Computing embedding for train
2019-02-12 22:39:09,465 : Computed train embeddings
2019-02-12 22:39:09,466 : Computing embedding for dev
2019-02-12 22:39:11,015 : Computed dev embeddings
2019-02-12 22:39:11,015 : Computing embedding for test
2019-02-12 22:39:20,267 : Computed test embeddings
2019-02-12 22:39:20,294 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:39:23,666 : [('reg:1e-05', 80.2), ('reg:0.0001', 80.0), ('reg:0.001', 79.4), ('reg:0.01', 74.0)]
2019-02-12 22:39:23,666 : Validation : best param found is reg = 1e-05 with score             80.2
2019-02-12 22:39:23,667 : Evaluating...
2019-02-12 22:39:24,152 : 
Dev acc : 80.2 Test acc : 78.71 for                        SICK entailment

2019-02-12 22:39:24,152 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 22:39:24,182 : loading BERT mode bert-base-uncased
2019-02-12 22:39:24,182 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:39:24,242 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:39:24,243 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbxxfkzup
2019-02-12 22:39:26,702 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:39:28,183 : Computing embedding for train
2019-02-12 22:39:38,131 : Computed train embeddings
2019-02-12 22:39:38,132 : Computing embedding for dev
2019-02-12 22:39:38,958 : Computed dev embeddings
2019-02-12 22:39:38,959 : Computing embedding for test
2019-02-12 22:39:49,466 : Computed test embeddings
2019-02-12 22:40:41,778 : Dev : Pearson 0.8062596673158293
2019-02-12 22:40:41,778 : Test : Pearson 0.8068017446765752 Spearman 0.7402394011926606 MSE 0.3554067758133547                        for SICK Relatedness

2019-02-12 22:40:41,779 : 

***** Transfer task : STSBenchmark*****


2019-02-12 22:40:41,817 : loading BERT mode bert-base-uncased
2019-02-12 22:40:41,817 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:40:41,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:40:41,846 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf1mmx3rs
2019-02-12 22:40:44,268 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:40:45,785 : Computing embedding for train
2019-02-12 22:41:01,405 : Computed train embeddings
2019-02-12 22:41:01,405 : Computing embedding for dev
2019-02-12 22:41:05,866 : Computed dev embeddings
2019-02-12 22:41:05,866 : Computing embedding for test
2019-02-12 22:41:09,852 : Computed test embeddings
2019-02-12 22:42:10,048 : Dev : Pearson 0.7480499822719735
2019-02-12 22:42:10,049 : Test : Pearson 0.6836250222714078 Spearman 0.6770241178091033 MSE 1.4143526060633713                        for SICK Relatedness

2019-02-12 22:42:10,049 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 22:42:10,362 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 22:42:10,372 : loading BERT mode bert-base-uncased
2019-02-12 22:42:10,372 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:42:10,395 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:42:10,395 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz_cpnp1s
2019-02-12 22:42:12,824 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:42:14,237 : Computing embeddings for train/dev/test
2019-02-12 22:44:23,804 : Computed embeddings
2019-02-12 22:44:23,804 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:45:19,606 : [('reg:1e-05', 92.42), ('reg:0.0001', 91.12), ('reg:0.001', 86.58), ('reg:0.01', 80.69)]
2019-02-12 22:45:19,606 : Validation : best param found is reg = 1e-05 with score             92.42
2019-02-12 22:45:19,606 : Evaluating...
2019-02-12 22:45:34,987 : 
Dev acc : 92.4 Test acc : 92.3 for LENGTH classification

2019-02-12 22:45:34,995 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 22:45:35,338 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 22:45:35,383 : loading BERT mode bert-base-uncased
2019-02-12 22:45:35,383 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:45:35,411 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:45:35,411 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0iah3qos
2019-02-12 22:45:37,823 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:45:39,283 : Computing embeddings for train/dev/test
2019-02-12 22:48:16,717 : Computed embeddings
2019-02-12 22:48:16,717 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:49:18,121 : [('reg:1e-05', 71.9), ('reg:0.0001', 35.5), ('reg:0.001', 2.9), ('reg:0.01', 0.94)]
2019-02-12 22:49:18,121 : Validation : best param found is reg = 1e-05 with score             71.9
2019-02-12 22:49:18,121 : Evaluating...
2019-02-12 22:49:44,147 : 
Dev acc : 71.9 Test acc : 72.0 for WORDCONTENT classification

2019-02-12 22:49:44,148 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 22:49:44,487 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 22:49:44,551 : loading BERT mode bert-base-uncased
2019-02-12 22:49:44,552 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:49:44,645 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:49:44,645 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplwn0tmlj
2019-02-12 22:49:47,046 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:49:48,574 : Computing embeddings for train/dev/test
2019-02-12 22:52:38,115 : Computed embeddings
2019-02-12 22:52:38,115 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:53:30,343 : [('reg:1e-05', 38.19), ('reg:0.0001', 37.88), ('reg:0.001', 35.81), ('reg:0.01', 29.9)]
2019-02-12 22:53:30,343 : Validation : best param found is reg = 1e-05 with score             38.19
2019-02-12 22:53:30,343 : Evaluating...
2019-02-12 22:53:49,642 : 
Dev acc : 38.2 Test acc : 38.3 for DEPTH classification

2019-02-12 22:53:49,650 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 22:53:50,204 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 22:53:50,267 : loading BERT mode bert-base-uncased
2019-02-12 22:53:50,267 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:53:50,293 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:53:50,294 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpixs0zxvt
2019-02-12 22:53:52,690 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:53:54,226 : Computing embeddings for train/dev/test
2019-02-12 22:55:56,635 : Computed embeddings
2019-02-12 22:55:56,636 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 22:56:29,074 : [('reg:1e-05', 76.12), ('reg:0.0001', 75.28), ('reg:0.001', 68.31), ('reg:0.01', 54.57)]
2019-02-12 22:56:29,074 : Validation : best param found is reg = 1e-05 with score             76.12
2019-02-12 22:56:29,075 : Evaluating...
2019-02-12 22:56:42,357 : 
Dev acc : 76.1 Test acc : 76.0 for TOPCONSTITUENTS classification

2019-02-12 22:56:42,367 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 22:56:42,704 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 22:56:42,769 : loading BERT mode bert-base-uncased
2019-02-12 22:56:42,769 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 22:56:42,888 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 22:56:42,888 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp292qty0r
2019-02-12 22:56:45,326 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 22:56:46,772 : Computing embeddings for train/dev/test
2019-02-12 22:59:29,825 : Computed embeddings
2019-02-12 22:59:29,825 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:00:37,018 : [('reg:1e-05', 87.7), ('reg:0.0001', 87.73), ('reg:0.001', 87.43), ('reg:0.01', 86.23)]
2019-02-12 23:00:37,018 : Validation : best param found is reg = 0.0001 with score             87.73
2019-02-12 23:00:37,018 : Evaluating...
2019-02-12 23:00:59,983 : 
Dev acc : 87.7 Test acc : 87.0 for BIGRAMSHIFT classification

2019-02-12 23:00:59,991 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 23:01:00,548 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 23:01:00,613 : loading BERT mode bert-base-uncased
2019-02-12 23:01:00,613 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:01:00,642 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:01:00,643 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjszwtlnp
2019-02-12 23:01:03,069 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:01:04,559 : Computing embeddings for train/dev/test
2019-02-12 23:03:36,433 : Computed embeddings
2019-02-12 23:03:36,434 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:04:46,118 : [('reg:1e-05', 89.86), ('reg:0.0001', 89.97), ('reg:0.001', 90.11), ('reg:0.01', 89.77)]
2019-02-12 23:04:46,118 : Validation : best param found is reg = 0.001 with score             90.11
2019-02-12 23:04:46,118 : Evaluating...
2019-02-12 23:05:03,058 : 
Dev acc : 90.1 Test acc : 89.1 for TENSE classification

2019-02-12 23:05:03,066 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-12 23:05:03,628 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-12 23:05:03,690 : loading BERT mode bert-base-uncased
2019-02-12 23:05:03,691 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:05:03,719 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:05:03,719 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt4pnaqvg
2019-02-12 23:05:06,142 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:05:07,601 : Computing embeddings for train/dev/test
2019-02-12 23:07:55,422 : Computed embeddings
2019-02-12 23:07:55,422 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:08:44,823 : [('reg:1e-05', 85.54), ('reg:0.0001', 85.83), ('reg:0.001', 84.74), ('reg:0.01', 81.84)]
2019-02-12 23:08:44,823 : Validation : best param found is reg = 0.0001 with score             85.83
2019-02-12 23:08:44,824 : Evaluating...
2019-02-12 23:08:59,446 : 
Dev acc : 85.8 Test acc : 84.4 for SUBJNUMBER classification

2019-02-12 23:08:59,454 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-12 23:08:59,888 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-12 23:08:59,956 : loading BERT mode bert-base-uncased
2019-02-12 23:08:59,956 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:08:59,984 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:08:59,984 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfarvrm4f
2019-02-12 23:09:02,416 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:09:03,903 : Computing embeddings for train/dev/test
2019-02-12 23:11:38,717 : Computed embeddings
2019-02-12 23:11:38,717 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:12:58,810 : [('reg:1e-05', 82.78), ('reg:0.0001', 82.9), ('reg:0.001', 82.71), ('reg:0.01', 82.32)]
2019-02-12 23:12:58,810 : Validation : best param found is reg = 0.0001 with score             82.9
2019-02-12 23:12:58,810 : Evaluating...
2019-02-12 23:13:27,991 : 
Dev acc : 82.9 Test acc : 83.3 for OBJNUMBER classification

2019-02-12 23:13:27,999 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-12 23:13:28,383 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-12 23:13:28,451 : loading BERT mode bert-base-uncased
2019-02-12 23:13:28,451 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:13:28,571 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:13:28,571 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkp9je9pr
2019-02-12 23:13:30,967 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:13:32,514 : Computing embeddings for train/dev/test
2019-02-12 23:16:23,063 : Computed embeddings
2019-02-12 23:16:23,063 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:17:40,416 : [('reg:1e-05', 60.56), ('reg:0.0001', 60.43), ('reg:0.001', 60.29), ('reg:0.01', 59.44)]
2019-02-12 23:17:40,416 : Validation : best param found is reg = 1e-05 with score             60.56
2019-02-12 23:17:40,416 : Evaluating...
2019-02-12 23:18:08,117 : 
Dev acc : 60.6 Test acc : 60.0 for ODDMANOUT classification

2019-02-12 23:18:08,124 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-12 23:18:08,510 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-12 23:18:08,586 : loading BERT mode bert-base-uncased
2019-02-12 23:18:08,586 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:18:08,708 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:18:08,708 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpejah2lum
2019-02-12 23:18:11,088 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:18:12,651 : Computing embeddings for train/dev/test
2019-02-12 23:21:05,315 : Computed embeddings
2019-02-12 23:21:05,315 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:22:40,010 : [('reg:1e-05', 60.34), ('reg:0.0001', 60.28), ('reg:0.001', 60.5), ('reg:0.01', 57.62)]
2019-02-12 23:22:40,010 : Validation : best param found is reg = 0.001 with score             60.5
2019-02-12 23:22:40,010 : Evaluating...
2019-02-12 23:23:04,273 : 
Dev acc : 60.5 Test acc : 60.7 for COORDINATIONINVERSION classification

2019-02-12 23:23:04,282 : {'STS12': {'MSRpar': {'pearson': (0.41669596205122517, 7.319884047588117e-33), 'spearman': SpearmanrResult(correlation=0.4487255545506555, pvalue=1.954273269227737e-38), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5432490003562456, 8.38830050498777e-59), 'spearman': SpearmanrResult(correlation=0.5497239670717528, pvalue=1.9036278513612374e-60), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4811778186479036, 5.647202926571223e-28), 'spearman': SpearmanrResult(correlation=0.5806051613512803, pvalue=1.0151783328283563e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6512017928775169, 1.0724313235545436e-91), 'spearman': SpearmanrResult(correlation=0.6550084584482017, pvalue=4.163615908006536e-93), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6240798566228135, 1.8508973122707708e-44), 'spearman': SpearmanrResult(correlation=0.5167617123125526, pvalue=1.2710530444438913e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5432808861111409, 'wmean': 0.5399705752946046}, 'spearman': {'mean': 0.5501649707468885, 'wmean': 0.5510872513918609}}}, 'STS13': {'FNWN': {'pearson': (0.3742114451539917, 1.130280266334392e-07), 'spearman': SpearmanrResult(correlation=0.3873953297993763, pvalue=3.6491946642880904e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6363809779242725, 2.1621238459811184e-86), 'spearman': SpearmanrResult(correlation=0.6133400885143385, pvalue=1.0636160778850496e-78), 'nsamples': 750}, 'OnWN': {'pearson': (0.47118471177356785, 2.3912712679216086e-32), 'spearman': SpearmanrResult(correlation=0.5007545959131352, pvalue=6.073961325689327e-37), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.49392571161727733, 'wmean': 0.5415642132548536}, 'spearman': {'mean': 0.50049667140895, 'wmean': 0.5427640746834033}}}, 'STS14': {'deft-forum': {'pearson': (0.3201617974094088, 3.4829266590887763e-12), 'spearman': SpearmanrResult(correlation=0.3497009138857929, pvalue=2.1689233637305787e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7425218103435448, 8.481446298931032e-54), 'spearman': SpearmanrResult(correlation=0.7110929059828098, pvalue=1.6667671471929202e-47), 'nsamples': 300}, 'headlines': {'pearson': (0.598646387833195, 4.066385401711838e-74), 'spearman': SpearmanrResult(correlation=0.5515319688829481, pvalue=6.517011488727611e-61), 'nsamples': 750}, 'images': {'pearson': (0.5862322844564154, 1.9944444146905862e-70), 'spearman': SpearmanrResult(correlation=0.581066443681681, pvalue=6.153476930165846e-69), 'nsamples': 750}, 'OnWN': {'pearson': (0.6462210667084184, 7.014314039669795e-90), 'spearman': SpearmanrResult(correlation=0.6825060996062785, pvalue=6.122045693755671e-104), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6660622555870068, 2.535274515572247e-97), 'spearman': SpearmanrResult(correlation=0.6245560023250033, pvalue=2.3069428105775976e-82), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5933076003896648, 'wmean': 0.5972535594336197}, 'spearman': {'mean': 0.583409055727419, 'wmean': 0.5867836450441022}}}, 'STS15': {'answers-forums': {'pearson': (0.5296536027763514, 1.6650109688828526e-28), 'spearman': SpearmanrResult(correlation=0.5238985205823194, pvalue=8.050136745234755e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.6772401016819416, 8.974260202816976e-102), 'spearman': SpearmanrResult(correlation=0.6842424544070798, pvalue=1.1550504041076335e-104), 'nsamples': 750}, 'belief': {'pearson': (0.650704606673009, 1.590932941116758e-46), 'spearman': SpearmanrResult(correlation=0.6777514236488688, pvalue=9.39063035425924e-52), 'nsamples': 375}, 'headlines': {'pearson': (0.6556784218639292, 2.339022111745915e-93), 'spearman': SpearmanrResult(correlation=0.6453079503227963, pvalue=1.496802639406859e-89), 'nsamples': 750}, 'images': {'pearson': (0.715839691722635, 8.639446377074131e-119), 'spearman': SpearmanrResult(correlation=0.7263371494331214, pvalue=6.410558970814167e-124), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6458232849435732, 'wmean': 0.6597343299982965}, 'spearman': {'mean': 0.6515074996788371, 'wmean': 0.6641781315696479}}}, 'STS16': {'answer-answer': {'pearson': (0.5103682319808112, 2.9936754612944755e-18), 'spearman': SpearmanrResult(correlation=0.5336369445038536, pvalue=4.262392012110318e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6596956782923812, 1.7487203658749342e-32), 'spearman': SpearmanrResult(correlation=0.6598682798206951, pvalue=1.6633197204473855e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7512440803852835, 5.0694599893400114e-43), 'spearman': SpearmanrResult(correlation=0.7620399583183082, pvalue=6.403563153269467e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.7948514960789547, 2.0229651452346283e-54), 'spearman': SpearmanrResult(correlation=0.8307692316239217, pvalue=1.636713318306325e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.33927848568295105, 5.025637693354026e-07), 'spearman': SpearmanrResult(correlation=0.3394442799746902, pvalue=4.957670318161775e-07), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6110875944840763, 'wmean': 0.6168101702002915}, 'spearman': {'mean': 0.6251517388482938, 'wmean': 0.6313420936117116}}}, 'MR': {'devacc': 77.04, 'acc': 76.63, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.82, 'acc': 80.45, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.59, 'acc': 88.03, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.65, 'acc': 94.29, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.31, 'acc': 82.04, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.33, 'acc': 42.35, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 83.09, 'acc': 91.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.92, 'acc': 74.14, 'f1': 80.34, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.2, 'acc': 78.71, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8062596673158293, 'pearson': 0.8068017446765752, 'spearman': 0.7402394011926606, 'mse': 0.3554067758133547, 'yhat': array([3.94677961, 4.24509852, 1.21235522, ..., 2.97456187, 4.67383004,
       4.44367676]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7480499822719735, 'pearson': 0.6836250222714078, 'spearman': 0.6770241178091033, 'mse': 1.4143526060633713, 'yhat': array([1.79079639, 1.93888161, 2.15114161, ..., 3.96303776, 3.87706636,
       3.70890691]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 92.42, 'acc': 92.32, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 71.9, 'acc': 72.0, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 38.19, 'acc': 38.26, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 76.12, 'acc': 76.04, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.73, 'acc': 86.99, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.11, 'acc': 89.07, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.83, 'acc': 84.38, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 82.9, 'acc': 83.35, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.56, 'acc': 60.03, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 60.5, 'acc': 60.71, 'ndev': 10002, 'ntest': 10002}}
2019-02-12 23:23:04,282 : ********************************************************************************
2019-02-12 23:23:04,282 : ********************************************************************************
2019-02-12 23:23:04,282 : ********************************************************************************
2019-02-12 23:23:04,282 : layer 6
2019-02-12 23:23:04,282 : ********************************************************************************
2019-02-12 23:23:04,282 : ********************************************************************************
2019-02-12 23:23:04,282 : ********************************************************************************
2019-02-12 23:23:04,371 : ***** Transfer task : STS12 *****


2019-02-12 23:23:04,408 : loading BERT mode bert-base-uncased
2019-02-12 23:23:04,408 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:23:04,425 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:23:04,425 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyo09o8t5
2019-02-12 23:23:06,851 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:23:11,593 : MSRpar : pearson = 0.3999, spearman = 0.4281
2019-02-12 23:23:14,085 : MSRvid : pearson = 0.4839, spearman = 0.4969
2019-02-12 23:23:15,873 : SMTeuroparl : pearson = 0.4647, spearman = 0.5811
2019-02-12 23:23:18,899 : surprise.OnWN : pearson = 0.6455, spearman = 0.6475
2019-02-12 23:23:20,539 : surprise.SMTnews : pearson = 0.6555, spearman = 0.5368
2019-02-12 23:23:20,539 : ALL (weighted average) : Pearson = 0.5218,             Spearman = 0.5342
2019-02-12 23:23:20,539 : ALL (average) : Pearson = 0.5299,             Spearman = 0.5381

2019-02-12 23:23:20,539 : ***** Transfer task : STS13 (-SMT) *****


2019-02-12 23:23:20,548 : loading BERT mode bert-base-uncased
2019-02-12 23:23:20,548 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:23:20,565 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:23:20,566 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa44m8pzr
2019-02-12 23:23:22,991 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:23:25,456 : FNWN : pearson = 0.3267, spearman = 0.3481
2019-02-12 23:23:27,566 : headlines : pearson = 0.6275, spearman = 0.6051
2019-02-12 23:23:29,231 : OnWN : pearson = 0.4352, spearman = 0.4644
2019-02-12 23:23:29,231 : ALL (weighted average) : Pearson = 0.5177,             Spearman = 0.5201
2019-02-12 23:23:29,231 : ALL (average) : Pearson = 0.4632,             Spearman = 0.4725

2019-02-12 23:23:29,231 : ***** Transfer task : STS14 *****


2019-02-12 23:23:29,249 : loading BERT mode bert-base-uncased
2019-02-12 23:23:29,249 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:23:29,297 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:23:29,297 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8bxrkcwe
2019-02-12 23:23:31,735 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:23:34,683 : deft-forum : pearson = 0.3019, spearman = 0.3307
2019-02-12 23:23:36,002 : deft-news : pearson = 0.7436, spearman = 0.7125
2019-02-12 23:23:38,300 : headlines : pearson = 0.5852, spearman = 0.5402
2019-02-12 23:23:40,669 : images : pearson = 0.5560, spearman = 0.5507
2019-02-12 23:23:43,078 : OnWN : pearson = 0.6241, spearman = 0.6608
2019-02-12 23:23:45,813 : tweet-news : pearson = 0.6649, spearman = 0.6198
2019-02-12 23:23:45,813 : ALL (weighted average) : Pearson = 0.5818,             Spearman = 0.5710
2019-02-12 23:23:45,813 : ALL (average) : Pearson = 0.5793,             Spearman = 0.5691

2019-02-12 23:23:45,813 : ***** Transfer task : STS15 *****


2019-02-12 23:23:45,845 : loading BERT mode bert-base-uncased
2019-02-12 23:23:45,845 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:23:45,863 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:23:45,863 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0en5t6v9
2019-02-12 23:23:48,321 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:23:51,361 : answers-forums : pearson = 0.5410, spearman = 0.5321
2019-02-12 23:23:53,163 : answers-students : pearson = 0.6891, spearman = 0.6949
2019-02-12 23:23:54,425 : belief : pearson = 0.6459, spearman = 0.6760
2019-02-12 23:23:56,308 : headlines : pearson = 0.6419, spearman = 0.6321
2019-02-12 23:23:58,208 : images : pearson = 0.6817, spearman = 0.6897
2019-02-12 23:23:58,208 : ALL (weighted average) : Pearson = 0.6515,             Spearman = 0.6552
2019-02-12 23:23:58,208 : ALL (average) : Pearson = 0.6399,             Spearman = 0.6450

2019-02-12 23:23:58,208 : ***** Transfer task : STS16 *****


2019-02-12 23:23:58,281 : loading BERT mode bert-base-uncased
2019-02-12 23:23:58,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:23:58,299 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:23:58,299 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppjkq26ds
2019-02-12 23:24:00,730 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:24:03,011 : answer-answer : pearson = 0.5459, spearman = 0.5658
2019-02-12 23:24:03,857 : headlines : pearson = 0.6578, spearman = 0.6567
2019-02-12 23:24:04,385 : plagiarism : pearson = 0.7634, spearman = 0.7733
2019-02-12 23:24:05,266 : postediting : pearson = 0.7980, spearman = 0.8327
2019-02-12 23:24:05,740 : question-question : pearson = 0.2986, spearman = 0.2923
2019-02-12 23:24:05,740 : ALL (weighted average) : Pearson = 0.6199,             Spearman = 0.6318
2019-02-12 23:24:05,740 : ALL (average) : Pearson = 0.6128,             Spearman = 0.6242

2019-02-12 23:24:05,740 : ***** Transfer task : MR *****


2019-02-12 23:24:05,756 : loading BERT mode bert-base-uncased
2019-02-12 23:24:05,756 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:24:05,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:24:05,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwf18nogc
2019-02-12 23:24:08,210 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:24:09,742 : Generating sentence embeddings
2019-02-12 23:24:28,400 : Generated sentence embeddings
2019-02-12 23:24:28,401 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:24:43,221 : Best param found at split 1: l2reg = 0.0001                 with score 77.67
2019-02-12 23:24:55,853 : Best param found at split 2: l2reg = 0.0001                 with score 77.08
2019-02-12 23:25:16,991 : Best param found at split 3: l2reg = 0.001                 with score 77.98
2019-02-12 23:25:39,609 : Best param found at split 4: l2reg = 0.0001                 with score 77.81
2019-02-12 23:26:18,325 : Best param found at split 5: l2reg = 1e-05                 with score 78.0
2019-02-12 23:26:21,052 : Dev acc : 77.71 Test acc : 77.68

2019-02-12 23:26:21,053 : ***** Transfer task : CR *****


2019-02-12 23:26:21,061 : loading BERT mode bert-base-uncased
2019-02-12 23:26:21,061 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:26:21,080 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:26:21,080 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaqid1sjf
2019-02-12 23:26:23,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:26:25,086 : Generating sentence embeddings
2019-02-12 23:26:33,196 : Generated sentence embeddings
2019-02-12 23:26:33,196 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:26:46,190 : Best param found at split 1: l2reg = 0.001                 with score 81.68
2019-02-12 23:26:57,746 : Best param found at split 2: l2reg = 1e-05                 with score 81.88
2019-02-12 23:27:09,486 : Best param found at split 3: l2reg = 1e-05                 with score 82.42
2019-02-12 23:27:20,167 : Best param found at split 4: l2reg = 0.001                 with score 82.03
2019-02-12 23:27:31,125 : Best param found at split 5: l2reg = 1e-05                 with score 82.26
2019-02-12 23:27:31,596 : Dev acc : 82.05 Test acc : 81.24

2019-02-12 23:27:31,597 : ***** Transfer task : MPQA *****


2019-02-12 23:27:31,602 : loading BERT mode bert-base-uncased
2019-02-12 23:27:31,602 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:27:31,620 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:27:31,620 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp930bsp6p
2019-02-12 23:27:34,086 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:27:35,654 : Generating sentence embeddings
2019-02-12 23:27:45,753 : Generated sentence embeddings
2019-02-12 23:27:45,753 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:28:07,250 : Best param found at split 1: l2reg = 0.01                 with score 87.69
2019-02-12 23:28:24,935 : Best param found at split 2: l2reg = 0.001                 with score 88.0
2019-02-12 23:28:47,099 : Best param found at split 3: l2reg = 0.001                 with score 87.81
2019-02-12 23:29:12,515 : Best param found at split 4: l2reg = 0.001                 with score 88.33
2019-02-12 23:29:41,852 : Best param found at split 5: l2reg = 1e-05                 with score 87.36
2019-02-12 23:29:44,469 : Dev acc : 87.84 Test acc : 88.41

2019-02-12 23:29:44,470 : ***** Transfer task : SUBJ *****


2019-02-12 23:29:44,486 : loading BERT mode bert-base-uncased
2019-02-12 23:29:44,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:29:44,506 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:29:44,506 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoo5qddlb
2019-02-12 23:29:46,940 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:29:48,523 : Generating sentence embeddings
2019-02-12 23:30:13,357 : Generated sentence embeddings
2019-02-12 23:30:13,358 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-12 23:30:49,669 : Best param found at split 1: l2reg = 0.001                 with score 94.96
2019-02-12 23:31:16,707 : Best param found at split 2: l2reg = 0.001                 with score 94.94
2019-02-12 23:31:38,845 : Best param found at split 3: l2reg = 0.001                 with score 94.44
2019-02-12 23:32:04,681 : Best param found at split 4: l2reg = 0.001                 with score 95.1
2019-02-12 23:32:17,919 : Best param found at split 5: l2reg = 0.001                 with score 94.58
2019-02-12 23:32:18,955 : Dev acc : 94.8 Test acc : 94.63

2019-02-12 23:32:18,956 : ***** Transfer task : SST Binary classification *****


2019-02-12 23:32:19,089 : loading BERT mode bert-base-uncased
2019-02-12 23:32:19,089 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:32:19,112 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:32:19,112 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfttd8ljp
2019-02-12 23:32:21,543 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:32:22,958 : Computing embedding for train
2019-02-12 23:33:33,916 : Computed train embeddings
2019-02-12 23:33:33,916 : Computing embedding for dev
2019-02-12 23:33:35,310 : Computed dev embeddings
2019-02-12 23:33:35,310 : Computing embedding for test
2019-02-12 23:33:38,390 : Computed test embeddings
2019-02-12 23:33:38,390 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:34:52,897 : [('reg:1e-05', 81.77), ('reg:0.0001', 81.88), ('reg:0.001', 82.11), ('reg:0.01', 81.54)]
2019-02-12 23:34:52,897 : Validation : best param found is reg = 0.001 with score             82.11
2019-02-12 23:34:52,897 : Evaluating...
2019-02-12 23:35:11,096 : 
Dev acc : 82.11 Test acc : 81.66 for             SST Binary classification

2019-02-12 23:35:11,096 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-12 23:35:11,145 : loading BERT mode bert-base-uncased
2019-02-12 23:35:11,146 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:35:11,168 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:35:11,168 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp05qg0qo_
2019-02-12 23:35:13,635 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:35:15,222 : Computing embedding for train
2019-02-12 23:35:32,034 : Computed train embeddings
2019-02-12 23:35:32,034 : Computing embedding for dev
2019-02-12 23:35:34,197 : Computed dev embeddings
2019-02-12 23:35:34,197 : Computing embedding for test
2019-02-12 23:35:38,521 : Computed test embeddings
2019-02-12 23:35:38,521 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:35:45,116 : [('reg:1e-05', 42.6), ('reg:0.0001', 42.78), ('reg:0.001', 42.05), ('reg:0.01', 41.87)]
2019-02-12 23:35:45,116 : Validation : best param found is reg = 0.0001 with score             42.78
2019-02-12 23:35:45,116 : Evaluating...
2019-02-12 23:35:46,796 : 
Dev acc : 42.78 Test acc : 45.11 for             SST Fine-Grained classification

2019-02-12 23:35:46,797 : ***** Transfer task : TREC *****


2019-02-12 23:35:46,810 : loading BERT mode bert-base-uncased
2019-02-12 23:35:46,810 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:35:46,830 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:35:46,830 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgmawr444
2019-02-12 23:35:49,263 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:35:59,263 : Computed train embeddings
2019-02-12 23:36:00,030 : Computed test embeddings
2019-02-12 23:36:00,030 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 23:36:18,387 : [('reg:1e-05', 82.92), ('reg:0.0001', 82.96), ('reg:0.001', 82.26), ('reg:0.01', 77.18)]
2019-02-12 23:36:18,387 : Cross-validation : best param found is reg = 0.0001             with score 82.96
2019-02-12 23:36:18,387 : Evaluating...
2019-02-12 23:36:19,554 : 
Dev acc : 82.96 Test acc : 91.4             for TREC

2019-02-12 23:36:19,554 : ***** Transfer task : MRPC *****


2019-02-12 23:36:19,612 : loading BERT mode bert-base-uncased
2019-02-12 23:36:19,612 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:36:19,631 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:36:19,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe6zn168j
2019-02-12 23:36:22,062 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:36:23,564 : Computing embedding for train
2019-02-12 23:36:38,795 : Computed train embeddings
2019-02-12 23:36:38,795 : Computing embedding for test
2019-02-12 23:36:44,271 : Computed test embeddings
2019-02-12 23:36:44,287 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-12 23:36:51,683 : [('reg:1e-05', 74.68), ('reg:0.0001', 74.9), ('reg:0.001', 74.76), ('reg:0.01', 72.74)]
2019-02-12 23:36:51,683 : Cross-validation : best param found is reg = 0.0001             with score 74.9
2019-02-12 23:36:51,683 : Evaluating...
2019-02-12 23:36:52,132 : Dev acc : 74.9 Test acc 72.93; Test F1 78.9 for MRPC.

2019-02-12 23:36:52,132 : ***** Transfer task : SICK-Entailment*****


2019-02-12 23:36:52,155 : loading BERT mode bert-base-uncased
2019-02-12 23:36:52,155 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:36:52,175 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:36:52,176 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwxwlgyif
2019-02-12 23:36:54,608 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:36:56,050 : Computing embedding for train
2019-02-12 23:37:03,822 : Computed train embeddings
2019-02-12 23:37:03,822 : Computing embedding for dev
2019-02-12 23:37:04,997 : Computed dev embeddings
2019-02-12 23:37:04,998 : Computing embedding for test
2019-02-12 23:37:15,486 : Computed test embeddings
2019-02-12 23:37:15,513 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:37:18,234 : [('reg:1e-05', 77.8), ('reg:0.0001', 78.2), ('reg:0.001', 78.8), ('reg:0.01', 77.2)]
2019-02-12 23:37:18,234 : Validation : best param found is reg = 0.001 with score             78.8
2019-02-12 23:37:18,235 : Evaluating...
2019-02-12 23:37:18,932 : 
Dev acc : 78.8 Test acc : 77.71 for                        SICK entailment

2019-02-12 23:37:18,932 : ***** Transfer task : SICK-Relatedness*****


2019-02-12 23:37:18,959 : loading BERT mode bert-base-uncased
2019-02-12 23:37:18,959 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:37:18,978 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:37:18,979 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1dyzf6ed
2019-02-12 23:37:21,411 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:37:22,923 : Computing embedding for train
2019-02-12 23:37:33,401 : Computed train embeddings
2019-02-12 23:37:33,401 : Computing embedding for dev
2019-02-12 23:37:34,703 : Computed dev embeddings
2019-02-12 23:37:34,703 : Computing embedding for test
2019-02-12 23:37:46,539 : Computed test embeddings
2019-02-12 23:38:37,572 : Dev : Pearson 0.791794694684616
2019-02-12 23:38:37,572 : Test : Pearson 0.802239181943406 Spearman 0.7393939491242097 MSE 0.3627628387879465                        for SICK Relatedness

2019-02-12 23:38:37,573 : 

***** Transfer task : STSBenchmark*****


2019-02-12 23:38:37,665 : loading BERT mode bert-base-uncased
2019-02-12 23:38:37,665 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:38:37,686 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:38:37,687 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7hnysxfz
2019-02-12 23:38:40,127 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:38:41,737 : Computing embedding for train
2019-02-12 23:39:02,310 : Computed train embeddings
2019-02-12 23:39:02,310 : Computing embedding for dev
2019-02-12 23:39:05,962 : Computed dev embeddings
2019-02-12 23:39:05,962 : Computing embedding for test
2019-02-12 23:39:11,556 : Computed test embeddings
2019-02-12 23:39:58,068 : Dev : Pearson 0.7283543310232476
2019-02-12 23:39:58,068 : Test : Pearson 0.6755339489390899 Spearman 0.6671697691282354 MSE 1.4424014281618733                        for SICK Relatedness

2019-02-12 23:39:58,069 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-12 23:39:58,327 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-12 23:39:58,337 : loading BERT mode bert-base-uncased
2019-02-12 23:39:58,337 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:39:58,434 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:39:58,434 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps4ybjdvl
2019-02-12 23:40:00,891 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:40:02,333 : Computing embeddings for train/dev/test
2019-02-12 23:42:25,518 : Computed embeddings
2019-02-12 23:42:25,519 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:43:34,636 : [('reg:1e-05', 90.69), ('reg:0.0001', 89.73), ('reg:0.001', 85.91), ('reg:0.01', 77.28)]
2019-02-12 23:43:34,636 : Validation : best param found is reg = 1e-05 with score             90.69
2019-02-12 23:43:34,636 : Evaluating...
2019-02-12 23:43:45,467 : 
Dev acc : 90.7 Test acc : 90.7 for LENGTH classification

2019-02-12 23:43:45,474 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-12 23:43:45,961 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-12 23:43:46,007 : loading BERT mode bert-base-uncased
2019-02-12 23:43:46,008 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:43:46,038 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:43:46,038 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0d4b1_qf
2019-02-12 23:43:48,470 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:43:49,905 : Computing embeddings for train/dev/test
2019-02-12 23:46:17,255 : Computed embeddings
2019-02-12 23:46:17,256 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:47:38,528 : [('reg:1e-05', 69.3), ('reg:0.0001', 34.36), ('reg:0.001', 2.87), ('reg:0.01', 0.86)]
2019-02-12 23:47:38,528 : Validation : best param found is reg = 1e-05 with score             69.3
2019-02-12 23:47:38,528 : Evaluating...
2019-02-12 23:47:53,604 : 
Dev acc : 69.3 Test acc : 69.0 for WORDCONTENT classification

2019-02-12 23:47:53,611 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-12 23:47:53,959 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-12 23:47:54,023 : loading BERT mode bert-base-uncased
2019-02-12 23:47:54,024 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:47:54,121 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:47:54,121 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa0bo26h6
2019-02-12 23:47:56,563 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:47:57,998 : Computing embeddings for train/dev/test
2019-02-12 23:50:55,756 : Computed embeddings
2019-02-12 23:50:55,756 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:52:02,109 : [('reg:1e-05', 38.84), ('reg:0.0001', 38.55), ('reg:0.001', 36.27), ('reg:0.01', 30.93)]
2019-02-12 23:52:02,109 : Validation : best param found is reg = 1e-05 with score             38.84
2019-02-12 23:52:02,109 : Evaluating...
2019-02-12 23:52:14,227 : 
Dev acc : 38.8 Test acc : 38.6 for DEPTH classification

2019-02-12 23:52:14,234 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-12 23:52:14,791 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-12 23:52:14,853 : loading BERT mode bert-base-uncased
2019-02-12 23:52:14,853 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:52:14,879 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:52:14,880 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1oo7v7w3
2019-02-12 23:52:17,342 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:52:18,870 : Computing embeddings for train/dev/test
2019-02-12 23:54:44,833 : Computed embeddings
2019-02-12 23:54:44,833 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:55:34,636 : [('reg:1e-05', 75.05), ('reg:0.0001', 74.69), ('reg:0.001', 68.79), ('reg:0.01', 57.87)]
2019-02-12 23:55:34,636 : Validation : best param found is reg = 1e-05 with score             75.05
2019-02-12 23:55:34,636 : Evaluating...
2019-02-12 23:55:45,880 : 
Dev acc : 75.0 Test acc : 75.5 for TOPCONSTITUENTS classification

2019-02-12 23:55:45,887 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-12 23:55:46,419 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-12 23:55:46,487 : loading BERT mode bert-base-uncased
2019-02-12 23:55:46,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:55:46,518 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:55:46,518 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnn1qkzw9
2019-02-12 23:55:48,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:55:50,406 : Computing embeddings for train/dev/test
2019-02-12 23:58:26,751 : Computed embeddings
2019-02-12 23:58:26,751 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-12 23:59:36,268 : [('reg:1e-05', 88.8), ('reg:0.0001', 88.79), ('reg:0.001', 88.52), ('reg:0.01', 87.25)]
2019-02-12 23:59:36,268 : Validation : best param found is reg = 1e-05 with score             88.8
2019-02-12 23:59:36,268 : Evaluating...
2019-02-12 23:59:48,595 : 
Dev acc : 88.8 Test acc : 88.5 for BIGRAMSHIFT classification

2019-02-12 23:59:48,603 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-12 23:59:49,033 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-12 23:59:49,102 : loading BERT mode bert-base-uncased
2019-02-12 23:59:49,102 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-12 23:59:49,135 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-12 23:59:49,135 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0n44ujd5
2019-02-12 23:59:51,585 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-12 23:59:53,009 : Computing embeddings for train/dev/test
2019-02-13 00:02:03,765 : Computed embeddings
2019-02-13 00:02:03,765 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:03:10,145 : [('reg:1e-05', 89.11), ('reg:0.0001', 89.17), ('reg:0.001', 89.66), ('reg:0.01', 89.48)]
2019-02-13 00:03:10,146 : Validation : best param found is reg = 0.001 with score             89.66
2019-02-13 00:03:10,146 : Evaluating...
2019-02-13 00:03:25,179 : 
Dev acc : 89.7 Test acc : 88.2 for TENSE classification

2019-02-13 00:03:25,186 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 00:03:25,594 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 00:03:25,658 : loading BERT mode bert-base-uncased
2019-02-13 00:03:25,658 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:03:25,776 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:03:25,776 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9627omuw
2019-02-13 00:03:28,209 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:03:29,752 : Computing embeddings for train/dev/test
2019-02-13 00:05:45,090 : Computed embeddings
2019-02-13 00:05:45,091 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:06:49,531 : [('reg:1e-05', 85.6), ('reg:0.0001', 85.5), ('reg:0.001', 85.39), ('reg:0.01', 82.4)]
2019-02-13 00:06:49,531 : Validation : best param found is reg = 1e-05 with score             85.6
2019-02-13 00:06:49,531 : Evaluating...
2019-02-13 00:07:16,881 : 
Dev acc : 85.6 Test acc : 84.7 for SUBJNUMBER classification

2019-02-13 00:07:16,889 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 00:07:17,289 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 00:07:17,353 : loading BERT mode bert-base-uncased
2019-02-13 00:07:17,354 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:07:17,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:07:17,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8i3a8njf
2019-02-13 00:07:19,904 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:07:21,449 : Computing embeddings for train/dev/test
2019-02-13 00:09:50,335 : Computed embeddings
2019-02-13 00:09:50,336 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:11:40,130 : [('reg:1e-05', 83.44), ('reg:0.0001', 83.35), ('reg:0.001', 83.76), ('reg:0.01', 82.84)]
2019-02-13 00:11:40,131 : Validation : best param found is reg = 0.001 with score             83.76
2019-02-13 00:11:40,131 : Evaluating...
2019-02-13 00:11:52,329 : 
Dev acc : 83.8 Test acc : 84.3 for OBJNUMBER classification

2019-02-13 00:11:52,336 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 00:11:52,911 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 00:11:52,980 : loading BERT mode bert-base-uncased
2019-02-13 00:11:52,980 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:11:53,009 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:11:53,010 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyuryzvus
2019-02-13 00:11:55,442 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:11:56,853 : Computing embeddings for train/dev/test
2019-02-13 00:14:25,771 : Computed embeddings
2019-02-13 00:14:25,771 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:16:10,697 : [('reg:1e-05', 62.92), ('reg:0.0001', 62.85), ('reg:0.001', 62.53), ('reg:0.01', 61.87)]
2019-02-13 00:16:10,698 : Validation : best param found is reg = 1e-05 with score             62.92
2019-02-13 00:16:10,698 : Evaluating...
2019-02-13 00:16:28,417 : 
Dev acc : 62.9 Test acc : 62.2 for ODDMANOUT classification

2019-02-13 00:16:28,424 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 00:16:29,015 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 00:16:29,091 : loading BERT mode bert-base-uncased
2019-02-13 00:16:29,091 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:16:29,120 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:16:29,121 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpugytrjya
2019-02-13 00:16:31,555 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:16:33,049 : Computing embeddings for train/dev/test
2019-02-13 00:19:04,195 : Computed embeddings
2019-02-13 00:19:04,195 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:20:57,092 : [('reg:1e-05', 64.07), ('reg:0.0001', 64.08), ('reg:0.001', 63.62), ('reg:0.01', 62.31)]
2019-02-13 00:20:57,092 : Validation : best param found is reg = 0.0001 with score             64.08
2019-02-13 00:20:57,092 : Evaluating...
2019-02-13 00:21:14,214 : 
Dev acc : 64.1 Test acc : 64.0 for COORDINATIONINVERSION classification

2019-02-13 00:21:14,216 : {'STS12': {'MSRpar': {'pearson': (0.39987696810347373, 3.6258325420391065e-30), 'spearman': SpearmanrResult(correlation=0.42810782471775116, pvalue=8.848200416801382e-35), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4839341278601113, 2.7477406701135628e-45), 'spearman': SpearmanrResult(correlation=0.4969126491646342, pvalue=5.068994125325079e-48), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.46469820335602596, 5.747226270672293e-26), 'spearman': SpearmanrResult(correlation=0.5811379211127722, pvalue=8.19316081824216e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6455041418376783, 1.2721327826421232e-89), 'spearman': SpearmanrResult(correlation=0.6474581138614914, pvalue=2.5016307021251743e-90), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6555340899797732, 2.328048731436526e-50), 'spearman': SpearmanrResult(correlation=0.5368096745101075, pvalue=3.691018262363576e-31), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5299095062274125, 'wmean': 0.5218278653775075}, 'spearman': {'mean': 0.5380852366733513, 'wmean': 0.5341982968880962}}}, 'STS13': {'FNWN': {'pearson': (0.3267387874508668, 4.460343551413744e-06), 'spearman': SpearmanrResult(correlation=0.34806457935937474, pvalue=9.215811950757538e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6275274563974401, 2.329016726859941e-83), 'spearman': SpearmanrResult(correlation=0.6051046990267782, pvalue=4.212414215182887e-76), 'nsamples': 750}, 'OnWN': {'pearson': (0.4352071701250386, 2.4931316379733817e-27), 'spearman': SpearmanrResult(correlation=0.46436159142579775, pvalue=2.3810198257234e-31), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.46315780465778184, 'wmean': 0.5177002970442937}, 'spearman': {'mean': 0.47251028993731686, 'wmean': 0.5200797217059187}}}, 'STS14': {'deft-forum': {'pearson': (0.3019240870963417, 6.14330050410409e-11), 'spearman': SpearmanrResult(correlation=0.3306725895101315, pvalue=6.083106612833528e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7436307262953313, 4.8944090294914674e-54), 'spearman': SpearmanrResult(correlation=0.7124588364696102, pvalue=9.24739516663663e-48), 'nsamples': 300}, 'headlines': {'pearson': (0.5851754708229826, 4.042740722587093e-70), 'spearman': SpearmanrResult(correlation=0.5401767427911999, pvalue=4.9126171939649495e-58), 'nsamples': 750}, 'images': {'pearson': (0.55600150784908, 4.476182597624546e-62), 'spearman': SpearmanrResult(correlation=0.5507212057792595, pvalue=1.0547751230837684e-60), 'nsamples': 750}, 'OnWN': {'pearson': (0.6241044000454519, 3.261723689218141e-82), 'spearman': SpearmanrResult(correlation=0.6607922410812295, pvalue=2.729989547748402e-95), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6648915451584066, 7.228432627234121e-97), 'spearman': SpearmanrResult(correlation=0.6197994846551127, pvalue=8.608542801857631e-81), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5792879562112657, 'wmean': 0.5817559333303718}, 'spearman': {'mean': 0.5691035167144238, 'wmean': 0.570975352520145}}}, 'STS15': {'answers-forums': {'pearson': (0.5409696348688737, 6.868880217893726e-30), 'spearman': SpearmanrResult(correlation=0.5320623092598971, pvalue=8.532334952175063e-29), 'nsamples': 375}, 'answers-students': {'pearson': (0.6890741589706266, 1.047635680574934e-106), 'spearman': SpearmanrResult(correlation=0.6949460583651783, pvalue=3.043957536872437e-109), 'nsamples': 750}, 'belief': {'pearson': (0.6458556861655027, 1.2112624062624087e-45), 'spearman': SpearmanrResult(correlation=0.6760403512783062, pvalue=2.0896179024618307e-51), 'nsamples': 375}, 'headlines': {'pearson': (0.6419238118671431, 2.4287115390374725e-88), 'spearman': SpearmanrResult(correlation=0.6321219746923213, pvalue=6.395771982682992e-85), 'nsamples': 750}, 'images': {'pearson': (0.6816544393402457, 1.381380107711421e-103), 'spearman': SpearmanrResult(correlation=0.689661417127059, pvalue=5.87806112357306e-107), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6398955462424784, 'wmean': 0.6515162676738009}, 'spearman': {'mean': 0.6449664221445524, 'wmean': 0.6551951951134152}}}, 'STS16': {'answer-answer': {'pearson': (0.5459480092434162, 3.9226408812001924e-21), 'spearman': SpearmanrResult(correlation=0.5657926282459426, pvalue=6.775691537511024e-23), 'nsamples': 254}, 'headlines': {'pearson': (0.6578206494131161, 3.006200494660631e-32), 'spearman': SpearmanrResult(correlation=0.6566768120810473, pvalue=4.1757465461607004e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7633579091317967, 3.695959591591333e-45), 'spearman': SpearmanrResult(correlation=0.7733140757091234, pvalue=5.162319277518417e-47), 'nsamples': 230}, 'postediting': {'pearson': (0.7979953435642065, 3.8434136389495176e-55), 'spearman': SpearmanrResult(correlation=0.8327431661758349, pvalue=4.498134703490814e-64), 'nsamples': 244}, 'question-question': {'pearson': (0.2986284508699804, 1.1233149195517652e-05), 'spearman': SpearmanrResult(correlation=0.29228122263339584, pvalue=1.7527250060032648e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6127500724445032, 'wmean': 0.6198690263182961}, 'spearman': {'mean': 0.6241615809690688, 'wmean': 0.6318399656602294}}}, 'MR': {'devacc': 77.71, 'acc': 77.68, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 82.05, 'acc': 81.24, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.84, 'acc': 88.41, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.8, 'acc': 94.63, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.11, 'acc': 81.66, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.78, 'acc': 45.11, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 82.96, 'acc': 91.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.9, 'acc': 72.93, 'f1': 78.9, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.8, 'acc': 77.71, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.791794694684616, 'pearson': 0.802239181943406, 'spearman': 0.7393939491242097, 'mse': 0.3627628387879465, 'yhat': array([3.51093882, 4.03542727, 1.80998682, ..., 3.15631378, 4.41123722,
       4.52352101]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7283543310232476, 'pearson': 0.6755339489390899, 'spearman': 0.6671697691282354, 'mse': 1.4424014281618733, 'yhat': array([1.9585546 , 2.0015251 , 2.13933595, ..., 3.95861833, 3.84973922,
       3.61874943]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 90.69, 'acc': 90.69, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 69.3, 'acc': 68.98, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 38.84, 'acc': 38.64, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 75.05, 'acc': 75.52, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.8, 'acc': 88.45, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.66, 'acc': 88.18, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.6, 'acc': 84.69, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.76, 'acc': 84.31, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.92, 'acc': 62.21, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 64.08, 'acc': 64.05, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 00:21:14,216 : ********************************************************************************
2019-02-13 00:21:14,216 : ********************************************************************************
2019-02-13 00:21:14,216 : ********************************************************************************
2019-02-13 00:21:14,216 : layer 7
2019-02-13 00:21:14,216 : ********************************************************************************
2019-02-13 00:21:14,216 : ********************************************************************************
2019-02-13 00:21:14,217 : ********************************************************************************
2019-02-13 00:21:14,300 : ***** Transfer task : STS12 *****


2019-02-13 00:21:14,312 : loading BERT mode bert-base-uncased
2019-02-13 00:21:14,312 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:21:14,329 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:21:14,330 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp0x643y_
2019-02-13 00:21:16,766 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:21:20,829 : MSRpar : pearson = 0.3924, spearman = 0.4191
2019-02-13 00:21:22,696 : MSRvid : pearson = 0.4716, spearman = 0.4905
2019-02-13 00:21:24,136 : SMTeuroparl : pearson = 0.4642, spearman = 0.5847
2019-02-13 00:21:26,675 : surprise.OnWN : pearson = 0.6317, spearman = 0.6294
2019-02-13 00:21:28,092 : surprise.SMTnews : pearson = 0.6679, spearman = 0.5461
2019-02-13 00:21:28,092 : ALL (weighted average) : Pearson = 0.5152,             Spearman = 0.5278
2019-02-13 00:21:28,092 : ALL (average) : Pearson = 0.5256,             Spearman = 0.5339

2019-02-13 00:21:28,092 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 00:21:28,102 : loading BERT mode bert-base-uncased
2019-02-13 00:21:28,102 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:21:28,120 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:21:28,120 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb2bzznc7
2019-02-13 00:21:30,549 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:21:32,897 : FNWN : pearson = 0.3250, spearman = 0.3498
2019-02-13 00:21:34,987 : headlines : pearson = 0.6159, spearman = 0.5903
2019-02-13 00:21:36,654 : OnWN : pearson = 0.4151, spearman = 0.4339
2019-02-13 00:21:36,654 : ALL (weighted average) : Pearson = 0.5042,             Spearman = 0.5015
2019-02-13 00:21:36,654 : ALL (average) : Pearson = 0.4520,             Spearman = 0.4580

2019-02-13 00:21:36,655 : ***** Transfer task : STS14 *****


2019-02-13 00:21:36,671 : loading BERT mode bert-base-uncased
2019-02-13 00:21:36,671 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:21:36,689 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:21:36,689 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgk9olvxv
2019-02-13 00:21:39,137 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:21:41,560 : deft-forum : pearson = 0.3059, spearman = 0.3317
2019-02-13 00:21:42,533 : deft-news : pearson = 0.7434, spearman = 0.7054
2019-02-13 00:21:44,089 : headlines : pearson = 0.5790, spearman = 0.5362
2019-02-13 00:21:45,589 : images : pearson = 0.5357, spearman = 0.5299
2019-02-13 00:21:47,101 : OnWN : pearson = 0.6167, spearman = 0.6475
2019-02-13 00:21:48,936 : tweet-news : pearson = 0.6710, spearman = 0.6142
2019-02-13 00:21:48,936 : ALL (weighted average) : Pearson = 0.5767,             Spearman = 0.5618
2019-02-13 00:21:48,936 : ALL (average) : Pearson = 0.5753,             Spearman = 0.5608

2019-02-13 00:21:48,936 : ***** Transfer task : STS15 *****


2019-02-13 00:21:48,970 : loading BERT mode bert-base-uncased
2019-02-13 00:21:48,970 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:21:48,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:21:48,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9xapay5y
2019-02-13 00:21:51,438 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:21:54,064 : answers-forums : pearson = 0.5636, spearman = 0.5603
2019-02-13 00:21:55,562 : answers-students : pearson = 0.6789, spearman = 0.6845
2019-02-13 00:21:56,737 : belief : pearson = 0.6593, spearman = 0.6908
2019-02-13 00:21:58,310 : headlines : pearson = 0.6272, spearman = 0.6190
2019-02-13 00:21:59,827 : images : pearson = 0.6602, spearman = 0.6669
2019-02-13 00:21:59,827 : ALL (weighted average) : Pearson = 0.6444,             Spearman = 0.6490
2019-02-13 00:21:59,827 : ALL (average) : Pearson = 0.6378,             Spearman = 0.6443

2019-02-13 00:21:59,827 : ***** Transfer task : STS16 *****


2019-02-13 00:21:59,868 : loading BERT mode bert-base-uncased
2019-02-13 00:21:59,869 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:21:59,887 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:21:59,887 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp211caknc
2019-02-13 00:22:02,324 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:22:04,357 : answer-answer : pearson = 0.5644, spearman = 0.5845
2019-02-13 00:22:04,865 : headlines : pearson = 0.6490, spearman = 0.6468
2019-02-13 00:22:05,448 : plagiarism : pearson = 0.7653, spearman = 0.7772
2019-02-13 00:22:06,284 : postediting : pearson = 0.8062, spearman = 0.8399
2019-02-13 00:22:06,655 : question-question : pearson = 0.3339, spearman = 0.3299
2019-02-13 00:22:06,656 : ALL (weighted average) : Pearson = 0.6302,             Spearman = 0.6426
2019-02-13 00:22:06,656 : ALL (average) : Pearson = 0.6238,             Spearman = 0.6357

2019-02-13 00:22:06,656 : ***** Transfer task : MR *****


2019-02-13 00:22:06,707 : loading BERT mode bert-base-uncased
2019-02-13 00:22:06,707 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:22:06,730 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:22:06,730 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpemwvjyqg
2019-02-13 00:22:09,186 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:22:10,664 : Generating sentence embeddings
2019-02-13 00:22:31,134 : Generated sentence embeddings
2019-02-13 00:22:31,134 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:22:54,395 : Best param found at split 1: l2reg = 1e-05                 with score 78.41
2019-02-13 00:23:19,201 : Best param found at split 2: l2reg = 1e-05                 with score 78.5
2019-02-13 00:23:54,602 : Best param found at split 3: l2reg = 1e-05                 with score 79.1
2019-02-13 00:24:32,250 : Best param found at split 4: l2reg = 0.001                 with score 78.65
2019-02-13 00:25:03,426 : Best param found at split 5: l2reg = 1e-05                 with score 79.16
2019-02-13 00:25:05,020 : Dev acc : 78.76 Test acc : 78.01

2019-02-13 00:25:05,021 : ***** Transfer task : CR *****


2019-02-13 00:25:05,028 : loading BERT mode bert-base-uncased
2019-02-13 00:25:05,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:25:05,049 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:25:05,050 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxqc6nrvn
2019-02-13 00:25:07,522 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:25:09,023 : Generating sentence embeddings
2019-02-13 00:25:15,589 : Generated sentence embeddings
2019-02-13 00:25:15,589 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:25:25,151 : Best param found at split 1: l2reg = 1e-05                 with score 83.44
2019-02-13 00:25:34,778 : Best param found at split 2: l2reg = 1e-05                 with score 83.7
2019-02-13 00:25:44,098 : Best param found at split 3: l2reg = 0.0001                 with score 84.37
2019-02-13 00:25:51,964 : Best param found at split 4: l2reg = 0.01                 with score 84.01
2019-02-13 00:25:57,697 : Best param found at split 5: l2reg = 1e-05                 with score 83.55
2019-02-13 00:25:57,902 : Dev acc : 83.81 Test acc : 82.62

2019-02-13 00:25:57,902 : ***** Transfer task : MPQA *****


2019-02-13 00:25:57,937 : loading BERT mode bert-base-uncased
2019-02-13 00:25:57,937 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:25:57,956 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:25:57,957 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdbf2usa5
2019-02-13 00:26:00,405 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:26:01,881 : Generating sentence embeddings
2019-02-13 00:26:10,988 : Generated sentence embeddings
2019-02-13 00:26:10,989 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:26:36,695 : Best param found at split 1: l2reg = 0.001                 with score 88.79
2019-02-13 00:27:03,991 : Best param found at split 2: l2reg = 0.001                 with score 88.57
2019-02-13 00:27:41,571 : Best param found at split 3: l2reg = 0.001                 with score 86.28
2019-02-13 00:28:17,029 : Best param found at split 4: l2reg = 1e-05                 with score 88.87
2019-02-13 00:28:43,556 : Best param found at split 5: l2reg = 0.001                 with score 87.85
2019-02-13 00:28:44,889 : Dev acc : 88.07 Test acc : 88.14

2019-02-13 00:28:44,890 : ***** Transfer task : SUBJ *****


2019-02-13 00:28:44,905 : loading BERT mode bert-base-uncased
2019-02-13 00:28:44,905 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:28:44,959 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:28:44,959 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5p5xb83m
2019-02-13 00:28:47,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:28:48,872 : Generating sentence embeddings
2019-02-13 00:29:10,757 : Generated sentence embeddings
2019-02-13 00:29:10,757 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 00:29:33,286 : Best param found at split 1: l2reg = 0.001                 with score 94.85
2019-02-13 00:29:46,432 : Best param found at split 2: l2reg = 0.001                 with score 94.98
2019-02-13 00:30:09,969 : Best param found at split 3: l2reg = 1e-05                 with score 94.4
2019-02-13 00:30:34,730 : Best param found at split 4: l2reg = 0.001                 with score 95.24
2019-02-13 00:31:06,619 : Best param found at split 5: l2reg = 0.001                 with score 95.0
2019-02-13 00:31:08,584 : Dev acc : 94.89 Test acc : 94.27

2019-02-13 00:31:08,585 : ***** Transfer task : SST Binary classification *****


2019-02-13 00:31:08,727 : loading BERT mode bert-base-uncased
2019-02-13 00:31:08,727 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:31:08,750 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:31:08,750 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfk3f4hzv
2019-02-13 00:31:11,188 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:31:12,755 : Computing embedding for train
2019-02-13 00:33:05,918 : Computed train embeddings
2019-02-13 00:33:05,918 : Computing embedding for dev
2019-02-13 00:33:07,624 : Computed dev embeddings
2019-02-13 00:33:07,624 : Computing embedding for test
2019-02-13 00:33:11,329 : Computed test embeddings
2019-02-13 00:33:11,329 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:33:51,434 : [('reg:1e-05', 83.26), ('reg:0.0001', 83.37), ('reg:0.001', 83.37), ('reg:0.01', 82.57)]
2019-02-13 00:33:51,434 : Validation : best param found is reg = 0.0001 with score             83.37
2019-02-13 00:33:51,434 : Evaluating...
2019-02-13 00:34:03,296 : 
Dev acc : 83.37 Test acc : 83.42 for             SST Binary classification

2019-02-13 00:34:03,301 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 00:34:03,350 : loading BERT mode bert-base-uncased
2019-02-13 00:34:03,350 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:34:03,372 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:34:03,372 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp49kmf3kk
2019-02-13 00:34:05,813 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:34:07,284 : Computing embedding for train
2019-02-13 00:34:21,464 : Computed train embeddings
2019-02-13 00:34:21,465 : Computing embedding for dev
2019-02-13 00:34:23,366 : Computed dev embeddings
2019-02-13 00:34:23,366 : Computing embedding for test
2019-02-13 00:34:27,191 : Computed test embeddings
2019-02-13 00:34:27,191 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:34:32,837 : [('reg:1e-05', 42.05), ('reg:0.0001', 42.23), ('reg:0.001', 42.78), ('reg:0.01', 43.23)]
2019-02-13 00:34:32,837 : Validation : best param found is reg = 0.01 with score             43.23
2019-02-13 00:34:32,837 : Evaluating...
2019-02-13 00:34:34,240 : 
Dev acc : 43.23 Test acc : 42.67 for             SST Fine-Grained classification

2019-02-13 00:34:34,241 : ***** Transfer task : TREC *****


2019-02-13 00:34:34,253 : loading BERT mode bert-base-uncased
2019-02-13 00:34:34,253 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:34:34,273 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:34:34,273 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcsyn4kcg
2019-02-13 00:34:36,737 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:34:45,632 : Computed train embeddings
2019-02-13 00:34:46,288 : Computed test embeddings
2019-02-13 00:34:46,288 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 00:35:03,537 : [('reg:1e-05', 84.54), ('reg:0.0001', 84.37), ('reg:0.001', 83.77), ('reg:0.01', 77.92)]
2019-02-13 00:35:03,537 : Cross-validation : best param found is reg = 1e-05             with score 84.54
2019-02-13 00:35:03,537 : Evaluating...
2019-02-13 00:35:04,562 : 
Dev acc : 84.54 Test acc : 93.0             for TREC

2019-02-13 00:35:04,563 : ***** Transfer task : MRPC *****


2019-02-13 00:35:04,583 : loading BERT mode bert-base-uncased
2019-02-13 00:35:04,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:35:04,606 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:35:04,606 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp21alvqn9
2019-02-13 00:35:07,080 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:35:08,579 : Computing embedding for train
2019-02-13 00:35:24,787 : Computed train embeddings
2019-02-13 00:35:24,787 : Computing embedding for test
2019-02-13 00:35:31,937 : Computed test embeddings
2019-02-13 00:35:31,954 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 00:35:45,392 : [('reg:1e-05', 74.63), ('reg:0.0001', 74.56), ('reg:0.001', 74.24), ('reg:0.01', 72.65)]
2019-02-13 00:35:45,393 : Cross-validation : best param found is reg = 1e-05             with score 74.63
2019-02-13 00:35:45,393 : Evaluating...
2019-02-13 00:35:46,136 : Dev acc : 74.63 Test acc 70.26; Test F1 76.35 for MRPC.

2019-02-13 00:35:46,136 : ***** Transfer task : SICK-Entailment*****


2019-02-13 00:35:46,160 : loading BERT mode bert-base-uncased
2019-02-13 00:35:46,160 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:35:46,180 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:35:46,180 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsb8ial_2
2019-02-13 00:35:48,615 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:35:50,110 : Computing embedding for train
2019-02-13 00:36:02,372 : Computed train embeddings
2019-02-13 00:36:02,373 : Computing embedding for dev
2019-02-13 00:36:03,783 : Computed dev embeddings
2019-02-13 00:36:03,783 : Computing embedding for test
2019-02-13 00:36:16,664 : Computed test embeddings
2019-02-13 00:36:16,691 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:36:21,976 : [('reg:1e-05', 78.4), ('reg:0.0001', 78.0), ('reg:0.001', 77.8), ('reg:0.01', 78.0)]
2019-02-13 00:36:21,976 : Validation : best param found is reg = 1e-05 with score             78.4
2019-02-13 00:36:21,977 : Evaluating...
2019-02-13 00:36:23,141 : 
Dev acc : 78.4 Test acc : 78.16 for                        SICK entailment

2019-02-13 00:36:23,142 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 00:36:23,168 : loading BERT mode bert-base-uncased
2019-02-13 00:36:23,168 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:36:23,226 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:36:23,226 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4rvxiy31
2019-02-13 00:36:25,667 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:36:27,153 : Computing embedding for train
2019-02-13 00:36:37,301 : Computed train embeddings
2019-02-13 00:36:37,301 : Computing embedding for dev
2019-02-13 00:36:38,592 : Computed dev embeddings
2019-02-13 00:36:38,592 : Computing embedding for test
2019-02-13 00:36:50,069 : Computed test embeddings
2019-02-13 00:37:38,773 : Dev : Pearson 0.7878668978345402
2019-02-13 00:37:38,774 : Test : Pearson 0.8056425074402782 Spearman 0.7394980348519594 MSE 0.35792682498059625                        for SICK Relatedness

2019-02-13 00:37:38,774 : 

***** Transfer task : STSBenchmark*****


2019-02-13 00:37:38,814 : loading BERT mode bert-base-uncased
2019-02-13 00:37:38,814 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:37:38,842 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:37:38,842 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnzkwxn_5
2019-02-13 00:37:41,271 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:37:42,803 : Computing embedding for train
2019-02-13 00:38:02,200 : Computed train embeddings
2019-02-13 00:38:02,200 : Computing embedding for dev
2019-02-13 00:38:06,236 : Computed dev embeddings
2019-02-13 00:38:06,236 : Computing embedding for test
2019-02-13 00:38:10,029 : Computed test embeddings
2019-02-13 00:38:54,528 : Dev : Pearson 0.7271555770577297
2019-02-13 00:38:54,529 : Test : Pearson 0.668318077803957 Spearman 0.6647568027610277 MSE 1.4627233652334592                        for SICK Relatedness

2019-02-13 00:38:54,529 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 00:38:54,804 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 00:38:54,815 : loading BERT mode bert-base-uncased
2019-02-13 00:38:54,815 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:38:54,840 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:38:54,840 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_c0ii_m9
2019-02-13 00:38:57,279 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:38:58,700 : Computing embeddings for train/dev/test
2019-02-13 00:41:40,688 : Computed embeddings
2019-02-13 00:41:40,688 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:42:42,541 : [('reg:1e-05', 89.77), ('reg:0.0001', 88.98), ('reg:0.001', 82.85), ('reg:0.01', 72.69)]
2019-02-13 00:42:42,542 : Validation : best param found is reg = 1e-05 with score             89.77
2019-02-13 00:42:42,542 : Evaluating...
2019-02-13 00:42:55,739 : 
Dev acc : 89.8 Test acc : 89.6 for LENGTH classification

2019-02-13 00:42:55,746 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 00:42:56,102 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 00:42:56,146 : loading BERT mode bert-base-uncased
2019-02-13 00:42:56,146 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:42:56,247 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:42:56,247 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx70e05kd
2019-02-13 00:42:58,706 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:43:00,090 : Computing embeddings for train/dev/test
2019-02-13 00:45:26,641 : Computed embeddings
2019-02-13 00:45:26,641 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:46:42,628 : [('reg:1e-05', 65.55), ('reg:0.0001', 37.55), ('reg:0.001', 4.06), ('reg:0.01', 0.92)]
2019-02-13 00:46:42,628 : Validation : best param found is reg = 1e-05 with score             65.55
2019-02-13 00:46:42,628 : Evaluating...
2019-02-13 00:46:56,274 : 
Dev acc : 65.5 Test acc : 65.8 for WORDCONTENT classification

2019-02-13 00:46:56,282 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 00:46:56,624 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 00:46:56,689 : loading BERT mode bert-base-uncased
2019-02-13 00:46:56,689 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:46:56,789 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:46:56,789 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe46glh6l
2019-02-13 00:46:59,222 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:47:00,648 : Computing embeddings for train/dev/test
2019-02-13 00:49:18,415 : Computed embeddings
2019-02-13 00:49:18,415 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:50:40,840 : [('reg:1e-05', 38.46), ('reg:0.0001', 38.27), ('reg:0.001', 36.31), ('reg:0.01', 30.37)]
2019-02-13 00:50:40,840 : Validation : best param found is reg = 1e-05 with score             38.46
2019-02-13 00:50:40,840 : Evaluating...
2019-02-13 00:50:53,856 : 
Dev acc : 38.5 Test acc : 38.6 for DEPTH classification

2019-02-13 00:50:53,864 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 00:50:54,429 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 00:50:54,493 : loading BERT mode bert-base-uncased
2019-02-13 00:50:54,493 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:50:54,522 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:50:54,522 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_7zfugx1
2019-02-13 00:50:56,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:50:58,444 : Computing embeddings for train/dev/test
2019-02-13 00:53:18,009 : Computed embeddings
2019-02-13 00:53:18,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:54:27,768 : [('reg:1e-05', 74.51), ('reg:0.0001', 73.63), ('reg:0.001', 67.13), ('reg:0.01', 58.58)]
2019-02-13 00:54:27,768 : Validation : best param found is reg = 1e-05 with score             74.51
2019-02-13 00:54:27,769 : Evaluating...
2019-02-13 00:54:41,821 : 
Dev acc : 74.5 Test acc : 74.0 for TOPCONSTITUENTS classification

2019-02-13 00:54:41,828 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 00:54:42,354 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 00:54:42,419 : loading BERT mode bert-base-uncased
2019-02-13 00:54:42,419 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:54:42,452 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:54:42,452 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_ki8j1_9
2019-02-13 00:54:44,894 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:54:46,358 : Computing embeddings for train/dev/test
2019-02-13 00:57:07,127 : Computed embeddings
2019-02-13 00:57:07,127 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 00:58:14,039 : [('reg:1e-05', 90.1), ('reg:0.0001', 90.04), ('reg:0.001', 90.06), ('reg:0.01', 88.52)]
2019-02-13 00:58:14,039 : Validation : best param found is reg = 1e-05 with score             90.1
2019-02-13 00:58:14,040 : Evaluating...
2019-02-13 00:58:36,123 : 
Dev acc : 90.1 Test acc : 89.5 for BIGRAMSHIFT classification

2019-02-13 00:58:36,130 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 00:58:36,551 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 00:58:36,618 : loading BERT mode bert-base-uncased
2019-02-13 00:58:36,618 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 00:58:36,648 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 00:58:36,648 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7lbaxz4v
2019-02-13 00:58:39,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 00:58:40,632 : Computing embeddings for train/dev/test
2019-02-13 01:00:56,428 : Computed embeddings
2019-02-13 01:00:56,428 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:01:58,938 : [('reg:1e-05', 89.65), ('reg:0.0001', 89.74), ('reg:0.001', 89.86), ('reg:0.01', 89.67)]
2019-02-13 01:01:58,938 : Validation : best param found is reg = 0.001 with score             89.86
2019-02-13 01:01:58,938 : Evaluating...
2019-02-13 01:02:17,828 : 
Dev acc : 89.9 Test acc : 88.9 for TENSE classification

2019-02-13 01:02:17,836 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 01:02:18,257 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 01:02:18,321 : loading BERT mode bert-base-uncased
2019-02-13 01:02:18,321 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:02:18,442 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:02:18,442 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx1p6mpgn
2019-02-13 01:02:20,874 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:02:22,394 : Computing embeddings for train/dev/test
2019-02-13 01:04:48,269 : Computed embeddings
2019-02-13 01:04:48,269 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:05:36,788 : [('reg:1e-05', 86.37), ('reg:0.0001', 86.27), ('reg:0.001', 86.46), ('reg:0.01', 85.4)]
2019-02-13 01:05:36,788 : Validation : best param found is reg = 0.001 with score             86.46
2019-02-13 01:05:36,789 : Evaluating...
2019-02-13 01:05:53,166 : 
Dev acc : 86.5 Test acc : 86.6 for SUBJNUMBER classification

2019-02-13 01:05:53,173 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 01:05:53,575 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 01:05:53,642 : loading BERT mode bert-base-uncased
2019-02-13 01:05:53,642 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:05:53,760 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:05:53,760 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpohquqg2l
2019-02-13 01:05:56,190 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:05:57,789 : Computing embeddings for train/dev/test
2019-02-13 01:08:31,835 : Computed embeddings
2019-02-13 01:08:31,835 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:09:45,638 : [('reg:1e-05', 83.66), ('reg:0.0001', 83.65), ('reg:0.001', 83.61), ('reg:0.01', 82.44)]
2019-02-13 01:09:45,638 : Validation : best param found is reg = 1e-05 with score             83.66
2019-02-13 01:09:45,638 : Evaluating...
2019-02-13 01:10:11,206 : 
Dev acc : 83.7 Test acc : 84.5 for OBJNUMBER classification

2019-02-13 01:10:11,213 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 01:10:11,775 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 01:10:11,843 : loading BERT mode bert-base-uncased
2019-02-13 01:10:11,843 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:10:11,872 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:10:11,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa2bf_nr8
2019-02-13 01:10:14,327 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:10:15,805 : Computing embeddings for train/dev/test
2019-02-13 01:13:08,779 : Computed embeddings
2019-02-13 01:13:08,780 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:14:18,778 : [('reg:1e-05', 65.02), ('reg:0.0001', 64.96), ('reg:0.001', 64.8), ('reg:0.01', 63.73)]
2019-02-13 01:14:18,779 : Validation : best param found is reg = 1e-05 with score             65.02
2019-02-13 01:14:18,779 : Evaluating...
2019-02-13 01:14:36,517 : 
Dev acc : 65.0 Test acc : 64.6 for ODDMANOUT classification

2019-02-13 01:14:36,524 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 01:14:36,946 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 01:14:37,022 : loading BERT mode bert-base-uncased
2019-02-13 01:14:37,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:14:37,054 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:14:37,055 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9ffd2si0
2019-02-13 01:14:39,533 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:14:41,027 : Computing embeddings for train/dev/test
2019-02-13 01:17:25,621 : Computed embeddings
2019-02-13 01:17:25,622 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:19:00,514 : [('reg:1e-05', 67.62), ('reg:0.0001', 67.44), ('reg:0.001', 69.94), ('reg:0.01', 66.04)]
2019-02-13 01:19:00,515 : Validation : best param found is reg = 0.001 with score             69.94
2019-02-13 01:19:00,515 : Evaluating...
2019-02-13 01:19:20,148 : 
Dev acc : 69.9 Test acc : 69.3 for COORDINATIONINVERSION classification

2019-02-13 01:19:20,157 : {'STS12': {'MSRpar': {'pearson': (0.3924020319939267, 5.110540932933977e-29), 'spearman': SpearmanrResult(correlation=0.4190738932657634, pvalue=2.9577727188863983e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.47162075912408014, 8.445546325743629e-43), 'spearman': SpearmanrResult(correlation=0.49051205726496033, pvalue=1.1693451991403414e-46), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.46416593300116343, 6.645117651820876e-26), 'spearman': SpearmanrResult(correlation=0.5847147533163599, pvalue=1.9230923240514167e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6317233154812663, 8.758057096270178e-85), 'spearman': SpearmanrResult(correlation=0.6293590219685372, pvalue=5.5966812076654985e-84), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.667909471111825, 7.0284900138157e-53), 'spearman': SpearmanrResult(correlation=0.5460680688746249, pvalue=2.1728970254519727e-32), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5255643021424523, 'wmean': 0.515237330170723}, 'spearman': {'mean': 0.5339455589380492, 'wmean': 0.5278230246549647}}}, 'STS13': {'FNWN': {'pearson': (0.32499569866746186, 5.047889574288899e-06), 'spearman': SpearmanrResult(correlation=0.3498446463955529, pvalue=8.036016272545803e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6159097504304419, 1.5864569540560712e-79), 'spearman': SpearmanrResult(correlation=0.5902683769867664, pvalue=1.310579983990421e-71), 'nsamples': 750}, 'OnWN': {'pearson': (0.41511402113403356, 8.888723949730702e-25), 'spearman': SpearmanrResult(correlation=0.43389061963754916, pvalue=3.709027701142556e-27), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4520064900773125, 'wmean': 0.5041569771514498}, 'spearman': {'mean': 0.45800121433995616, 'wmean': 0.5014897056836662}}}, 'STS14': {'deft-forum': {'pearson': (0.3059469671024042, 3.3173982700092136e-11), 'spearman': SpearmanrResult(correlation=0.3317155514959915, pvalue=5.09708439361095e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7434077473106647, 5.4677746570982e-54), 'spearman': SpearmanrResult(correlation=0.7054090593408965, pvalue=1.8651850848580034e-46), 'nsamples': 300}, 'headlines': {'pearson': (0.5790238274332603, 2.347931446148623e-68), 'spearman': SpearmanrResult(correlation=0.5361764450306294, pvalue=4.7756458772479184e-57), 'nsamples': 750}, 'images': {'pearson': (0.5356773794172422, 6.328920326927348e-57), 'spearman': SpearmanrResult(correlation=0.5298844982241631, pvalue=1.6070702528854876e-55), 'nsamples': 750}, 'OnWN': {'pearson': (0.6166935741395135, 8.84793508414423e-80), 'spearman': SpearmanrResult(correlation=0.6475438836983032, pvalue=2.3286306185683466e-90), 'nsamples': 750}, 'tweet-news': {'pearson': (0.670981045133349, 2.947154321986188e-99), 'spearman': SpearmanrResult(correlation=0.6141684573114827, pvalue=5.770704330507687e-79), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.575288423422739, 'wmean': 0.5766614210618147}, 'spearman': {'mean': 0.5608163158502445, 'wmean': 0.5617932477797064}}}, 'STS15': {'answers-forums': {'pearson': (0.5635858292899794, 8.076388605188986e-33), 'spearman': SpearmanrResult(correlation=0.5603225341018957, pvalue=2.207360477871102e-32), 'nsamples': 375}, 'answers-students': {'pearson': (0.6789339114920632, 1.8248550928792453e-102), 'spearman': SpearmanrResult(correlation=0.6844870668803423, pvalue=9.123410263499205e-105), 'nsamples': 750}, 'belief': {'pearson': (0.6592526715400386, 4.0512455843000424e-48), 'spearman': SpearmanrResult(correlation=0.6907674703659085, pvalue=1.7825095300566316e-54), 'nsamples': 375}, 'headlines': {'pearson': (0.6271697762001254, 3.0734001506537003e-83), 'spearman': SpearmanrResult(correlation=0.6190311166778797, pvalue=1.5357571088527542e-80), 'nsamples': 750}, 'images': {'pearson': (0.660192855528555, 4.6203864139896516e-95), 'spearman': SpearmanrResult(correlation=0.6668800735718796, pvalue=1.2160010524068813e-97), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6378270088101523, 'wmean': 0.6444289484089382}, 'spearman': {'mean': 0.6442976523195811, 'wmean': 0.6489858148410009}}}, 'STS16': {'answer-answer': {'pearson': (0.5643685628397859, 9.149683270743288e-23), 'spearman': SpearmanrResult(correlation=0.5845260755580295, pvalue=1.1339889247163047e-24), 'nsamples': 254}, 'headlines': {'pearson': (0.6490067712757833, 3.646728357603356e-31), 'spearman': SpearmanrResult(correlation=0.6468326482926066, pvalue=6.6646181964762275e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7652704464132425, 1.6541091849053773e-45), 'spearman': SpearmanrResult(correlation=0.7771756512889494, pvalue=9.284316153054097e-48), 'nsamples': 230}, 'postediting': {'pearson': (0.8062280074637963, 4.3039328643622865e-57), 'spearman': SpearmanrResult(correlation=0.8399125031710475, pvalue=3.567092603866178e-66), 'nsamples': 244}, 'question-question': {'pearson': (0.33391115160413, 7.776502946586766e-07), 'spearman': SpearmanrResult(correlation=0.3298970644326125, pvalue=1.0721346647484549e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6237569879193476, 'wmean': 0.6302458416445623}, 'spearman': {'mean': 0.635668788548649, 'wmean': 0.6426378496232787}}}, 'MR': {'devacc': 78.76, 'acc': 78.01, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 83.81, 'acc': 82.62, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.07, 'acc': 88.14, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.89, 'acc': 94.27, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.37, 'acc': 83.42, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.23, 'acc': 42.67, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 84.54, 'acc': 93.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.63, 'acc': 70.26, 'f1': 76.35, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.4, 'acc': 78.16, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7878668978345402, 'pearson': 0.8056425074402782, 'spearman': 0.7394980348519594, 'mse': 0.35792682498059625, 'yhat': array([3.38725744, 3.85517148, 1.81030706, ..., 3.17998841, 4.39484073,
       4.4519012 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7271555770577297, 'pearson': 0.668318077803957, 'spearman': 0.6647568027610277, 'mse': 1.4627233652334592, 'yhat': array([1.62377271, 1.71060255, 2.2759178 , ..., 3.95847193, 3.93101264,
       3.52923849]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 89.77, 'acc': 89.57, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 65.55, 'acc': 65.84, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 38.46, 'acc': 38.56, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.51, 'acc': 73.97, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.1, 'acc': 89.45, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.86, 'acc': 88.94, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.46, 'acc': 86.57, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.66, 'acc': 84.48, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.02, 'acc': 64.61, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.94, 'acc': 69.28, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 01:19:20,157 : ********************************************************************************
2019-02-13 01:19:20,157 : ********************************************************************************
2019-02-13 01:19:20,157 : ********************************************************************************
2019-02-13 01:19:20,157 : layer 8
2019-02-13 01:19:20,158 : ********************************************************************************
2019-02-13 01:19:20,158 : ********************************************************************************
2019-02-13 01:19:20,158 : ********************************************************************************
2019-02-13 01:19:20,246 : ***** Transfer task : STS12 *****


2019-02-13 01:19:20,259 : loading BERT mode bert-base-uncased
2019-02-13 01:19:20,259 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:19:20,276 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:19:20,276 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_amxfc0t
2019-02-13 01:19:22,708 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:19:27,125 : MSRpar : pearson = 0.4010, spearman = 0.4245
2019-02-13 01:19:29,234 : MSRvid : pearson = 0.4220, spearman = 0.4449
2019-02-13 01:19:30,786 : SMTeuroparl : pearson = 0.4798, spearman = 0.5882
2019-02-13 01:19:33,229 : surprise.OnWN : pearson = 0.6257, spearman = 0.6279
2019-02-13 01:19:34,606 : surprise.SMTnews : pearson = 0.6661, spearman = 0.5625
2019-02-13 01:19:34,606 : ALL (weighted average) : Pearson = 0.5060,             Spearman = 0.5204
2019-02-13 01:19:34,606 : ALL (average) : Pearson = 0.5189,             Spearman = 0.5296

2019-02-13 01:19:34,606 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 01:19:34,616 : loading BERT mode bert-base-uncased
2019-02-13 01:19:34,616 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:19:34,634 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:19:34,634 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv6xer_pv
2019-02-13 01:19:37,064 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:19:39,469 : FNWN : pearson = 0.3227, spearman = 0.3535
2019-02-13 01:19:41,733 : headlines : pearson = 0.6106, spearman = 0.5848
2019-02-13 01:19:43,571 : OnWN : pearson = 0.4210, spearman = 0.4402
2019-02-13 01:19:43,571 : ALL (weighted average) : Pearson = 0.5034,             Spearman = 0.5016
2019-02-13 01:19:43,571 : ALL (average) : Pearson = 0.4515,             Spearman = 0.4595

2019-02-13 01:19:43,571 : ***** Transfer task : STS14 *****


2019-02-13 01:19:43,589 : loading BERT mode bert-base-uncased
2019-02-13 01:19:43,589 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:19:43,607 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:19:43,607 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpayq6eoz0
2019-02-13 01:19:46,033 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:19:49,073 : deft-forum : pearson = 0.2863, spearman = 0.3008
2019-02-13 01:19:50,357 : deft-news : pearson = 0.7435, spearman = 0.7101
2019-02-13 01:19:52,090 : headlines : pearson = 0.5712, spearman = 0.5276
2019-02-13 01:19:53,887 : images : pearson = 0.5312, spearman = 0.5192
2019-02-13 01:19:55,800 : OnWN : pearson = 0.6150, spearman = 0.6463
2019-02-13 01:19:58,091 : tweet-news : pearson = 0.6725, spearman = 0.6075
2019-02-13 01:19:58,092 : ALL (weighted average) : Pearson = 0.5718,             Spearman = 0.5530
2019-02-13 01:19:58,092 : ALL (average) : Pearson = 0.5699,             Spearman = 0.5519

2019-02-13 01:19:58,092 : ***** Transfer task : STS15 *****


2019-02-13 01:19:58,128 : loading BERT mode bert-base-uncased
2019-02-13 01:19:58,128 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:19:58,146 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:19:58,146 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8dmivucj
2019-02-13 01:20:00,594 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:20:03,278 : answers-forums : pearson = 0.5626, spearman = 0.5576
2019-02-13 01:20:05,048 : answers-students : pearson = 0.6751, spearman = 0.6813
2019-02-13 01:20:06,353 : belief : pearson = 0.6443, spearman = 0.6727
2019-02-13 01:20:08,294 : headlines : pearson = 0.6189, spearman = 0.6123
2019-02-13 01:20:10,319 : images : pearson = 0.6555, spearman = 0.6624
2019-02-13 01:20:10,319 : ALL (weighted average) : Pearson = 0.6382,             Spearman = 0.6428
2019-02-13 01:20:10,319 : ALL (average) : Pearson = 0.6313,             Spearman = 0.6373

2019-02-13 01:20:10,319 : ***** Transfer task : STS16 *****


2019-02-13 01:20:10,358 : loading BERT mode bert-base-uncased
2019-02-13 01:20:10,358 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:20:10,377 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:20:10,377 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6nzyp7qc
2019-02-13 01:20:12,803 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:20:14,741 : answer-answer : pearson = 0.5608, spearman = 0.5761
2019-02-13 01:20:15,201 : headlines : pearson = 0.6451, spearman = 0.6454
2019-02-13 01:20:15,741 : plagiarism : pearson = 0.7616, spearman = 0.7771
2019-02-13 01:20:16,524 : postediting : pearson = 0.8046, spearman = 0.8429
2019-02-13 01:20:16,937 : question-question : pearson = 0.2971, spearman = 0.2978
2019-02-13 01:20:16,937 : ALL (weighted average) : Pearson = 0.6211,             Spearman = 0.6355
2019-02-13 01:20:16,937 : ALL (average) : Pearson = 0.6138,             Spearman = 0.6279

2019-02-13 01:20:16,937 : ***** Transfer task : MR *****


2019-02-13 01:20:16,954 : loading BERT mode bert-base-uncased
2019-02-13 01:20:16,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:20:17,011 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:20:17,011 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk7v1uv79
2019-02-13 01:20:19,442 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:20:20,887 : Generating sentence embeddings
2019-02-13 01:20:37,778 : Generated sentence embeddings
2019-02-13 01:20:37,778 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:20:52,603 : Best param found at split 1: l2reg = 0.001                 with score 79.2
2019-02-13 01:21:12,785 : Best param found at split 2: l2reg = 0.01                 with score 79.6
2019-02-13 01:21:38,386 : Best param found at split 3: l2reg = 0.01                 with score 80.14
2019-02-13 01:22:17,049 : Best param found at split 4: l2reg = 0.01                 with score 79.94
2019-02-13 01:23:00,267 : Best param found at split 5: l2reg = 0.001                 with score 79.89
2019-02-13 01:23:02,581 : Dev acc : 79.75 Test acc : 79.34

2019-02-13 01:23:02,582 : ***** Transfer task : CR *****


2019-02-13 01:23:02,590 : loading BERT mode bert-base-uncased
2019-02-13 01:23:02,590 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:23:02,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:23:02,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4tgk6hg6
2019-02-13 01:23:05,053 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:23:06,698 : Generating sentence embeddings
2019-02-13 01:23:15,206 : Generated sentence embeddings
2019-02-13 01:23:15,207 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:23:24,943 : Best param found at split 1: l2reg = 0.001                 with score 85.62
2019-02-13 01:23:34,098 : Best param found at split 2: l2reg = 0.001                 with score 85.06
2019-02-13 01:23:43,480 : Best param found at split 3: l2reg = 0.001                 with score 85.83
2019-02-13 01:23:53,575 : Best param found at split 4: l2reg = 0.001                 with score 85.2
2019-02-13 01:24:01,565 : Best param found at split 5: l2reg = 1e-05                 with score 84.94
2019-02-13 01:24:01,909 : Dev acc : 85.33 Test acc : 84.4

2019-02-13 01:24:01,909 : ***** Transfer task : MPQA *****


2019-02-13 01:24:01,945 : loading BERT mode bert-base-uncased
2019-02-13 01:24:01,945 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:24:01,964 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:24:01,964 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1wasl8el
2019-02-13 01:24:04,402 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:24:05,830 : Generating sentence embeddings
2019-02-13 01:24:17,234 : Generated sentence embeddings
2019-02-13 01:24:17,234 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:24:34,352 : Best param found at split 1: l2reg = 0.01                 with score 88.74
2019-02-13 01:24:50,456 : Best param found at split 2: l2reg = 0.01                 with score 88.6
2019-02-13 01:25:07,357 : Best param found at split 3: l2reg = 0.001                 with score 88.41
2019-02-13 01:25:43,609 : Best param found at split 4: l2reg = 0.001                 with score 89.26
2019-02-13 01:26:21,108 : Best param found at split 5: l2reg = 0.01                 with score 88.66
2019-02-13 01:26:23,061 : Dev acc : 88.73 Test acc : 88.73

2019-02-13 01:26:23,062 : ***** Transfer task : SUBJ *****


2019-02-13 01:26:23,079 : loading BERT mode bert-base-uncased
2019-02-13 01:26:23,079 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:26:23,131 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:26:23,131 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj18kwyeb
2019-02-13 01:26:25,570 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:26:27,137 : Generating sentence embeddings
2019-02-13 01:26:54,084 : Generated sentence embeddings
2019-02-13 01:26:54,085 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 01:27:29,498 : Best param found at split 1: l2reg = 0.001                 with score 94.89
2019-02-13 01:27:59,852 : Best param found at split 2: l2reg = 0.001                 with score 95.18
2019-02-13 01:28:13,962 : Best param found at split 3: l2reg = 1e-05                 with score 94.65
2019-02-13 01:28:29,093 : Best param found at split 4: l2reg = 0.001                 with score 95.52
2019-02-13 01:28:43,805 : Best param found at split 5: l2reg = 0.001                 with score 95.11
2019-02-13 01:28:44,702 : Dev acc : 95.07 Test acc : 94.68

2019-02-13 01:28:44,703 : ***** Transfer task : SST Binary classification *****


2019-02-13 01:28:44,844 : loading BERT mode bert-base-uncased
2019-02-13 01:28:44,844 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:28:44,866 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:28:44,867 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8uimyc14
2019-02-13 01:28:47,310 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:28:48,716 : Computing embedding for train
2019-02-13 01:30:20,156 : Computed train embeddings
2019-02-13 01:30:20,156 : Computing embedding for dev
2019-02-13 01:30:22,042 : Computed dev embeddings
2019-02-13 01:30:22,043 : Computing embedding for test
2019-02-13 01:30:26,115 : Computed test embeddings
2019-02-13 01:30:26,115 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:31:41,164 : [('reg:1e-05', 84.06), ('reg:0.0001', 84.17), ('reg:0.001', 84.4), ('reg:0.01', 83.72)]
2019-02-13 01:31:41,164 : Validation : best param found is reg = 0.001 with score             84.4
2019-02-13 01:31:41,164 : Evaluating...
2019-02-13 01:32:00,641 : 
Dev acc : 84.4 Test acc : 84.84 for             SST Binary classification

2019-02-13 01:32:00,646 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 01:32:00,696 : loading BERT mode bert-base-uncased
2019-02-13 01:32:00,696 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:32:00,718 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:32:00,718 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8wm7lv9e
2019-02-13 01:32:03,157 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:32:04,650 : Computing embedding for train
2019-02-13 01:32:21,132 : Computed train embeddings
2019-02-13 01:32:21,132 : Computing embedding for dev
2019-02-13 01:32:23,058 : Computed dev embeddings
2019-02-13 01:32:23,058 : Computing embedding for test
2019-02-13 01:32:27,088 : Computed test embeddings
2019-02-13 01:32:27,088 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:32:32,186 : [('reg:1e-05', 42.23), ('reg:0.0001', 42.23), ('reg:0.001', 42.33), ('reg:0.01', 42.51)]
2019-02-13 01:32:32,187 : Validation : best param found is reg = 0.01 with score             42.51
2019-02-13 01:32:32,187 : Evaluating...
2019-02-13 01:32:33,269 : 
Dev acc : 42.51 Test acc : 43.62 for             SST Fine-Grained classification

2019-02-13 01:32:33,269 : ***** Transfer task : TREC *****


2019-02-13 01:32:33,282 : loading BERT mode bert-base-uncased
2019-02-13 01:32:33,282 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:32:33,302 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:32:33,302 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2ufwu0z5
2019-02-13 01:32:35,737 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:32:42,347 : Computed train embeddings
2019-02-13 01:32:42,771 : Computed test embeddings
2019-02-13 01:32:42,771 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 01:32:51,319 : [('reg:1e-05', 85.4), ('reg:0.0001', 85.38), ('reg:0.001', 84.39), ('reg:0.01', 79.16)]
2019-02-13 01:32:51,319 : Cross-validation : best param found is reg = 1e-05             with score 85.4
2019-02-13 01:32:51,319 : Evaluating...
2019-02-13 01:32:51,695 : 
Dev acc : 85.4 Test acc : 92.0             for TREC

2019-02-13 01:32:51,695 : ***** Transfer task : MRPC *****


2019-02-13 01:32:51,716 : loading BERT mode bert-base-uncased
2019-02-13 01:32:51,717 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:32:51,738 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:32:51,738 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuui0sbop
2019-02-13 01:32:54,176 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:32:55,660 : Computing embedding for train
2019-02-13 01:33:08,088 : Computed train embeddings
2019-02-13 01:33:08,088 : Computing embedding for test
2019-02-13 01:33:13,443 : Computed test embeddings
2019-02-13 01:33:13,460 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 01:33:20,734 : [('reg:1e-05', 75.12), ('reg:0.0001', 75.39), ('reg:0.001', 75.42), ('reg:0.01', 73.23)]
2019-02-13 01:33:20,734 : Cross-validation : best param found is reg = 0.001             with score 75.42
2019-02-13 01:33:20,734 : Evaluating...
2019-02-13 01:33:21,071 : Dev acc : 75.42 Test acc 73.39; Test F1 79.79 for MRPC.

2019-02-13 01:33:21,071 : ***** Transfer task : SICK-Entailment*****


2019-02-13 01:33:21,095 : loading BERT mode bert-base-uncased
2019-02-13 01:33:21,096 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:33:21,115 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:33:21,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5exjug4x
2019-02-13 01:33:23,558 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:33:25,022 : Computing embedding for train
2019-02-13 01:33:35,802 : Computed train embeddings
2019-02-13 01:33:35,802 : Computing embedding for dev
2019-02-13 01:33:37,236 : Computed dev embeddings
2019-02-13 01:33:37,236 : Computing embedding for test
2019-02-13 01:33:51,417 : Computed test embeddings
2019-02-13 01:33:51,444 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:33:55,488 : [('reg:1e-05', 79.4), ('reg:0.0001', 78.0), ('reg:0.001', 79.6), ('reg:0.01', 77.2)]
2019-02-13 01:33:55,488 : Validation : best param found is reg = 0.001 with score             79.6
2019-02-13 01:33:55,488 : Evaluating...
2019-02-13 01:33:56,569 : 
Dev acc : 79.6 Test acc : 77.49 for                        SICK entailment

2019-02-13 01:33:56,569 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 01:33:56,638 : loading BERT mode bert-base-uncased
2019-02-13 01:33:56,638 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:33:56,658 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:33:56,658 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx8_w4rvd
2019-02-13 01:33:59,095 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:34:00,636 : Computing embedding for train
2019-02-13 01:34:15,120 : Computed train embeddings
2019-02-13 01:34:15,120 : Computing embedding for dev
2019-02-13 01:34:16,871 : Computed dev embeddings
2019-02-13 01:34:16,872 : Computing embedding for test
2019-02-13 01:34:33,739 : Computed test embeddings
2019-02-13 01:35:45,619 : Dev : Pearson 0.8049233398271021
2019-02-13 01:35:45,619 : Test : Pearson 0.8083412013242576 Spearman 0.743372157010972 MSE 0.35310002413158736                        for SICK Relatedness

2019-02-13 01:35:45,620 : 

***** Transfer task : STSBenchmark*****


2019-02-13 01:35:45,659 : loading BERT mode bert-base-uncased
2019-02-13 01:35:45,660 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:35:45,737 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:35:45,737 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr4q4qg2x
2019-02-13 01:35:48,196 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:35:49,772 : Computing embedding for train
2019-02-13 01:36:13,863 : Computed train embeddings
2019-02-13 01:36:13,863 : Computing embedding for dev
2019-02-13 01:36:19,443 : Computed dev embeddings
2019-02-13 01:36:19,444 : Computing embedding for test
2019-02-13 01:36:24,969 : Computed test embeddings
2019-02-13 01:37:10,850 : Dev : Pearson 0.724498551024837
2019-02-13 01:37:10,850 : Test : Pearson 0.6705213744602287 Spearman 0.6645309038555148 MSE 1.448943953210956                        for SICK Relatedness

2019-02-13 01:37:10,850 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 01:37:11,122 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 01:37:11,132 : loading BERT mode bert-base-uncased
2019-02-13 01:37:11,132 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:37:11,159 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:37:11,159 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnq37szrx
2019-02-13 01:37:13,617 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:37:15,054 : Computing embeddings for train/dev/test
2019-02-13 01:39:53,387 : Computed embeddings
2019-02-13 01:39:53,387 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:41:00,898 : [('reg:1e-05', 87.73), ('reg:0.0001', 86.91), ('reg:0.001', 80.81), ('reg:0.01', 69.73)]
2019-02-13 01:41:00,898 : Validation : best param found is reg = 1e-05 with score             87.73
2019-02-13 01:41:00,898 : Evaluating...
2019-02-13 01:41:09,422 : 
Dev acc : 87.7 Test acc : 86.8 for LENGTH classification

2019-02-13 01:41:09,430 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 01:41:09,792 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 01:41:09,836 : loading BERT mode bert-base-uncased
2019-02-13 01:41:09,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:41:09,866 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:41:09,867 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn2ima3au
2019-02-13 01:41:12,300 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:41:13,721 : Computing embeddings for train/dev/test
2019-02-13 01:43:34,918 : Computed embeddings
2019-02-13 01:43:34,918 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:44:52,070 : [('reg:1e-05', 62.19), ('reg:0.0001', 38.04), ('reg:0.001', 4.79), ('reg:0.01', 0.97)]
2019-02-13 01:44:52,070 : Validation : best param found is reg = 1e-05 with score             62.19
2019-02-13 01:44:52,070 : Evaluating...
2019-02-13 01:45:04,746 : 
Dev acc : 62.2 Test acc : 61.9 for WORDCONTENT classification

2019-02-13 01:45:04,754 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 01:45:05,105 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 01:45:05,170 : loading BERT mode bert-base-uncased
2019-02-13 01:45:05,170 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:45:05,197 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:45:05,198 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmiv2c34t
2019-02-13 01:45:07,683 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:45:09,091 : Computing embeddings for train/dev/test
2019-02-13 01:47:22,760 : Computed embeddings
2019-02-13 01:47:22,760 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:48:12,455 : [('reg:1e-05', 37.92), ('reg:0.0001', 37.67), ('reg:0.001', 35.96), ('reg:0.01', 29.84)]
2019-02-13 01:48:12,456 : Validation : best param found is reg = 1e-05 with score             37.92
2019-02-13 01:48:12,456 : Evaluating...
2019-02-13 01:48:28,534 : 
Dev acc : 37.9 Test acc : 37.8 for DEPTH classification

2019-02-13 01:48:28,542 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 01:48:28,936 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 01:48:28,999 : loading BERT mode bert-base-uncased
2019-02-13 01:48:28,999 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:48:29,025 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:48:29,026 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpni5q0w7r
2019-02-13 01:48:31,461 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:48:32,984 : Computing embeddings for train/dev/test
2019-02-13 01:51:09,363 : Computed embeddings
2019-02-13 01:51:09,363 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:52:02,139 : [('reg:1e-05', 74.49), ('reg:0.0001', 73.33), ('reg:0.001', 68.72), ('reg:0.01', 59.0)]
2019-02-13 01:52:02,139 : Validation : best param found is reg = 1e-05 with score             74.49
2019-02-13 01:52:02,139 : Evaluating...
2019-02-13 01:52:17,349 : 
Dev acc : 74.5 Test acc : 73.7 for TOPCONSTITUENTS classification

2019-02-13 01:52:17,357 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 01:52:17,716 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 01:52:17,781 : loading BERT mode bert-base-uncased
2019-02-13 01:52:17,782 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:52:17,905 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:52:17,905 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn8y2ibbf
2019-02-13 01:52:20,356 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:52:21,853 : Computing embeddings for train/dev/test
2019-02-13 01:55:17,759 : Computed embeddings
2019-02-13 01:55:17,759 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 01:56:23,515 : [('reg:1e-05', 90.58), ('reg:0.0001', 90.6), ('reg:0.001', 90.41), ('reg:0.01', 89.23)]
2019-02-13 01:56:23,515 : Validation : best param found is reg = 0.0001 with score             90.6
2019-02-13 01:56:23,515 : Evaluating...
2019-02-13 01:56:51,529 : 
Dev acc : 90.6 Test acc : 90.6 for BIGRAMSHIFT classification

2019-02-13 01:56:51,537 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 01:56:51,920 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 01:56:51,987 : loading BERT mode bert-base-uncased
2019-02-13 01:56:51,987 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 01:56:52,107 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 01:56:52,108 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp0vvazxj
2019-02-13 01:56:54,540 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 01:56:56,121 : Computing embeddings for train/dev/test
2019-02-13 02:00:06,617 : Computed embeddings
2019-02-13 02:00:06,617 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:00:52,261 : [('reg:1e-05', 89.84), ('reg:0.0001', 89.78), ('reg:0.001', 89.9), ('reg:0.01', 89.83)]
2019-02-13 02:00:52,261 : Validation : best param found is reg = 0.001 with score             89.9
2019-02-13 02:00:52,261 : Evaluating...
2019-02-13 02:01:15,582 : 
Dev acc : 89.9 Test acc : 89.4 for TENSE classification

2019-02-13 02:01:15,590 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 02:01:16,163 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 02:01:16,225 : loading BERT mode bert-base-uncased
2019-02-13 02:01:16,225 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:01:16,253 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:01:16,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi_8_qtku
2019-02-13 02:01:18,708 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:01:20,181 : Computing embeddings for train/dev/test
2019-02-13 02:04:07,544 : Computed embeddings
2019-02-13 02:04:07,545 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:04:43,929 : [('reg:1e-05', 86.18), ('reg:0.0001', 86.29), ('reg:0.001', 86.02), ('reg:0.01', 84.31)]
2019-02-13 02:04:43,929 : Validation : best param found is reg = 0.0001 with score             86.29
2019-02-13 02:04:43,929 : Evaluating...
2019-02-13 02:04:50,915 : 
Dev acc : 86.3 Test acc : 85.2 for SUBJNUMBER classification

2019-02-13 02:04:50,923 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 02:04:51,355 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 02:04:51,423 : loading BERT mode bert-base-uncased
2019-02-13 02:04:51,423 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:04:51,454 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:04:51,454 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsksx9ooc
2019-02-13 02:04:53,905 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:04:55,392 : Computing embeddings for train/dev/test
2019-02-13 02:07:41,682 : Computed embeddings
2019-02-13 02:07:41,682 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:08:31,118 : [('reg:1e-05', 83.31), ('reg:0.0001', 83.32), ('reg:0.001', 83.18), ('reg:0.01', 82.44)]
2019-02-13 02:08:31,118 : Validation : best param found is reg = 0.0001 with score             83.32
2019-02-13 02:08:31,118 : Evaluating...
2019-02-13 02:08:47,494 : 
Dev acc : 83.3 Test acc : 83.8 for OBJNUMBER classification

2019-02-13 02:08:47,502 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 02:08:47,910 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 02:08:47,979 : loading BERT mode bert-base-uncased
2019-02-13 02:08:47,980 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:08:48,006 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:08:48,007 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprlgh0efo
2019-02-13 02:08:50,442 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:08:51,906 : Computing embeddings for train/dev/test
2019-02-13 02:11:42,390 : Computed embeddings
2019-02-13 02:11:42,390 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:12:48,493 : [('reg:1e-05', 65.4), ('reg:0.0001', 65.34), ('reg:0.001', 65.5), ('reg:0.01', 65.09)]
2019-02-13 02:12:48,493 : Validation : best param found is reg = 0.001 with score             65.5
2019-02-13 02:12:48,493 : Evaluating...
2019-02-13 02:13:00,213 : 
Dev acc : 65.5 Test acc : 65.3 for ODDMANOUT classification

2019-02-13 02:13:00,221 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 02:13:00,627 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 02:13:00,702 : loading BERT mode bert-base-uncased
2019-02-13 02:13:00,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:13:00,829 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:13:00,829 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4r4ue7ob
2019-02-13 02:13:03,257 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:13:04,711 : Computing embeddings for train/dev/test
2019-02-13 02:16:16,639 : Computed embeddings
2019-02-13 02:16:16,640 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:17:01,941 : [('reg:1e-05', 71.79), ('reg:0.0001', 71.77), ('reg:0.001', 71.39), ('reg:0.01', 67.01)]
2019-02-13 02:17:01,941 : Validation : best param found is reg = 1e-05 with score             71.79
2019-02-13 02:17:01,941 : Evaluating...
2019-02-13 02:17:16,044 : 
Dev acc : 71.8 Test acc : 71.4 for COORDINATIONINVERSION classification

2019-02-13 02:17:16,053 : {'STS12': {'MSRpar': {'pearson': (0.4009581636643262, 2.4590128084466852e-30), 'spearman': SpearmanrResult(correlation=0.42454353270071554, pvalue=3.578606093766433e-34), 'nsamples': 750}, 'MSRvid': {'pearson': (0.422023412402888, 9.515479295839915e-34), 'spearman': SpearmanrResult(correlation=0.44485743218310975, pvalue=9.911738920172136e-38), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47982453914212286, 8.333612480326332e-28), 'spearman': SpearmanrResult(correlation=0.5881634748078525, pvalue=4.673692536589134e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6256901734364018, 9.642702212486319e-83), 'spearman': SpearmanrResult(correlation=0.6279365365625404, pvalue=1.6952135736947276e-83), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.666138511801448, 1.6402961048214583e-52), 'spearman': SpearmanrResult(correlation=0.5625196694579535, pvalue=1.1335533654184154e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5189269600894374, 'wmean': 0.5059628512878777}, 'spearman': {'mean': 0.5296041291424344, 'wmean': 0.5204039604682439}}}, 'STS13': {'FNWN': {'pearson': (0.32274857953926, 5.914289229748303e-06), 'spearman': SpearmanrResult(correlation=0.3534635814811164, pvalue=6.06680383769636e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6106439889285629, 7.685216926350378e-78), 'spearman': SpearmanrResult(correlation=0.5847961372190434, pvalue=5.206473459568935e-70), 'nsamples': 750}, 'OnWN': {'pearson': (0.42097769189942813, 1.6662203218310701e-25), 'spearman': SpearmanrResult(correlation=0.44023041577508676, pvalue=5.389312450970203e-28), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4514567534557503, 'wmean': 0.5034339722566143}, 'spearman': {'mean': 0.4594967114917488, 'wmean': 0.5015806553760249}}}, 'STS14': {'deft-forum': {'pearson': (0.2862534538602421, 6.195885331865943e-10), 'spearman': SpearmanrResult(correlation=0.3007672448253487, pvalue=7.321440230046186e-11), 'nsamples': 450}, 'deft-news': {'pearson': (0.7434526599166917, 5.347172540593258e-54), 'spearman': SpearmanrResult(correlation=0.7100594995545526, pvalue=2.5969067648197485e-47), 'nsamples': 300}, 'headlines': {'pearson': (0.571222603431986, 3.5849416081480236e-66), 'spearman': SpearmanrResult(correlation=0.5276114032429411, pvalue=5.6213502432295096e-55), 'nsamples': 750}, 'images': {'pearson': (0.5312260295340473, 7.6409660215855e-56), 'spearman': SpearmanrResult(correlation=0.5192375737315853, pvalue=5.2233135357569645e-53), 'nsamples': 750}, 'OnWN': {'pearson': (0.6149819995042345, 3.1597755811911554e-79), 'spearman': SpearmanrResult(correlation=0.6463494301526064, pvalue=6.304047875518373e-90), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6724796365758339, 7.456704694649256e-100), 'spearman': SpearmanrResult(correlation=0.6075222407809413, pvalue=7.410392453510771e-77), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5699360638038393, 'wmean': 0.5718086810657846}, 'spearman': {'mean': 0.5519245653813292, 'wmean': 0.5530409589250208}}}, 'STS15': {'answers-forums': {'pearson': (0.5625592878763502, 1.1094267945420228e-32), 'spearman': SpearmanrResult(correlation=0.5575985718078277, pvalue=5.066197508413609e-32), 'nsamples': 375}, 'answers-students': {'pearson': (0.6751364674029731, 6.392874728171972e-101), 'spearman': SpearmanrResult(correlation=0.6812584812357168, pvalue=2.01472230267085e-103), 'nsamples': 750}, 'belief': {'pearson': (0.6442897318844597, 2.3149242058874142e-45), 'spearman': SpearmanrResult(correlation=0.6727478752294631, pvalue=9.592237469746419e-51), 'nsamples': 375}, 'headlines': {'pearson': (0.6188911560553052, 1.706237373529436e-80), 'spearman': SpearmanrResult(correlation=0.6122691801278722, pvalue=2.3384434964134333e-78), 'nsamples': 750}, 'images': {'pearson': (0.6555384140861834, 2.6389002906605188e-93), 'spearman': SpearmanrResult(correlation=0.6624221408579154, pvalue=6.487575535632345e-96), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6312830114610544, 'wmean': 0.6382476368562167}, 'spearman': {'mean': 0.6372592498517591, 'wmean': 0.6427807564350374}}}, 'STS16': {'answer-answer': {'pearson': (0.5608342039560309, 1.9163278699692928e-22), 'spearman': SpearmanrResult(correlation=0.5761307199663898, pvalue=7.324159334007489e-24), 'nsamples': 254}, 'headlines': {'pearson': (0.6451171842168822, 1.0688169373808515e-30), 'spearman': SpearmanrResult(correlation=0.6454226918105143, pvalue=9.828067409659457e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7616263325031424, 7.603511990816543e-45), 'spearman': SpearmanrResult(correlation=0.7771130284363965, pvalue=9.548872332720755e-48), 'nsamples': 230}, 'postediting': {'pearson': (0.8045524480610218, 1.0925313413815377e-56), 'spearman': SpearmanrResult(correlation=0.8429261884780218, pvalue=4.3460087601746527e-67), 'nsamples': 244}, 'question-question': {'pearson': (0.29711499711465517, 1.2502228685099508e-05), 'spearman': SpearmanrResult(correlation=0.2978320227508343, pvalue=1.1884926212787783e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6138490331703466, 'wmean': 0.6211365555433478}, 'spearman': {'mean': 0.6278849302884313, 'wmean': 0.6355011234538059}}}, 'MR': {'devacc': 79.75, 'acc': 79.34, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.33, 'acc': 84.4, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.73, 'acc': 88.73, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.07, 'acc': 94.68, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.4, 'acc': 84.84, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.51, 'acc': 43.62, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 85.4, 'acc': 92.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 75.42, 'acc': 73.39, 'f1': 79.79, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.6, 'acc': 77.49, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8049233398271021, 'pearson': 0.8083412013242576, 'spearman': 0.743372157010972, 'mse': 0.35310002413158736, 'yhat': array([2.65290246, 3.81894881, 1.66134218, ..., 3.21806341, 4.4088747 ,
       4.42255518]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.724498551024837, 'pearson': 0.6705213744602287, 'spearman': 0.6645309038555148, 'mse': 1.448943953210956, 'yhat': array([1.46583604, 1.70530432, 2.3650643 , ..., 3.89356629, 3.83307051,
       3.62722068]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 87.73, 'acc': 86.81, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 62.19, 'acc': 61.94, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 37.92, 'acc': 37.83, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.49, 'acc': 73.72, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.6, 'acc': 90.56, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.9, 'acc': 89.39, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.29, 'acc': 85.2, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.32, 'acc': 83.78, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.5, 'acc': 65.27, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.79, 'acc': 71.37, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 02:17:16,053 : ********************************************************************************
2019-02-13 02:17:16,053 : ********************************************************************************
2019-02-13 02:17:16,053 : ********************************************************************************
2019-02-13 02:17:16,053 : layer 9
2019-02-13 02:17:16,053 : ********************************************************************************
2019-02-13 02:17:16,053 : ********************************************************************************
2019-02-13 02:17:16,053 : ********************************************************************************
2019-02-13 02:17:16,140 : ***** Transfer task : STS12 *****


2019-02-13 02:17:16,153 : loading BERT mode bert-base-uncased
2019-02-13 02:17:16,153 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:17:16,169 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:17:16,169 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpch8wml1w
2019-02-13 02:17:18,603 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:17:22,518 : MSRpar : pearson = 0.3828, spearman = 0.4113
2019-02-13 02:17:24,111 : MSRvid : pearson = 0.3291, spearman = 0.3695
2019-02-13 02:17:25,046 : SMTeuroparl : pearson = 0.4882, spearman = 0.5826
2019-02-13 02:17:26,700 : surprise.OnWN : pearson = 0.5922, spearman = 0.5924
2019-02-13 02:17:27,593 : surprise.SMTnews : pearson = 0.6180, spearman = 0.5632
2019-02-13 02:17:27,593 : ALL (weighted average) : Pearson = 0.4661,             Spearman = 0.4897
2019-02-13 02:17:27,593 : ALL (average) : Pearson = 0.4821,             Spearman = 0.5038

2019-02-13 02:17:27,593 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 02:17:27,603 : loading BERT mode bert-base-uncased
2019-02-13 02:17:27,603 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:17:27,621 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:17:27,622 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpranmmapf
2019-02-13 02:17:30,089 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:17:32,791 : FNWN : pearson = 0.3203, spearman = 0.3505
2019-02-13 02:17:35,598 : headlines : pearson = 0.5885, spearman = 0.5582
2019-02-13 02:17:37,308 : OnWN : pearson = 0.4330, spearman = 0.4489
2019-02-13 02:17:37,309 : ALL (weighted average) : Pearson = 0.4965,             Spearman = 0.4912
2019-02-13 02:17:37,309 : ALL (average) : Pearson = 0.4473,             Spearman = 0.4525

2019-02-13 02:17:37,309 : ***** Transfer task : STS14 *****


2019-02-13 02:17:37,323 : loading BERT mode bert-base-uncased
2019-02-13 02:17:37,324 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:17:37,341 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:17:37,341 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf1ry28vy
2019-02-13 02:17:39,776 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:17:43,201 : deft-forum : pearson = 0.2593, spearman = 0.2632
2019-02-13 02:17:44,552 : deft-news : pearson = 0.7287, spearman = 0.6962
2019-02-13 02:17:47,442 : headlines : pearson = 0.5537, spearman = 0.5104
2019-02-13 02:17:50,179 : images : pearson = 0.4282, spearman = 0.4384
2019-02-13 02:17:52,988 : OnWN : pearson = 0.6203, spearman = 0.6486
2019-02-13 02:17:55,473 : tweet-news : pearson = 0.6670, spearman = 0.5889
2019-02-13 02:17:55,473 : ALL (weighted average) : Pearson = 0.5432,             Spearman = 0.5245
2019-02-13 02:17:55,473 : ALL (average) : Pearson = 0.5429,             Spearman = 0.5243

2019-02-13 02:17:55,473 : ***** Transfer task : STS15 *****


2019-02-13 02:17:55,537 : loading BERT mode bert-base-uncased
2019-02-13 02:17:55,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:17:55,555 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:17:55,555 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxxctib8k
2019-02-13 02:17:57,986 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:18:00,876 : answers-forums : pearson = 0.5607, spearman = 0.5609
2019-02-13 02:18:02,930 : answers-students : pearson = 0.6554, spearman = 0.6646
2019-02-13 02:18:04,342 : belief : pearson = 0.6660, spearman = 0.6872
2019-02-13 02:18:06,505 : headlines : pearson = 0.6003, spearman = 0.5940
2019-02-13 02:18:08,646 : images : pearson = 0.6202, spearman = 0.6277
2019-02-13 02:18:08,647 : ALL (weighted average) : Pearson = 0.6223,             Spearman = 0.6276
2019-02-13 02:18:08,647 : ALL (average) : Pearson = 0.6205,             Spearman = 0.6269

2019-02-13 02:18:08,647 : ***** Transfer task : STS16 *****


2019-02-13 02:18:08,717 : loading BERT mode bert-base-uncased
2019-02-13 02:18:08,717 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:18:08,735 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:18:08,735 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgm_esrk7
2019-02-13 02:18:11,170 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:18:13,358 : answer-answer : pearson = 0.5332, spearman = 0.5294
2019-02-13 02:18:14,097 : headlines : pearson = 0.6373, spearman = 0.6453
2019-02-13 02:18:14,880 : plagiarism : pearson = 0.7408, spearman = 0.7547
2019-02-13 02:18:15,911 : postediting : pearson = 0.7986, spearman = 0.8279
2019-02-13 02:18:16,591 : question-question : pearson = 0.2776, spearman = 0.2775
2019-02-13 02:18:16,591 : ALL (weighted average) : Pearson = 0.6049,             Spearman = 0.6145
2019-02-13 02:18:16,591 : ALL (average) : Pearson = 0.5975,             Spearman = 0.6070

2019-02-13 02:18:16,591 : ***** Transfer task : MR *****


2019-02-13 02:18:16,608 : loading BERT mode bert-base-uncased
2019-02-13 02:18:16,608 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:18:16,628 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:18:16,629 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl05bil1x
2019-02-13 02:18:19,062 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:18:20,570 : Generating sentence embeddings
2019-02-13 02:18:42,798 : Generated sentence embeddings
2019-02-13 02:18:42,798 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:19:06,838 : Best param found at split 1: l2reg = 1e-05                 with score 80.96
2019-02-13 02:19:32,764 : Best param found at split 2: l2reg = 0.001                 with score 81.03
2019-02-13 02:19:55,072 : Best param found at split 3: l2reg = 0.001                 with score 81.49
2019-02-13 02:20:19,840 : Best param found at split 4: l2reg = 0.001                 with score 81.09
2019-02-13 02:20:51,694 : Best param found at split 5: l2reg = 0.0001                 with score 80.97
2019-02-13 02:20:53,779 : Dev acc : 81.11 Test acc : 81.05

2019-02-13 02:20:53,780 : ***** Transfer task : CR *****


2019-02-13 02:20:53,788 : loading BERT mode bert-base-uncased
2019-02-13 02:20:53,788 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:20:53,807 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:20:53,807 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9tluht7k
2019-02-13 02:20:56,246 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:20:57,841 : Generating sentence embeddings
2019-02-13 02:21:05,977 : Generated sentence embeddings
2019-02-13 02:21:05,978 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:21:18,950 : Best param found at split 1: l2reg = 0.01                 with score 87.05
2019-02-13 02:21:31,871 : Best param found at split 2: l2reg = 0.01                 with score 87.12
2019-02-13 02:21:40,260 : Best param found at split 3: l2reg = 1e-05                 with score 87.25
2019-02-13 02:21:49,027 : Best param found at split 4: l2reg = 0.01                 with score 87.09
2019-02-13 02:21:58,652 : Best param found at split 5: l2reg = 0.01                 with score 87.32
2019-02-13 02:21:59,114 : Dev acc : 87.17 Test acc : 87.02

2019-02-13 02:21:59,115 : ***** Transfer task : MPQA *****


2019-02-13 02:21:59,120 : loading BERT mode bert-base-uncased
2019-02-13 02:21:59,120 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:21:59,140 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:21:59,140 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpswpt010e
2019-02-13 02:22:01,574 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:22:03,118 : Generating sentence embeddings
2019-02-13 02:22:14,447 : Generated sentence embeddings
2019-02-13 02:22:14,448 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:22:40,947 : Best param found at split 1: l2reg = 0.001                 with score 88.68
2019-02-13 02:23:05,210 : Best param found at split 2: l2reg = 0.0001                 with score 88.4
2019-02-13 02:23:19,708 : Best param found at split 3: l2reg = 0.01                 with score 88.01
2019-02-13 02:23:36,348 : Best param found at split 4: l2reg = 0.001                 with score 89.07
2019-02-13 02:24:04,527 : Best param found at split 5: l2reg = 0.01                 with score 88.65
2019-02-13 02:24:05,958 : Dev acc : 88.56 Test acc : 88.64

2019-02-13 02:24:05,959 : ***** Transfer task : SUBJ *****


2019-02-13 02:24:05,974 : loading BERT mode bert-base-uncased
2019-02-13 02:24:05,974 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:24:05,995 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:24:05,995 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptr_8gcal
2019-02-13 02:24:08,471 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:24:09,989 : Generating sentence embeddings
2019-02-13 02:24:29,501 : Generated sentence embeddings
2019-02-13 02:24:29,502 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 02:24:59,141 : Best param found at split 1: l2reg = 0.001                 with score 95.35
2019-02-13 02:25:37,246 : Best param found at split 2: l2reg = 0.001                 with score 95.52
2019-02-13 02:26:05,682 : Best param found at split 3: l2reg = 1e-05                 with score 95.04
2019-02-13 02:26:33,183 : Best param found at split 4: l2reg = 0.001                 with score 95.8
2019-02-13 02:26:57,217 : Best param found at split 5: l2reg = 0.001                 with score 95.32
2019-02-13 02:26:58,091 : Dev acc : 95.41 Test acc : 94.77

2019-02-13 02:26:58,092 : ***** Transfer task : SST Binary classification *****


2019-02-13 02:26:58,219 : loading BERT mode bert-base-uncased
2019-02-13 02:26:58,219 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:26:58,242 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:26:58,242 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5hhjwcss
2019-02-13 02:27:00,671 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:27:02,157 : Computing embedding for train
2019-02-13 02:28:18,408 : Computed train embeddings
2019-02-13 02:28:18,408 : Computing embedding for dev
2019-02-13 02:28:19,811 : Computed dev embeddings
2019-02-13 02:28:19,811 : Computing embedding for test
2019-02-13 02:28:22,847 : Computed test embeddings
2019-02-13 02:28:22,847 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:29:05,610 : [('reg:1e-05', 85.78), ('reg:0.0001', 85.89), ('reg:0.001', 85.67), ('reg:0.01', 85.44)]
2019-02-13 02:29:05,611 : Validation : best param found is reg = 0.0001 with score             85.89
2019-02-13 02:29:05,611 : Evaluating...
2019-02-13 02:29:22,453 : 
Dev acc : 85.89 Test acc : 85.39 for             SST Binary classification

2019-02-13 02:29:22,458 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 02:29:22,512 : loading BERT mode bert-base-uncased
2019-02-13 02:29:22,513 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:29:22,533 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:29:22,533 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp70uojaj3
2019-02-13 02:29:24,966 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:29:26,542 : Computing embedding for train
2019-02-13 02:29:46,060 : Computed train embeddings
2019-02-13 02:29:46,060 : Computing embedding for dev
2019-02-13 02:29:48,706 : Computed dev embeddings
2019-02-13 02:29:48,706 : Computing embedding for test
2019-02-13 02:29:54,020 : Computed test embeddings
2019-02-13 02:29:54,020 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:30:03,131 : [('reg:1e-05', 44.5), ('reg:0.0001', 44.41), ('reg:0.001', 45.05), ('reg:0.01', 44.32)]
2019-02-13 02:30:03,131 : Validation : best param found is reg = 0.001 with score             45.05
2019-02-13 02:30:03,131 : Evaluating...
2019-02-13 02:30:05,485 : 
Dev acc : 45.05 Test acc : 43.53 for             SST Fine-Grained classification

2019-02-13 02:30:05,486 : ***** Transfer task : TREC *****


2019-02-13 02:30:05,498 : loading BERT mode bert-base-uncased
2019-02-13 02:30:05,498 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:30:05,518 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:30:05,518 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5hjs6k_b
2019-02-13 02:30:07,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:30:19,471 : Computed train embeddings
2019-02-13 02:30:20,327 : Computed test embeddings
2019-02-13 02:30:20,327 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 02:30:40,354 : [('reg:1e-05', 86.56), ('reg:0.0001', 86.44), ('reg:0.001', 85.95), ('reg:0.01', 80.89)]
2019-02-13 02:30:40,354 : Cross-validation : best param found is reg = 1e-05             with score 86.56
2019-02-13 02:30:40,354 : Evaluating...
2019-02-13 02:30:41,497 : 
Dev acc : 86.56 Test acc : 93.2             for TREC

2019-02-13 02:30:41,498 : ***** Transfer task : MRPC *****


2019-02-13 02:30:41,517 : loading BERT mode bert-base-uncased
2019-02-13 02:30:41,518 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:30:41,541 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:30:41,541 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmi2zytq9
2019-02-13 02:30:43,986 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:30:45,474 : Computing embedding for train
2019-02-13 02:31:01,963 : Computed train embeddings
2019-02-13 02:31:01,963 : Computing embedding for test
2019-02-13 02:31:09,219 : Computed test embeddings
2019-02-13 02:31:09,235 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 02:31:23,968 : [('reg:1e-05', 75.47), ('reg:0.0001', 75.22), ('reg:0.001', 74.73), ('reg:0.01', 74.31)]
2019-02-13 02:31:23,968 : Cross-validation : best param found is reg = 1e-05             with score 75.47
2019-02-13 02:31:23,968 : Evaluating...
2019-02-13 02:31:25,082 : Dev acc : 75.47 Test acc 72.52; Test F1 78.74 for MRPC.

2019-02-13 02:31:25,082 : ***** Transfer task : SICK-Entailment*****


2019-02-13 02:31:25,106 : loading BERT mode bert-base-uncased
2019-02-13 02:31:25,106 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:31:25,160 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:31:25,160 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp18gymtb2
2019-02-13 02:31:27,596 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:31:29,057 : Computing embedding for train
2019-02-13 02:31:41,850 : Computed train embeddings
2019-02-13 02:31:41,850 : Computing embedding for dev
2019-02-13 02:31:43,185 : Computed dev embeddings
2019-02-13 02:31:43,185 : Computing embedding for test
2019-02-13 02:31:56,572 : Computed test embeddings
2019-02-13 02:31:56,599 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:32:00,596 : [('reg:1e-05', 77.4), ('reg:0.0001', 77.6), ('reg:0.001', 78.2), ('reg:0.01', 76.4)]
2019-02-13 02:32:00,596 : Validation : best param found is reg = 0.001 with score             78.2
2019-02-13 02:32:00,596 : Evaluating...
2019-02-13 02:32:01,686 : 
Dev acc : 78.2 Test acc : 77.8 for                        SICK entailment

2019-02-13 02:32:01,687 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 02:32:01,714 : loading BERT mode bert-base-uncased
2019-02-13 02:32:01,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:32:01,771 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:32:01,771 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp48ofdpb0
2019-02-13 02:32:04,207 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:32:05,638 : Computing embedding for train
2019-02-13 02:32:12,638 : Computed train embeddings
2019-02-13 02:32:12,638 : Computing embedding for dev
2019-02-13 02:32:13,782 : Computed dev embeddings
2019-02-13 02:32:13,782 : Computing embedding for test
2019-02-13 02:32:23,827 : Computed test embeddings
2019-02-13 02:33:09,833 : Dev : Pearson 0.8000069157066063
2019-02-13 02:33:09,834 : Test : Pearson 0.8012832496078378 Spearman 0.736573993322525 MSE 0.3644766684858283                        for SICK Relatedness

2019-02-13 02:33:09,834 : 

***** Transfer task : STSBenchmark*****


2019-02-13 02:33:09,873 : loading BERT mode bert-base-uncased
2019-02-13 02:33:09,873 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:33:09,901 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:33:09,901 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb9sbdg0h
2019-02-13 02:33:12,340 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:33:13,811 : Computing embedding for train
2019-02-13 02:33:33,467 : Computed train embeddings
2019-02-13 02:33:33,468 : Computing embedding for dev
2019-02-13 02:33:39,557 : Computed dev embeddings
2019-02-13 02:33:39,557 : Computing embedding for test
2019-02-13 02:33:45,137 : Computed test embeddings
2019-02-13 02:35:00,345 : Dev : Pearson 0.6956992452228284
2019-02-13 02:35:00,346 : Test : Pearson 0.6451650982494892 Spearman 0.6412762430950625 MSE 1.5022715488749476                        for SICK Relatedness

2019-02-13 02:35:00,346 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 02:35:00,670 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 02:35:00,681 : loading BERT mode bert-base-uncased
2019-02-13 02:35:00,681 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:35:00,706 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:35:00,706 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxx83fjom
2019-02-13 02:35:03,145 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:35:04,647 : Computing embeddings for train/dev/test
2019-02-13 02:37:51,411 : Computed embeddings
2019-02-13 02:37:51,411 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:39:18,031 : [('reg:1e-05', 85.35), ('reg:0.0001', 83.63), ('reg:0.001', 78.35), ('reg:0.01', 63.56)]
2019-02-13 02:39:18,031 : Validation : best param found is reg = 1e-05 with score             85.35
2019-02-13 02:39:18,031 : Evaluating...
2019-02-13 02:39:34,867 : 
Dev acc : 85.3 Test acc : 85.4 for LENGTH classification

2019-02-13 02:39:34,874 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 02:39:35,214 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 02:39:35,258 : loading BERT mode bert-base-uncased
2019-02-13 02:39:35,258 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:39:35,289 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:39:35,289 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxlrp77bh
2019-02-13 02:39:37,716 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:39:39,136 : Computing embeddings for train/dev/test
2019-02-13 02:41:49,910 : Computed embeddings
2019-02-13 02:41:49,910 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:43:08,965 : [('reg:1e-05', 57.61), ('reg:0.0001', 36.71), ('reg:0.001', 4.91), ('reg:0.01', 0.97)]
2019-02-13 02:43:08,965 : Validation : best param found is reg = 1e-05 with score             57.61
2019-02-13 02:43:08,965 : Evaluating...
2019-02-13 02:43:27,328 : 
Dev acc : 57.6 Test acc : 57.6 for WORDCONTENT classification

2019-02-13 02:43:27,336 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 02:43:27,694 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 02:43:27,760 : loading BERT mode bert-base-uncased
2019-02-13 02:43:27,760 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:43:27,787 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:43:27,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpamxzmswg
2019-02-13 02:43:30,231 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:43:31,670 : Computing embeddings for train/dev/test
2019-02-13 02:45:47,036 : Computed embeddings
2019-02-13 02:45:47,036 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:46:36,682 : [('reg:1e-05', 36.17), ('reg:0.0001', 36.1), ('reg:0.001', 34.51), ('reg:0.01', 29.23)]
2019-02-13 02:46:36,682 : Validation : best param found is reg = 1e-05 with score             36.17
2019-02-13 02:46:36,682 : Evaluating...
2019-02-13 02:46:50,926 : 
Dev acc : 36.2 Test acc : 35.3 for DEPTH classification

2019-02-13 02:46:50,934 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 02:46:51,326 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 02:46:51,389 : loading BERT mode bert-base-uncased
2019-02-13 02:46:51,389 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:46:51,415 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:46:51,415 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_87wohpy
2019-02-13 02:46:53,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:46:55,854 : Computing embeddings for train/dev/test
2019-02-13 02:49:13,547 : Computed embeddings
2019-02-13 02:49:13,547 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:50:41,322 : [('reg:1e-05', 75.96), ('reg:0.0001', 74.75), ('reg:0.001', 68.06), ('reg:0.01', 57.24)]
2019-02-13 02:50:41,322 : Validation : best param found is reg = 1e-05 with score             75.96
2019-02-13 02:50:41,322 : Evaluating...
2019-02-13 02:51:06,538 : 
Dev acc : 76.0 Test acc : 76.0 for TOPCONSTITUENTS classification

2019-02-13 02:51:06,546 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 02:51:06,920 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 02:51:06,987 : loading BERT mode bert-base-uncased
2019-02-13 02:51:06,988 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:51:07,111 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:51:07,111 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuhhpihbz
2019-02-13 02:51:09,567 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:51:11,081 : Computing embeddings for train/dev/test
2019-02-13 02:53:31,582 : Computed embeddings
2019-02-13 02:53:31,583 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:54:45,305 : [('reg:1e-05', 90.54), ('reg:0.0001', 90.2), ('reg:0.001', 90.07), ('reg:0.01', 88.69)]
2019-02-13 02:54:45,305 : Validation : best param found is reg = 1e-05 with score             90.54
2019-02-13 02:54:45,305 : Evaluating...
2019-02-13 02:55:09,067 : 
Dev acc : 90.5 Test acc : 90.0 for BIGRAMSHIFT classification

2019-02-13 02:55:09,075 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 02:55:09,459 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 02:55:09,526 : loading BERT mode bert-base-uncased
2019-02-13 02:55:09,526 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:55:09,647 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:55:09,647 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7eemt89f
2019-02-13 02:55:12,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:55:13,562 : Computing embeddings for train/dev/test
2019-02-13 02:58:07,112 : Computed embeddings
2019-02-13 02:58:07,112 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 02:59:14,549 : [('reg:1e-05', 89.49), ('reg:0.0001', 89.61), ('reg:0.001', 90.04), ('reg:0.01', 90.15)]
2019-02-13 02:59:14,549 : Validation : best param found is reg = 0.01 with score             90.15
2019-02-13 02:59:14,549 : Evaluating...
2019-02-13 02:59:30,713 : 
Dev acc : 90.2 Test acc : 89.5 for TENSE classification

2019-02-13 02:59:30,721 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 02:59:31,295 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 02:59:31,359 : loading BERT mode bert-base-uncased
2019-02-13 02:59:31,359 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 02:59:31,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 02:59:31,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx_345olp
2019-02-13 02:59:33,852 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 02:59:35,343 : Computing embeddings for train/dev/test
2019-02-13 03:02:36,882 : Computed embeddings
2019-02-13 03:02:36,882 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:03:38,121 : [('reg:1e-05', 85.59), ('reg:0.0001', 85.67), ('reg:0.001', 85.79), ('reg:0.01', 84.93)]
2019-02-13 03:03:38,122 : Validation : best param found is reg = 0.001 with score             85.79
2019-02-13 03:03:38,122 : Evaluating...
2019-02-13 03:03:48,634 : 
Dev acc : 85.8 Test acc : 85.9 for SUBJNUMBER classification

2019-02-13 03:03:48,641 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 03:03:49,083 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 03:03:49,151 : loading BERT mode bert-base-uncased
2019-02-13 03:03:49,152 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:03:49,182 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:03:49,182 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp24ok4_qn
2019-02-13 03:03:51,651 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:03:53,084 : Computing embeddings for train/dev/test
2019-02-13 03:06:15,709 : Computed embeddings
2019-02-13 03:06:15,709 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:07:30,877 : [('reg:1e-05', 82.05), ('reg:0.0001', 82.02), ('reg:0.001', 81.86), ('reg:0.01', 80.79)]
2019-02-13 03:07:30,877 : Validation : best param found is reg = 1e-05 with score             82.05
2019-02-13 03:07:30,877 : Evaluating...
2019-02-13 03:07:46,649 : 
Dev acc : 82.0 Test acc : 82.3 for OBJNUMBER classification

2019-02-13 03:07:46,657 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 03:07:47,063 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 03:07:47,133 : loading BERT mode bert-base-uncased
2019-02-13 03:07:47,133 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:07:47,160 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:07:47,161 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmojwspj6
2019-02-13 03:07:49,595 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:07:51,002 : Computing embeddings for train/dev/test
2019-02-13 03:10:15,231 : Computed embeddings
2019-02-13 03:10:15,232 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:11:31,551 : [('reg:1e-05', 65.86), ('reg:0.0001', 65.85), ('reg:0.001', 65.8), ('reg:0.01', 65.2)]
2019-02-13 03:11:31,552 : Validation : best param found is reg = 1e-05 with score             65.86
2019-02-13 03:11:31,552 : Evaluating...
2019-02-13 03:11:43,201 : 
Dev acc : 65.9 Test acc : 65.5 for ODDMANOUT classification

2019-02-13 03:11:43,209 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 03:11:43,620 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 03:11:43,697 : loading BERT mode bert-base-uncased
2019-02-13 03:11:43,697 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:11:43,826 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:11:43,826 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8gsg3gxo
2019-02-13 03:11:46,280 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:11:47,715 : Computing embeddings for train/dev/test
2019-02-13 03:14:12,980 : Computed embeddings
2019-02-13 03:14:12,980 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:15:48,579 : [('reg:1e-05', 70.52), ('reg:0.0001', 70.32), ('reg:0.001', 69.39), ('reg:0.01', 67.36)]
2019-02-13 03:15:48,580 : Validation : best param found is reg = 1e-05 with score             70.52
2019-02-13 03:15:48,580 : Evaluating...
2019-02-13 03:15:58,467 : 
Dev acc : 70.5 Test acc : 70.3 for COORDINATIONINVERSION classification

2019-02-13 03:15:58,476 : {'STS12': {'MSRpar': {'pearson': (0.3828151259012993, 1.3792388318332394e-27), 'spearman': SpearmanrResult(correlation=0.4112583963770455, pvalue=5.657478670992601e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3291030760088587, 2.1041572932514723e-20), 'spearman': SpearmanrResult(correlation=0.369507392647296, pvalue=1.1195175402053012e-25), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48819885929891493, 7.291906855929607e-29), 'spearman': SpearmanrResult(correlation=0.5826315527518718, pvalue=4.482794726754743e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5921556820933219, 3.6211176833430453e-72), 'spearman': SpearmanrResult(correlation=0.5924335763710942, pvalue=2.994174846102081e-72), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6180302217145976, 2.1189500583553706e-43), 'spearman': SpearmanrResult(correlation=0.5631719647854353, pvalue=9.147317638453837e-35), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48206059300339843, 'wmean': 0.46613022776220603}, 'spearman': {'mean': 0.5038005765865485, 'wmean': 0.48971461412775885}}}, 'STS13': {'FNWN': {'pearson': (0.32025624012500503, 7.039822301099561e-06), 'spearman': SpearmanrResult(correlation=0.3504611661097847, pvalue=7.662146373482486e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.5885193593828035, 4.284253158185351e-71), 'spearman': SpearmanrResult(correlation=0.5582382056762079, pvalue=1.1539402900416263e-62), 'nsamples': 750}, 'OnWN': {'pearson': (0.4329782618444952, 4.8794205155135395e-27), 'spearman': SpearmanrResult(correlation=0.4489265413514015, pvalue=3.5757180162607706e-29), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4472512871174346, 'wmean': 0.49654583587699364}, 'spearman': {'mean': 0.452541971045798, 'wmean': 0.49117573623336097}}}, 'STS14': {'deft-forum': {'pearson': (0.25926635833519024, 2.4014034407490316e-08), 'spearman': SpearmanrResult(correlation=0.26324895385057673, pvalue=1.4353260786388081e-08), 'nsamples': 450}, 'deft-news': {'pearson': (0.7286834968974828, 6.430162989544107e-51), 'spearman': SpearmanrResult(correlation=0.6961760636778783, pvalue=8.357320448299725e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5536685992746222, 1.8205999525218607e-61), 'spearman': SpearmanrResult(correlation=0.5103776615530352, pvalue=5.513387879996314e-51), 'nsamples': 750}, 'images': {'pearson': (0.42816357646345854, 8.655753653454326e-35), 'spearman': SpearmanrResult(correlation=0.4384485898641238, pvalue=1.3953771383681995e-36), 'nsamples': 750}, 'OnWN': {'pearson': (0.6203270709048873, 5.779768065261601e-81), 'spearman': SpearmanrResult(correlation=0.6486463661064126, pvalue=9.250288619984482e-91), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6670283414239054, 1.0640815920823196e-97), 'spearman': SpearmanrResult(correlation=0.5888555482703328, pvalue=3.41382023492567e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5428562405499244, 'wmean': 0.543244160365396}, 'spearman': {'mean': 0.5242921972203932, 'wmean': 0.5245495927150804}}}, 'STS15': {'answers-forums': {'pearson': (0.5606909538224976, 1.971593224937272e-32), 'spearman': SpearmanrResult(correlation=0.560940963193196, pvalue=1.8259676477691671e-32), 'nsamples': 375}, 'answers-students': {'pearson': (0.655364212752704, 3.0659654433576466e-93), 'spearman': SpearmanrResult(correlation=0.6645993466553045, pvalue=9.38189825987902e-97), 'nsamples': 750}, 'belief': {'pearson': (0.6659891896926381, 2.0610344388712928e-49), 'spearman': SpearmanrResult(correlation=0.6872367218304624, pvalue=1.008021469085547e-53), 'nsamples': 375}, 'headlines': {'pearson': (0.6003190730228357, 1.2575411258461446e-74), 'spearman': SpearmanrResult(correlation=0.593983199140433, pvalue=1.0336906562857055e-72), 'nsamples': 750}, 'images': {'pearson': (0.6201764073572635, 6.476694886268889e-81), 'spearman': SpearmanrResult(correlation=0.6276716567021452, pvalue=2.082425897712839e-83), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6205079673295877, 'wmean': 0.6222999412225928}, 'spearman': {'mean': 0.6268863775043082, 'wmean': 0.6275857612524279}}}, 'STS16': {'answer-answer': {'pearson': (0.5331622171576518, 4.6640449308039206e-20), 'spearman': SpearmanrResult(correlation=0.5294180813322269, pvalue=9.442088323119602e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6373108757495696, 8.830760648461452e-30), 'spearman': SpearmanrResult(correlation=0.6452597785042347, pvalue=1.027785607363864e-30), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7408326713043127, 2.790536038332583e-41), 'spearman': SpearmanrResult(correlation=0.754694047222435, pvalue=1.2852623758828504e-43), 'nsamples': 230}, 'postediting': {'pearson': (0.7986297636075997, 2.7391574301305685e-55), 'spearman': SpearmanrResult(correlation=0.8279465722413261, pvalue=1.0083578057039882e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.27755221784152473, 4.727847388318004e-05), 'spearman': SpearmanrResult(correlation=0.27753398112505406, pvalue=4.733497623980791e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5974975491321317, 'wmean': 0.604872851154141}, 'spearman': {'mean': 0.6069704920850554, 'wmean': 0.6144562175793592}}}, 'MR': {'devacc': 81.11, 'acc': 81.05, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.17, 'acc': 87.02, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.56, 'acc': 88.64, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.41, 'acc': 94.77, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 85.89, 'acc': 85.39, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.05, 'acc': 43.53, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 86.56, 'acc': 93.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 75.47, 'acc': 72.52, 'f1': 78.74, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 77.8, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8000069157066063, 'pearson': 0.8012832496078378, 'spearman': 0.736573993322525, 'mse': 0.3644766684858283, 'yhat': array([2.4171165 , 3.82702025, 1.57020752, ..., 3.15177663, 4.48826416,
       4.3184512 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6956992452228284, 'pearson': 0.6451650982494892, 'spearman': 0.6412762430950625, 'mse': 1.5022715488749476, 'yhat': array([1.5141775 , 1.99109381, 2.41360134, ..., 3.8456554 , 3.69567911,
       3.63102346]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 85.35, 'acc': 85.37, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 57.61, 'acc': 57.6, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 36.17, 'acc': 35.27, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 75.96, 'acc': 75.97, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.54, 'acc': 89.97, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.15, 'acc': 89.54, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.79, 'acc': 85.89, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 82.05, 'acc': 82.35, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.86, 'acc': 65.51, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 70.52, 'acc': 70.35, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 03:15:58,476 : ********************************************************************************
2019-02-13 03:15:58,476 : ********************************************************************************
2019-02-13 03:15:58,476 : ********************************************************************************
2019-02-13 03:15:58,476 : layer 10
2019-02-13 03:15:58,476 : ********************************************************************************
2019-02-13 03:15:58,476 : ********************************************************************************
2019-02-13 03:15:58,476 : ********************************************************************************
2019-02-13 03:15:58,565 : ***** Transfer task : STS12 *****


2019-02-13 03:15:58,578 : loading BERT mode bert-base-uncased
2019-02-13 03:15:58,578 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:15:58,595 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:15:58,595 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbutf5m8u
2019-02-13 03:16:01,032 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:16:04,675 : MSRpar : pearson = 0.3743, spearman = 0.4151
2019-02-13 03:16:05,867 : MSRvid : pearson = 0.3791, spearman = 0.4113
2019-02-13 03:16:06,789 : SMTeuroparl : pearson = 0.5222, spearman = 0.6099
2019-02-13 03:16:08,411 : surprise.OnWN : pearson = 0.5902, spearman = 0.5909
2019-02-13 03:16:09,302 : surprise.SMTnews : pearson = 0.5917, spearman = 0.5589
2019-02-13 03:16:09,302 : ALL (weighted average) : Pearson = 0.4773,             Spearman = 0.5038
2019-02-13 03:16:09,302 : ALL (average) : Pearson = 0.4915,             Spearman = 0.5172

2019-02-13 03:16:09,302 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 03:16:09,312 : loading BERT mode bert-base-uncased
2019-02-13 03:16:09,312 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:16:09,330 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:16:09,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphharm6d_
2019-02-13 03:16:11,774 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:16:13,930 : FNWN : pearson = 0.3346, spearman = 0.3854
2019-02-13 03:16:15,270 : headlines : pearson = 0.6114, spearman = 0.5797
2019-02-13 03:16:16,282 : OnWN : pearson = 0.5665, spearman = 0.5776
2019-02-13 03:16:16,282 : ALL (weighted average) : Pearson = 0.5597,             Spearman = 0.5545
2019-02-13 03:16:16,282 : ALL (average) : Pearson = 0.5042,             Spearman = 0.5143

2019-02-13 03:16:16,282 : ***** Transfer task : STS14 *****


2019-02-13 03:16:16,299 : loading BERT mode bert-base-uncased
2019-02-13 03:16:16,299 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:16:16,319 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:16:16,319 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpor4juu3m
2019-02-13 03:16:18,756 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:16:21,001 : deft-forum : pearson = 0.2913, spearman = 0.2889
2019-02-13 03:16:21,798 : deft-news : pearson = 0.7533, spearman = 0.7214
2019-02-13 03:16:22,996 : headlines : pearson = 0.5759, spearman = 0.5278
2019-02-13 03:16:24,289 : images : pearson = 0.4319, spearman = 0.4331
2019-02-13 03:16:26,284 : OnWN : pearson = 0.6978, spearman = 0.7210
2019-02-13 03:16:28,822 : tweet-news : pearson = 0.6670, spearman = 0.5886
2019-02-13 03:16:28,823 : ALL (weighted average) : Pearson = 0.5698,             Spearman = 0.5465
2019-02-13 03:16:28,823 : ALL (average) : Pearson = 0.5695,             Spearman = 0.5468

2019-02-13 03:16:28,823 : ***** Transfer task : STS15 *****


2019-02-13 03:16:28,890 : loading BERT mode bert-base-uncased
2019-02-13 03:16:28,890 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:16:28,909 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:16:28,909 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1j_4e2ra
2019-02-13 03:16:31,347 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:16:34,520 : answers-forums : pearson = 0.5931, spearman = 0.5962
2019-02-13 03:16:37,566 : answers-students : pearson = 0.5937, spearman = 0.6128
2019-02-13 03:16:39,094 : belief : pearson = 0.6926, spearman = 0.7215
2019-02-13 03:16:41,487 : headlines : pearson = 0.6364, spearman = 0.6284
2019-02-13 03:16:43,932 : images : pearson = 0.6318, spearman = 0.6412
2019-02-13 03:16:43,932 : ALL (weighted average) : Pearson = 0.6262,             Spearman = 0.6353
2019-02-13 03:16:43,932 : ALL (average) : Pearson = 0.6295,             Spearman = 0.6400

2019-02-13 03:16:43,932 : ***** Transfer task : STS16 *****


2019-02-13 03:16:44,006 : loading BERT mode bert-base-uncased
2019-02-13 03:16:44,006 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:16:44,025 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:16:44,025 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx0ygw50k
2019-02-13 03:16:46,458 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:16:48,327 : answer-answer : pearson = 0.5548, spearman = 0.5392
2019-02-13 03:16:48,713 : headlines : pearson = 0.6461, spearman = 0.6555
2019-02-13 03:16:49,152 : plagiarism : pearson = 0.7671, spearman = 0.7783
2019-02-13 03:16:49,848 : postediting : pearson = 0.8073, spearman = 0.8305
2019-02-13 03:16:50,193 : question-question : pearson = 0.4378, spearman = 0.4305
2019-02-13 03:16:50,193 : ALL (weighted average) : Pearson = 0.6465,             Spearman = 0.6507
2019-02-13 03:16:50,193 : ALL (average) : Pearson = 0.6426,             Spearman = 0.6468

2019-02-13 03:16:50,193 : ***** Transfer task : MR *****


2019-02-13 03:16:50,210 : loading BERT mode bert-base-uncased
2019-02-13 03:16:50,211 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:16:50,233 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:16:50,233 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcsww1gtd
2019-02-13 03:16:52,667 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:16:54,200 : Generating sentence embeddings
2019-02-13 03:17:11,543 : Generated sentence embeddings
2019-02-13 03:17:11,543 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:17:42,667 : Best param found at split 1: l2reg = 0.0001                 with score 80.98
2019-02-13 03:18:11,651 : Best param found at split 2: l2reg = 0.0001                 with score 81.11
2019-02-13 03:18:47,484 : Best param found at split 3: l2reg = 0.01                 with score 81.76
2019-02-13 03:19:19,722 : Best param found at split 4: l2reg = 1e-05                 with score 81.01
2019-02-13 03:19:38,129 : Best param found at split 5: l2reg = 0.01                 with score 81.56
2019-02-13 03:19:38,755 : Dev acc : 81.28 Test acc : 80.46

2019-02-13 03:19:38,756 : ***** Transfer task : CR *****


2019-02-13 03:19:38,764 : loading BERT mode bert-base-uncased
2019-02-13 03:19:38,764 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:19:38,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:19:38,784 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi9dibyb3
2019-02-13 03:19:41,253 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:19:42,771 : Generating sentence embeddings
2019-02-13 03:19:48,436 : Generated sentence embeddings
2019-02-13 03:19:48,436 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:19:56,144 : Best param found at split 1: l2reg = 0.01                 with score 87.08
2019-02-13 03:20:04,752 : Best param found at split 2: l2reg = 0.0001                 with score 87.28
2019-02-13 03:20:10,257 : Best param found at split 3: l2reg = 0.001                 with score 87.75
2019-02-13 03:20:15,447 : Best param found at split 4: l2reg = 0.01                 with score 86.99
2019-02-13 03:20:20,265 : Best param found at split 5: l2reg = 0.01                 with score 87.65
2019-02-13 03:20:20,479 : Dev acc : 87.35 Test acc : 86.23

2019-02-13 03:20:20,480 : ***** Transfer task : MPQA *****


2019-02-13 03:20:20,485 : loading BERT mode bert-base-uncased
2019-02-13 03:20:20,485 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:20:20,505 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:20:20,505 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_fq1f41m
2019-02-13 03:20:22,975 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:20:24,470 : Generating sentence embeddings
2019-02-13 03:20:33,786 : Generated sentence embeddings
2019-02-13 03:20:33,786 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:20:59,987 : Best param found at split 1: l2reg = 0.001                 with score 88.65
2019-02-13 03:21:35,756 : Best param found at split 2: l2reg = 0.001                 with score 87.67
2019-02-13 03:22:15,296 : Best param found at split 3: l2reg = 0.01                 with score 87.34
2019-02-13 03:22:57,676 : Best param found at split 4: l2reg = 0.001                 with score 88.78
2019-02-13 03:23:28,833 : Best param found at split 5: l2reg = 0.001                 with score 87.94
2019-02-13 03:23:30,212 : Dev acc : 88.08 Test acc : 88.39

2019-02-13 03:23:30,213 : ***** Transfer task : SUBJ *****


2019-02-13 03:23:30,227 : loading BERT mode bert-base-uncased
2019-02-13 03:23:30,228 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:23:30,249 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:23:30,249 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsyoqcdtz
2019-02-13 03:23:32,692 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:23:34,182 : Generating sentence embeddings
2019-02-13 03:23:53,580 : Generated sentence embeddings
2019-02-13 03:23:53,580 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 03:24:08,196 : Best param found at split 1: l2reg = 0.0001                 with score 95.5
2019-02-13 03:24:22,033 : Best param found at split 2: l2reg = 0.001                 with score 95.48
2019-02-13 03:24:46,989 : Best param found at split 3: l2reg = 0.0001                 with score 95.32
2019-02-13 03:25:12,407 : Best param found at split 4: l2reg = 0.001                 with score 95.74
2019-02-13 03:25:43,973 : Best param found at split 5: l2reg = 0.0001                 with score 95.41
2019-02-13 03:25:45,408 : Dev acc : 95.49 Test acc : 94.95

2019-02-13 03:25:45,409 : ***** Transfer task : SST Binary classification *****


2019-02-13 03:25:45,537 : loading BERT mode bert-base-uncased
2019-02-13 03:25:45,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:25:45,560 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:25:45,561 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp99yj7g9l
2019-02-13 03:25:47,990 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:25:49,546 : Computing embedding for train
2019-02-13 03:27:46,266 : Computed train embeddings
2019-02-13 03:27:46,266 : Computing embedding for dev
2019-02-13 03:27:47,640 : Computed dev embeddings
2019-02-13 03:27:47,641 : Computing embedding for test
2019-02-13 03:27:50,720 : Computed test embeddings
2019-02-13 03:27:50,720 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:28:23,345 : [('reg:1e-05', 84.63), ('reg:0.0001', 84.52), ('reg:0.001', 86.47), ('reg:0.01', 84.98)]
2019-02-13 03:28:23,345 : Validation : best param found is reg = 0.001 with score             86.47
2019-02-13 03:28:23,345 : Evaluating...
2019-02-13 03:28:31,581 : 
Dev acc : 86.47 Test acc : 85.5 for             SST Binary classification

2019-02-13 03:28:31,586 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 03:28:31,641 : loading BERT mode bert-base-uncased
2019-02-13 03:28:31,641 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:28:31,662 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:28:31,662 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvsbrzr8c
2019-02-13 03:28:34,105 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:28:35,566 : Computing embedding for train
2019-02-13 03:28:50,264 : Computed train embeddings
2019-02-13 03:28:50,264 : Computing embedding for dev
2019-02-13 03:28:52,220 : Computed dev embeddings
2019-02-13 03:28:52,220 : Computing embedding for test
2019-02-13 03:28:56,090 : Computed test embeddings
2019-02-13 03:28:56,091 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:29:01,924 : [('reg:1e-05', 44.6), ('reg:0.0001', 44.78), ('reg:0.001', 45.23), ('reg:0.01', 45.69)]
2019-02-13 03:29:01,924 : Validation : best param found is reg = 0.01 with score             45.69
2019-02-13 03:29:01,924 : Evaluating...
2019-02-13 03:29:03,470 : 
Dev acc : 45.69 Test acc : 44.93 for             SST Fine-Grained classification

2019-02-13 03:29:03,470 : ***** Transfer task : TREC *****


2019-02-13 03:29:03,483 : loading BERT mode bert-base-uncased
2019-02-13 03:29:03,483 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:29:03,502 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:29:03,502 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5rh7eigd
2019-02-13 03:29:05,974 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:29:15,276 : Computed train embeddings
2019-02-13 03:29:15,965 : Computed test embeddings
2019-02-13 03:29:15,965 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 03:29:32,881 : [('reg:1e-05', 82.67), ('reg:0.0001', 84.26), ('reg:0.001', 83.49), ('reg:0.01', 79.67)]
2019-02-13 03:29:32,881 : Cross-validation : best param found is reg = 0.0001             with score 84.26
2019-02-13 03:29:32,881 : Evaluating...
2019-02-13 03:29:33,837 : 
Dev acc : 84.26 Test acc : 91.4             for TREC

2019-02-13 03:29:33,837 : ***** Transfer task : MRPC *****


2019-02-13 03:29:33,856 : loading BERT mode bert-base-uncased
2019-02-13 03:29:33,856 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:29:33,880 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:29:33,880 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwp_svgh0
2019-02-13 03:29:36,320 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:29:37,780 : Computing embedding for train
2019-02-13 03:29:54,351 : Computed train embeddings
2019-02-13 03:29:54,351 : Computing embedding for test
2019-02-13 03:30:02,426 : Computed test embeddings
2019-02-13 03:30:02,442 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 03:30:23,838 : [('reg:1e-05', 74.93), ('reg:0.0001', 74.53), ('reg:0.001', 74.73), ('reg:0.01', 74.66)]
2019-02-13 03:30:23,838 : Cross-validation : best param found is reg = 1e-05             with score 74.93
2019-02-13 03:30:23,838 : Evaluating...
2019-02-13 03:30:25,478 : Dev acc : 74.93 Test acc 73.33; Test F1 80.31 for MRPC.

2019-02-13 03:30:25,901 : ***** Transfer task : SICK-Entailment*****


2019-02-13 03:30:25,932 : loading BERT mode bert-base-uncased
2019-02-13 03:30:25,932 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:30:25,985 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:30:25,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkci3r_6a
2019-02-13 03:30:28,463 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:30:30,027 : Computing embedding for train
2019-02-13 03:30:47,393 : Computed train embeddings
2019-02-13 03:30:47,393 : Computing embedding for dev
2019-02-13 03:30:49,545 : Computed dev embeddings
2019-02-13 03:30:49,545 : Computing embedding for test
2019-02-13 03:31:07,560 : Computed test embeddings
2019-02-13 03:31:07,588 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:31:12,591 : [('reg:1e-05', 75.8), ('reg:0.0001', 76.2), ('reg:0.001', 75.4), ('reg:0.01', 73.8)]
2019-02-13 03:31:12,591 : Validation : best param found is reg = 0.0001 with score             76.2
2019-02-13 03:31:12,592 : Evaluating...
2019-02-13 03:31:14,074 : 
Dev acc : 76.2 Test acc : 76.68 for                        SICK entailment

2019-02-13 03:31:14,075 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 03:31:14,101 : loading BERT mode bert-base-uncased
2019-02-13 03:31:14,101 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:31:14,157 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:31:14,157 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9meb6day
2019-02-13 03:31:16,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:31:18,140 : Computing embedding for train
2019-02-13 03:31:34,104 : Computed train embeddings
2019-02-13 03:31:34,104 : Computing embedding for dev
2019-02-13 03:31:35,535 : Computed dev embeddings
2019-02-13 03:31:35,535 : Computing embedding for test
2019-02-13 03:31:48,924 : Computed test embeddings
2019-02-13 03:32:39,202 : Dev : Pearson 0.8092191887799094
2019-02-13 03:32:39,204 : Test : Pearson 0.8085801008942196 Spearman 0.7411812882756645 MSE 0.3533752033657978                        for SICK Relatedness

2019-02-13 03:32:39,205 : 

***** Transfer task : STSBenchmark*****


2019-02-13 03:32:39,272 : loading BERT mode bert-base-uncased
2019-02-13 03:32:39,272 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:32:39,292 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:32:39,292 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxbkzf560
2019-02-13 03:32:41,723 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:32:43,149 : Computing embedding for train
2019-02-13 03:32:54,943 : Computed train embeddings
2019-02-13 03:32:54,943 : Computing embedding for dev
2019-02-13 03:32:58,323 : Computed dev embeddings
2019-02-13 03:32:58,323 : Computing embedding for test
2019-02-13 03:33:00,930 : Computed test embeddings
2019-02-13 03:33:58,367 : Dev : Pearson 0.7005391109290238
2019-02-13 03:33:58,367 : Test : Pearson 0.6565506226440389 Spearman 0.6497495904346781 MSE 1.5007045658522677                        for SICK Relatedness

2019-02-13 03:33:58,367 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 03:33:58,684 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 03:33:58,694 : loading BERT mode bert-base-uncased
2019-02-13 03:33:58,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:33:58,719 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:33:58,719 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpswf898y4
2019-02-13 03:34:01,153 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:34:02,694 : Computing embeddings for train/dev/test
2019-02-13 03:37:27,643 : Computed embeddings
2019-02-13 03:37:27,643 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:38:24,648 : [('reg:1e-05', 82.02), ('reg:0.0001', 80.44), ('reg:0.001', 73.79), ('reg:0.01', 58.7)]
2019-02-13 03:38:24,648 : Validation : best param found is reg = 1e-05 with score             82.02
2019-02-13 03:38:24,648 : Evaluating...
2019-02-13 03:38:57,175 : 
Dev acc : 82.0 Test acc : 82.6 for LENGTH classification

2019-02-13 03:38:57,182 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 03:38:57,526 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 03:38:57,572 : loading BERT mode bert-base-uncased
2019-02-13 03:38:57,572 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:38:57,602 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:38:57,602 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfwngadzv
2019-02-13 03:39:00,032 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:39:01,517 : Computing embeddings for train/dev/test
2019-02-13 03:41:44,333 : Computed embeddings
2019-02-13 03:41:44,333 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:42:33,256 : [('reg:1e-05', 54.73), ('reg:0.0001', 34.06), ('reg:0.001', 5.16), ('reg:0.01', 1.08)]
2019-02-13 03:42:33,256 : Validation : best param found is reg = 1e-05 with score             54.73
2019-02-13 03:42:33,256 : Evaluating...
2019-02-13 03:42:54,005 : 
Dev acc : 54.7 Test acc : 55.0 for WORDCONTENT classification

2019-02-13 03:42:54,013 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 03:42:54,914 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 03:42:54,979 : loading BERT mode bert-base-uncased
2019-02-13 03:42:54,979 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:42:55,005 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:42:55,005 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoo2fzcnp
2019-02-13 03:42:57,446 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:42:58,904 : Computing embeddings for train/dev/test
2019-02-13 03:45:40,814 : Computed embeddings
2019-02-13 03:45:40,814 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:46:24,299 : [('reg:1e-05', 34.66), ('reg:0.0001', 34.32), ('reg:0.001', 32.86), ('reg:0.01', 28.83)]
2019-02-13 03:46:24,299 : Validation : best param found is reg = 1e-05 with score             34.66
2019-02-13 03:46:24,299 : Evaluating...
2019-02-13 03:46:39,824 : 
Dev acc : 34.7 Test acc : 34.5 for DEPTH classification

2019-02-13 03:46:39,833 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 03:46:40,242 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 03:46:40,305 : loading BERT mode bert-base-uncased
2019-02-13 03:46:40,306 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:46:40,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:46:40,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoid_tbbr
2019-02-13 03:46:42,764 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:46:44,236 : Computing embeddings for train/dev/test
2019-02-13 03:49:39,229 : Computed embeddings
2019-02-13 03:49:39,229 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:50:33,513 : [('reg:1e-05', 70.96), ('reg:0.0001', 69.97), ('reg:0.001', 64.56), ('reg:0.01', 54.62)]
2019-02-13 03:50:33,513 : Validation : best param found is reg = 1e-05 with score             70.96
2019-02-13 03:50:33,513 : Evaluating...
2019-02-13 03:50:50,637 : 
Dev acc : 71.0 Test acc : 70.7 for TOPCONSTITUENTS classification

2019-02-13 03:50:50,645 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 03:50:50,997 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 03:50:51,063 : loading BERT mode bert-base-uncased
2019-02-13 03:50:51,063 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:50:51,183 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:50:51,183 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8cfww64s
2019-02-13 03:50:53,618 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:50:55,082 : Computing embeddings for train/dev/test
2019-02-13 03:53:50,028 : Computed embeddings
2019-02-13 03:53:50,028 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:54:58,849 : [('reg:1e-05', 89.85), ('reg:0.0001', 89.84), ('reg:0.001', 89.54), ('reg:0.01', 88.01)]
2019-02-13 03:54:58,850 : Validation : best param found is reg = 1e-05 with score             89.85
2019-02-13 03:54:58,850 : Evaluating...
2019-02-13 03:55:23,149 : 
Dev acc : 89.8 Test acc : 89.3 for BIGRAMSHIFT classification

2019-02-13 03:55:23,156 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 03:55:23,545 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 03:55:23,613 : loading BERT mode bert-base-uncased
2019-02-13 03:55:23,613 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:55:23,734 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:55:23,735 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzq68gi7p
2019-02-13 03:55:26,180 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:55:27,734 : Computing embeddings for train/dev/test
2019-02-13 03:58:58,714 : Computed embeddings
2019-02-13 03:58:58,714 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 03:59:27,854 : [('reg:1e-05', 88.89), ('reg:0.0001', 88.92), ('reg:0.001', 89.27), ('reg:0.01', 89.89)]
2019-02-13 03:59:27,855 : Validation : best param found is reg = 0.01 with score             89.89
2019-02-13 03:59:27,855 : Evaluating...
2019-02-13 03:59:36,832 : 
Dev acc : 89.9 Test acc : 89.1 for TENSE classification

2019-02-13 03:59:36,839 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 03:59:37,415 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 03:59:37,478 : loading BERT mode bert-base-uncased
2019-02-13 03:59:37,479 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 03:59:37,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 03:59:37,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn3knb1mr
2019-02-13 03:59:39,981 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 03:59:41,389 : Computing embeddings for train/dev/test
2019-02-13 04:03:09,987 : Computed embeddings
2019-02-13 04:03:09,987 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:04:15,706 : [('reg:1e-05', 85.08), ('reg:0.0001', 85.26), ('reg:0.001', 84.98), ('reg:0.01', 83.57)]
2019-02-13 04:04:15,706 : Validation : best param found is reg = 0.0001 with score             85.26
2019-02-13 04:04:15,706 : Evaluating...
2019-02-13 04:04:28,676 : 
Dev acc : 85.3 Test acc : 85.2 for SUBJNUMBER classification

2019-02-13 04:04:28,684 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 04:04:29,110 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 04:04:29,177 : loading BERT mode bert-base-uncased
2019-02-13 04:04:29,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:04:29,207 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:04:29,207 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf3yc817l
2019-02-13 04:04:31,642 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:04:33,129 : Computing embeddings for train/dev/test
2019-02-13 04:07:12,555 : Computed embeddings
2019-02-13 04:07:12,555 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:08:20,053 : [('reg:1e-05', 81.03), ('reg:0.0001', 81.23), ('reg:0.001', 81.27), ('reg:0.01', 79.96)]
2019-02-13 04:08:20,053 : Validation : best param found is reg = 0.001 with score             81.27
2019-02-13 04:08:20,053 : Evaluating...
2019-02-13 04:08:31,513 : 
Dev acc : 81.3 Test acc : 81.1 for OBJNUMBER classification

2019-02-13 04:08:31,522 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 04:08:31,930 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 04:08:31,999 : loading BERT mode bert-base-uncased
2019-02-13 04:08:31,999 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:08:32,027 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:08:32,027 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbdaf1dda
2019-02-13 04:08:34,495 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:08:35,935 : Computing embeddings for train/dev/test
2019-02-13 04:11:19,519 : Computed embeddings
2019-02-13 04:11:19,519 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:12:01,865 : [('reg:1e-05', 64.73), ('reg:0.0001', 64.71), ('reg:0.001', 64.32), ('reg:0.01', 63.84)]
2019-02-13 04:12:01,865 : Validation : best param found is reg = 1e-05 with score             64.73
2019-02-13 04:12:01,865 : Evaluating...
2019-02-13 04:12:08,906 : 
Dev acc : 64.7 Test acc : 64.7 for ODDMANOUT classification

2019-02-13 04:12:08,914 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 04:12:09,318 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 04:12:09,393 : loading BERT mode bert-base-uncased
2019-02-13 04:12:09,394 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:12:09,519 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:12:09,519 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp628zo95p
2019-02-13 04:12:11,947 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:12:13,357 : Computing embeddings for train/dev/test
2019-02-13 04:15:01,675 : Computed embeddings
2019-02-13 04:15:01,675 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:16:04,733 : [('reg:1e-05', 69.2), ('reg:0.0001', 69.14), ('reg:0.001', 69.04), ('reg:0.01', 66.66)]
2019-02-13 04:16:04,734 : Validation : best param found is reg = 1e-05 with score             69.2
2019-02-13 04:16:04,734 : Evaluating...
2019-02-13 04:16:24,716 : 
Dev acc : 69.2 Test acc : 69.5 for COORDINATIONINVERSION classification

2019-02-13 04:16:24,725 : {'STS12': {'MSRpar': {'pearson': (0.3742786620814638, 2.369308900915339e-26), 'spearman': SpearmanrResult(correlation=0.4150805944186608, pvalue=1.3491109933950578e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.37905832937284883, 4.871469703875893e-27), 'spearman': SpearmanrResult(correlation=0.41128935430933333, pvalue=5.592585408505575e-32), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.522178372260006, 1.807385103460622e-33), 'spearman': SpearmanrResult(correlation=0.6098767342614917, pvalue=4.233081104025531e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5901947596461897, 1.3777767214121588e-71), 'spearman': SpearmanrResult(correlation=0.5909172198464765, pvalue=8.430028428744891e-72), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5916586139290059, 4.8256589945921295e-39), 'spearman': SpearmanrResult(correlation=0.5588820693635927, pvalue=3.7164605758760252e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4914737474579029, 'wmean': 0.4772845795207184}, 'spearman': {'mean': 0.517209194439911, 'wmean': 0.503826493929521}}}, 'STS13': {'FNWN': {'pearson': (0.3346327564804086, 2.522343460750998e-06), 'spearman': SpearmanrResult(correlation=0.38543796879913433, pvalue=4.3298049094046294e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6114286465469412, 4.330613404595034e-78), 'spearman': SpearmanrResult(correlation=0.579740632518533, pvalue=1.469154651504318e-68), 'nsamples': 750}, 'OnWN': {'pearson': (0.5664761648789631, 6.303222174376555e-49), 'spearman': SpearmanrResult(correlation=0.5776253390670718, pvalue=3.0866998863855346e-51), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5041791893021043, 'wmean': 0.5597401362547343}, 'spearman': {'mean': 0.5142679801282464, 'wmean': 0.5544673771390423}}}, 'STS14': {'deft-forum': {'pearson': (0.2913342078398123, 2.9741796904408413e-10), 'spearman': SpearmanrResult(correlation=0.2889242575345343, pvalue=4.220326818443346e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.7532784307803164, 3.622918995433998e-56), 'spearman': SpearmanrResult(correlation=0.721396255191701, pvalue=1.795859492492797e-49), 'nsamples': 300}, 'headlines': {'pearson': (0.5759365092168149, 1.7455860486360548e-67), 'spearman': SpearmanrResult(correlation=0.5277903312999938, pvalue=5.095469749864704e-55), 'nsamples': 750}, 'images': {'pearson': (0.43191546366640104, 1.95211625985353e-35), 'spearman': SpearmanrResult(correlation=0.43309890127134026, pvalue=1.2156227146774e-35), 'nsamples': 750}, 'OnWN': {'pearson': (0.6977728684611177, 1.738594104089402e-110), 'spearman': SpearmanrResult(correlation=0.7209761024148984, pvalue=2.8580113346045624e-121), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6670184579009216, 1.0735926132625847e-97), 'spearman': SpearmanrResult(correlation=0.588574431450969, pvalue=4.127864639464488e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5695426563108973, 'wmean': 0.5697510392522539}, 'spearman': {'mean': 0.5467933798605727, 'wmean': 0.5464705646069203}}}, 'STS15': {'answers-forums': {'pearson': (0.5931354490340288, 5.2851661123871735e-37), 'spearman': SpearmanrResult(correlation=0.5961946150554808, pvalue=1.8405105064354963e-37), 'nsamples': 375}, 'answers-students': {'pearson': (0.5936650947407499, 1.2865027589212489e-72), 'spearman': SpearmanrResult(correlation=0.6128044494565882, pvalue=1.577898460210915e-78), 'nsamples': 750}, 'belief': {'pearson': (0.6926102355214029, 7.145054349637188e-55), 'spearman': SpearmanrResult(correlation=0.7215043846279858, pvalue=1.6292464312163388e-61), 'nsamples': 375}, 'headlines': {'pearson': (0.6364431957103518, 2.056929123929693e-86), 'spearman': SpearmanrResult(correlation=0.6284001387579632, pvalue=1.1820640352963218e-83), 'nsamples': 750}, 'images': {'pearson': (0.6317730426904485, 8.421517625152642e-85), 'spearman': SpearmanrResult(correlation=0.6412043778238958, pvalue=4.3720808884194143e-88), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6295254035393965, 'wmean': 0.6261885438548166}, 'spearman': {'mean': 0.6400215931443828, 'wmean': 0.6353146164700451}}}, 'STS16': {'answer-answer': {'pearson': (0.5547949170060706, 6.64199712751142e-22), 'spearman': SpearmanrResult(correlation=0.539198663163022, pvalue=1.4683960436427394e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6461203225350568, 8.11170002083837e-31), 'spearman': SpearmanrResult(correlation=0.6554579242231074, pvalue=5.917420672452006e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7670784303649677, 7.680977758156366e-46), 'spearman': SpearmanrResult(correlation=0.7782677938374732, pvalue=5.679419665236807e-48), 'nsamples': 230}, 'postediting': {'pearson': (0.8073122033623287, 2.3440705018490744e-57), 'spearman': SpearmanrResult(correlation=0.8304533685822956, pvalue=2.009366649806199e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.4377921623098922, 3.3859021718555937e-11), 'spearman': SpearmanrResult(correlation=0.43050297360406875, pvalue=7.712692379408209e-11), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6426196071156632, 'wmean': 0.646469348868372}, 'spearman': {'mean': 0.6467761446819934, 'wmean': 0.650735935560633}}}, 'MR': {'devacc': 81.28, 'acc': 80.46, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.35, 'acc': 86.23, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.08, 'acc': 88.39, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.49, 'acc': 94.95, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.47, 'acc': 85.5, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.69, 'acc': 44.93, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 84.26, 'acc': 91.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.93, 'acc': 73.33, 'f1': 80.31, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.2, 'acc': 76.68, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8092191887799094, 'pearson': 0.8085801008942196, 'spearman': 0.7411812882756645, 'mse': 0.3533752033657978, 'yhat': array([3.0136602 , 3.76507893, 1.66455727, ..., 3.17183616, 4.63199878,
       4.71425885]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7005391109290238, 'pearson': 0.6565506226440389, 'spearman': 0.6497495904346781, 'mse': 1.5007045658522677, 'yhat': array([1.35970011, 1.71370868, 2.61022639, ..., 3.77048035, 3.69078057,
       3.3895704 ]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 82.02, 'acc': 82.62, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 54.73, 'acc': 55.03, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 34.66, 'acc': 34.49, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.96, 'acc': 70.71, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.85, 'acc': 89.34, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.89, 'acc': 89.08, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.26, 'acc': 85.21, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 81.27, 'acc': 81.07, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.73, 'acc': 64.73, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.2, 'acc': 69.46, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 04:16:24,725 : ********************************************************************************
2019-02-13 04:16:24,725 : ********************************************************************************
2019-02-13 04:16:24,725 : ********************************************************************************
2019-02-13 04:16:24,725 : layer 11
2019-02-13 04:16:24,725 : ********************************************************************************
2019-02-13 04:16:24,725 : ********************************************************************************
2019-02-13 04:16:24,725 : ********************************************************************************
2019-02-13 04:16:24,812 : ***** Transfer task : STS12 *****


2019-02-13 04:16:24,825 : loading BERT mode bert-base-uncased
2019-02-13 04:16:24,825 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:16:24,842 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:16:24,842 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoj4rck6q
2019-02-13 04:16:27,278 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:16:31,308 : MSRpar : pearson = 0.3727, spearman = 0.4188
2019-02-13 04:16:33,148 : MSRvid : pearson = 0.4332, spearman = 0.4563
2019-02-13 04:16:34,464 : SMTeuroparl : pearson = 0.5116, spearman = 0.6016
2019-02-13 04:16:36,734 : surprise.OnWN : pearson = 0.5700, spearman = 0.5780
2019-02-13 04:16:37,995 : surprise.SMTnews : pearson = 0.5530, spearman = 0.5570
2019-02-13 04:16:37,995 : ALL (weighted average) : Pearson = 0.4786,             Spearman = 0.5110
2019-02-13 04:16:37,995 : ALL (average) : Pearson = 0.4881,             Spearman = 0.5223

2019-02-13 04:16:37,996 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 04:16:38,005 : loading BERT mode bert-base-uncased
2019-02-13 04:16:38,005 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:16:38,023 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:16:38,023 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvr47ue2_
2019-02-13 04:16:40,462 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:16:42,842 : FNWN : pearson = 0.3218, spearman = 0.3989
2019-02-13 04:16:44,932 : headlines : pearson = 0.6145, spearman = 0.5847
2019-02-13 04:16:46,617 : OnWN : pearson = 0.5643, spearman = 0.5790
2019-02-13 04:16:46,617 : ALL (weighted average) : Pearson = 0.5589,             Spearman = 0.5592
2019-02-13 04:16:46,617 : ALL (average) : Pearson = 0.5002,             Spearman = 0.5209

2019-02-13 04:16:46,617 : ***** Transfer task : STS14 *****


2019-02-13 04:16:46,632 : loading BERT mode bert-base-uncased
2019-02-13 04:16:46,632 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:16:46,649 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:16:46,650 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_rx3ts06
2019-02-13 04:16:49,086 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:16:52,091 : deft-forum : pearson = 0.3093, spearman = 0.3024
2019-02-13 04:16:53,502 : deft-news : pearson = 0.7619, spearman = 0.7241
2019-02-13 04:16:56,016 : headlines : pearson = 0.5811, spearman = 0.5296
2019-02-13 04:16:58,633 : images : pearson = 0.4476, spearman = 0.4436
2019-02-13 04:17:01,475 : OnWN : pearson = 0.6964, spearman = 0.7190
2019-02-13 04:17:04,715 : tweet-news : pearson = 0.6447, spearman = 0.5694
2019-02-13 04:17:04,716 : ALL (weighted average) : Pearson = 0.5720,             Spearman = 0.5465
2019-02-13 04:17:04,716 : ALL (average) : Pearson = 0.5735,             Spearman = 0.5480

2019-02-13 04:17:04,716 : ***** Transfer task : STS15 *****


2019-02-13 04:17:04,777 : loading BERT mode bert-base-uncased
2019-02-13 04:17:04,777 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:17:04,795 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:17:04,795 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjh3szta9
2019-02-13 04:17:07,239 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:17:10,000 : answers-forums : pearson = 0.5766, spearman = 0.5893
2019-02-13 04:17:12,418 : answers-students : pearson = 0.5470, spearman = 0.5735
2019-02-13 04:17:13,939 : belief : pearson = 0.6770, spearman = 0.7064
2019-02-13 04:17:15,665 : headlines : pearson = 0.6408, spearman = 0.6345
2019-02-13 04:17:17,335 : images : pearson = 0.6345, spearman = 0.6435
2019-02-13 04:17:17,335 : ALL (weighted average) : Pearson = 0.6123,             Spearman = 0.6249
2019-02-13 04:17:17,335 : ALL (average) : Pearson = 0.6152,             Spearman = 0.6294

2019-02-13 04:17:17,336 : ***** Transfer task : STS16 *****


2019-02-13 04:17:17,378 : loading BERT mode bert-base-uncased
2019-02-13 04:17:17,379 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:17:17,429 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:17:17,429 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0_fd3b5v
2019-02-13 04:17:19,946 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:17:22,128 : answer-answer : pearson = 0.5358, spearman = 0.5214
2019-02-13 04:17:22,713 : headlines : pearson = 0.6446, spearman = 0.6519
2019-02-13 04:17:23,349 : plagiarism : pearson = 0.7591, spearman = 0.7712
2019-02-13 04:17:24,283 : postediting : pearson = 0.8021, spearman = 0.8328
2019-02-13 04:17:24,825 : question-question : pearson = 0.4799, spearman = 0.4727
2019-02-13 04:17:24,825 : ALL (weighted average) : Pearson = 0.6469,             Spearman = 0.6527
2019-02-13 04:17:24,825 : ALL (average) : Pearson = 0.6443,             Spearman = 0.6500

2019-02-13 04:17:24,825 : ***** Transfer task : MR *****


2019-02-13 04:17:24,842 : loading BERT mode bert-base-uncased
2019-02-13 04:17:24,842 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:17:24,862 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:17:24,862 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjlj9t842
2019-02-13 04:17:27,357 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:17:28,861 : Generating sentence embeddings
2019-02-13 04:17:49,123 : Generated sentence embeddings
2019-02-13 04:17:49,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:18:16,886 : Best param found at split 1: l2reg = 1e-05                 with score 80.85
2019-02-13 04:18:53,646 : Best param found at split 2: l2reg = 0.0001                 with score 81.03
2019-02-13 04:19:21,704 : Best param found at split 3: l2reg = 0.001                 with score 81.3
2019-02-13 04:19:47,224 : Best param found at split 4: l2reg = 1e-05                 with score 80.66
2019-02-13 04:20:07,319 : Best param found at split 5: l2reg = 0.001                 with score 81.11
2019-02-13 04:20:08,627 : Dev acc : 80.99 Test acc : 80.2

2019-02-13 04:20:08,628 : ***** Transfer task : CR *****


2019-02-13 04:20:08,635 : loading BERT mode bert-base-uncased
2019-02-13 04:20:08,635 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:20:08,690 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:20:08,690 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy8jlne12
2019-02-13 04:20:11,122 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:20:12,634 : Generating sentence embeddings
2019-02-13 04:20:17,892 : Generated sentence embeddings
2019-02-13 04:20:17,892 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:20:21,557 : Best param found at split 1: l2reg = 0.01                 with score 86.82
2019-02-13 04:20:29,500 : Best param found at split 2: l2reg = 0.001                 with score 86.92
2019-02-13 04:20:38,237 : Best param found at split 3: l2reg = 1e-05                 with score 87.22
2019-02-13 04:20:43,220 : Best param found at split 4: l2reg = 0.01                 with score 87.06
2019-02-13 04:20:51,596 : Best param found at split 5: l2reg = 0.0001                 with score 87.36
2019-02-13 04:20:51,941 : Dev acc : 87.08 Test acc : 85.38

2019-02-13 04:20:51,941 : ***** Transfer task : MPQA *****


2019-02-13 04:20:51,947 : loading BERT mode bert-base-uncased
2019-02-13 04:20:51,947 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:20:51,966 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:20:51,966 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9uwhazfw
2019-02-13 04:20:54,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:20:55,913 : Generating sentence embeddings
2019-02-13 04:21:05,926 : Generated sentence embeddings
2019-02-13 04:21:05,926 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:21:35,277 : Best param found at split 1: l2reg = 0.001                 with score 87.94
2019-02-13 04:22:06,619 : Best param found at split 2: l2reg = 0.01                 with score 87.74
2019-02-13 04:22:34,080 : Best param found at split 3: l2reg = 0.01                 with score 87.22
2019-02-13 04:22:55,835 : Best param found at split 4: l2reg = 1e-05                 with score 88.41
2019-02-13 04:23:15,883 : Best param found at split 5: l2reg = 0.0001                 with score 87.76
2019-02-13 04:23:17,194 : Dev acc : 87.81 Test acc : 88.22

2019-02-13 04:23:17,195 : ***** Transfer task : SUBJ *****


2019-02-13 04:23:17,210 : loading BERT mode bert-base-uncased
2019-02-13 04:23:17,210 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:23:17,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:23:17,230 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd4voan9_
2019-02-13 04:23:19,663 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:23:21,128 : Generating sentence embeddings
2019-02-13 04:23:40,449 : Generated sentence embeddings
2019-02-13 04:23:40,450 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 04:24:05,344 : Best param found at split 1: l2reg = 0.001                 with score 95.64
2019-02-13 04:24:36,756 : Best param found at split 2: l2reg = 0.0001                 with score 95.55
2019-02-13 04:25:15,258 : Best param found at split 3: l2reg = 0.001                 with score 95.35
2019-02-13 04:25:45,732 : Best param found at split 4: l2reg = 1e-05                 with score 95.69
2019-02-13 04:26:09,408 : Best param found at split 5: l2reg = 0.001                 with score 95.39
2019-02-13 04:26:10,701 : Dev acc : 95.52 Test acc : 95.14

2019-02-13 04:26:10,702 : ***** Transfer task : SST Binary classification *****


2019-02-13 04:26:10,835 : loading BERT mode bert-base-uncased
2019-02-13 04:26:10,835 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:26:10,859 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:26:10,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4y7xhget
2019-02-13 04:26:13,291 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:26:14,739 : Computing embedding for train
2019-02-13 04:27:36,414 : Computed train embeddings
2019-02-13 04:27:36,414 : Computing embedding for dev
2019-02-13 04:27:37,804 : Computed dev embeddings
2019-02-13 04:27:37,804 : Computing embedding for test
2019-02-13 04:27:40,863 : Computed test embeddings
2019-02-13 04:27:40,863 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:28:18,540 : [('reg:1e-05', 85.32), ('reg:0.0001', 85.21), ('reg:0.001', 85.44), ('reg:0.01', 85.09)]
2019-02-13 04:28:18,541 : Validation : best param found is reg = 0.001 with score             85.44
2019-02-13 04:28:18,541 : Evaluating...
2019-02-13 04:28:29,401 : 
Dev acc : 85.44 Test acc : 85.34 for             SST Binary classification

2019-02-13 04:28:29,402 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 04:28:29,456 : loading BERT mode bert-base-uncased
2019-02-13 04:28:29,456 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:28:29,512 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:28:29,512 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgsjtzvqs
2019-02-13 04:28:31,953 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:28:33,513 : Computing embedding for train
2019-02-13 04:28:52,778 : Computed train embeddings
2019-02-13 04:28:52,778 : Computing embedding for dev
2019-02-13 04:28:55,252 : Computed dev embeddings
2019-02-13 04:28:55,252 : Computing embedding for test
2019-02-13 04:29:00,530 : Computed test embeddings
2019-02-13 04:29:00,530 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:29:11,088 : [('reg:1e-05', 45.14), ('reg:0.0001', 44.87), ('reg:0.001', 45.87), ('reg:0.01', 44.5)]
2019-02-13 04:29:11,088 : Validation : best param found is reg = 0.001 with score             45.87
2019-02-13 04:29:11,088 : Evaluating...
2019-02-13 04:29:13,664 : 
Dev acc : 45.87 Test acc : 46.11 for             SST Fine-Grained classification

2019-02-13 04:29:13,665 : ***** Transfer task : TREC *****


2019-02-13 04:29:13,677 : loading BERT mode bert-base-uncased
2019-02-13 04:29:13,677 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:29:13,696 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:29:13,696 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpik8pnewr
2019-02-13 04:29:16,131 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:29:27,474 : Computed train embeddings
2019-02-13 04:29:28,475 : Computed test embeddings
2019-02-13 04:29:28,475 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 04:29:51,329 : [('reg:1e-05', 84.1), ('reg:0.0001', 84.21), ('reg:0.001', 83.73), ('reg:0.01', 75.47)]
2019-02-13 04:29:51,330 : Cross-validation : best param found is reg = 0.0001             with score 84.21
2019-02-13 04:29:51,330 : Evaluating...
2019-02-13 04:29:52,398 : 
Dev acc : 84.21 Test acc : 91.6             for TREC

2019-02-13 04:29:52,399 : ***** Transfer task : MRPC *****


2019-02-13 04:29:52,449 : loading BERT mode bert-base-uncased
2019-02-13 04:29:52,449 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:29:52,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:29:52,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1aux94s5
2019-02-13 04:29:54,904 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:29:56,374 : Computing embedding for train
2019-02-13 04:30:12,936 : Computed train embeddings
2019-02-13 04:30:12,936 : Computing embedding for test
2019-02-13 04:30:20,399 : Computed test embeddings
2019-02-13 04:30:20,415 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 04:30:34,219 : [('reg:1e-05', 73.72), ('reg:0.0001', 73.5), ('reg:0.001', 73.26), ('reg:0.01', 72.15)]
2019-02-13 04:30:34,219 : Cross-validation : best param found is reg = 1e-05             with score 73.72
2019-02-13 04:30:34,219 : Evaluating...
2019-02-13 04:30:35,303 : Dev acc : 73.72 Test acc 73.33; Test F1 80.57 for MRPC.

2019-02-13 04:30:35,303 : ***** Transfer task : SICK-Entailment*****


2019-02-13 04:30:35,327 : loading BERT mode bert-base-uncased
2019-02-13 04:30:35,327 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:30:35,381 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:30:35,382 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy_usl8uf
2019-02-13 04:30:37,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:30:39,332 : Computing embedding for train
2019-02-13 04:30:53,482 : Computed train embeddings
2019-02-13 04:30:53,482 : Computing embedding for dev
2019-02-13 04:30:55,315 : Computed dev embeddings
2019-02-13 04:30:55,315 : Computing embedding for test
2019-02-13 04:31:11,198 : Computed test embeddings
2019-02-13 04:31:11,226 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:31:14,740 : [('reg:1e-05', 76.8), ('reg:0.0001', 75.4), ('reg:0.001', 75.4), ('reg:0.01', 75.6)]
2019-02-13 04:31:14,740 : Validation : best param found is reg = 1e-05 with score             76.8
2019-02-13 04:31:14,741 : Evaluating...
2019-02-13 04:31:15,736 : 
Dev acc : 76.8 Test acc : 77.23 for                        SICK entailment

2019-02-13 04:31:15,736 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 04:31:15,763 : loading BERT mode bert-base-uncased
2019-02-13 04:31:15,764 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:31:15,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:31:15,783 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvlxbwcpw
2019-02-13 04:31:18,217 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:31:19,684 : Computing embedding for train
2019-02-13 04:31:32,324 : Computed train embeddings
2019-02-13 04:31:32,324 : Computing embedding for dev
2019-02-13 04:31:33,898 : Computed dev embeddings
2019-02-13 04:31:33,898 : Computing embedding for test
2019-02-13 04:31:45,759 : Computed test embeddings
2019-02-13 04:32:16,919 : Dev : Pearson 0.8057024050424405
2019-02-13 04:32:16,919 : Test : Pearson 0.8129063299277852 Spearman 0.7428656341923232 MSE 0.34691890003171844                        for SICK Relatedness

2019-02-13 04:32:16,920 : 

***** Transfer task : STSBenchmark*****


2019-02-13 04:32:17,026 : loading BERT mode bert-base-uncased
2019-02-13 04:32:17,026 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:32:17,046 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:32:17,046 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4f85988s
2019-02-13 04:32:19,477 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:32:20,970 : Computing embedding for train
2019-02-13 04:32:35,485 : Computed train embeddings
2019-02-13 04:32:35,485 : Computing embedding for dev
2019-02-13 04:32:39,643 : Computed dev embeddings
2019-02-13 04:32:39,643 : Computing embedding for test
2019-02-13 04:32:43,292 : Computed test embeddings
2019-02-13 04:33:52,204 : Dev : Pearson 0.7190162624647348
2019-02-13 04:33:52,204 : Test : Pearson 0.6504388803995269 Spearman 0.64292642411981 MSE 1.48381559769896                        for SICK Relatedness

2019-02-13 04:33:52,204 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 04:33:52,460 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 04:33:52,470 : loading BERT mode bert-base-uncased
2019-02-13 04:33:52,470 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:33:52,566 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:33:52,566 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn_aim3dq
2019-02-13 04:33:55,007 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:33:56,593 : Computing embeddings for train/dev/test
2019-02-13 04:37:09,535 : Computed embeddings
2019-02-13 04:37:09,535 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:38:36,996 : [('reg:1e-05', 78.63), ('reg:0.0001', 75.42), ('reg:0.001', 70.39), ('reg:0.01', 55.2)]
2019-02-13 04:38:36,996 : Validation : best param found is reg = 1e-05 with score             78.63
2019-02-13 04:38:36,996 : Evaluating...
2019-02-13 04:38:59,774 : 
Dev acc : 78.6 Test acc : 79.2 for LENGTH classification

2019-02-13 04:38:59,781 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 04:39:00,276 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 04:39:00,321 : loading BERT mode bert-base-uncased
2019-02-13 04:39:00,321 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:39:00,350 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:39:00,350 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsxil2i76
2019-02-13 04:39:02,790 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:39:04,311 : Computing embeddings for train/dev/test
2019-02-13 04:41:23,466 : Computed embeddings
2019-02-13 04:41:23,466 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:42:41,184 : [('reg:1e-05', 53.21), ('reg:0.0001', 31.31), ('reg:0.001', 4.76), ('reg:0.01', 1.09)]
2019-02-13 04:42:41,184 : Validation : best param found is reg = 1e-05 with score             53.21
2019-02-13 04:42:41,184 : Evaluating...
2019-02-13 04:43:04,452 : 
Dev acc : 53.2 Test acc : 52.9 for WORDCONTENT classification

2019-02-13 04:43:04,460 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 04:43:04,969 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 04:43:05,033 : loading BERT mode bert-base-uncased
2019-02-13 04:43:05,033 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:43:05,058 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:43:05,058 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg_4kl3a7
2019-02-13 04:43:07,528 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:43:08,984 : Computing embeddings for train/dev/test
2019-02-13 04:45:07,807 : Computed embeddings
2019-02-13 04:45:07,807 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:45:57,733 : [('reg:1e-05', 34.0), ('reg:0.0001', 33.98), ('reg:0.001', 33.37), ('reg:0.01', 28.79)]
2019-02-13 04:45:57,733 : Validation : best param found is reg = 1e-05 with score             34.0
2019-02-13 04:45:57,733 : Evaluating...
2019-02-13 04:46:09,857 : 
Dev acc : 34.0 Test acc : 33.0 for DEPTH classification

2019-02-13 04:46:09,858 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 04:46:10,265 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 04:46:10,328 : loading BERT mode bert-base-uncased
2019-02-13 04:46:10,328 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:46:10,356 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:46:10,356 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp66hqlejf
2019-02-13 04:46:12,786 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:46:14,246 : Computing embeddings for train/dev/test
2019-02-13 04:48:43,665 : Computed embeddings
2019-02-13 04:48:43,665 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:49:59,184 : [('reg:1e-05', 70.47), ('reg:0.0001', 70.25), ('reg:0.001', 63.7), ('reg:0.01', 52.97)]
2019-02-13 04:49:59,184 : Validation : best param found is reg = 1e-05 with score             70.47
2019-02-13 04:49:59,184 : Evaluating...
2019-02-13 04:50:16,445 : 
Dev acc : 70.5 Test acc : 70.0 for TOPCONSTITUENTS classification

2019-02-13 04:50:16,446 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 04:50:16,797 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 04:50:16,863 : loading BERT mode bert-base-uncased
2019-02-13 04:50:16,863 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:50:16,984 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:50:16,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0jwq977f
2019-02-13 04:50:19,424 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:50:20,913 : Computing embeddings for train/dev/test
2019-02-13 04:53:11,795 : Computed embeddings
2019-02-13 04:53:11,796 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:54:34,322 : [('reg:1e-05', 89.26), ('reg:0.0001', 89.39), ('reg:0.001', 89.29), ('reg:0.01', 88.06)]
2019-02-13 04:54:34,322 : Validation : best param found is reg = 0.0001 with score             89.39
2019-02-13 04:54:34,322 : Evaluating...
2019-02-13 04:54:54,734 : 
Dev acc : 89.4 Test acc : 88.8 for BIGRAMSHIFT classification

2019-02-13 04:54:54,735 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 04:54:55,114 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 04:54:55,179 : loading BERT mode bert-base-uncased
2019-02-13 04:54:55,180 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:54:55,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:54:55,301 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg7edzwo2
2019-02-13 04:54:57,725 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:54:59,209 : Computing embeddings for train/dev/test
2019-02-13 04:57:32,497 : Computed embeddings
2019-02-13 04:57:32,497 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 04:58:48,369 : [('reg:1e-05', 89.78), ('reg:0.0001', 89.99), ('reg:0.001', 90.34), ('reg:0.01', 90.07)]
2019-02-13 04:58:48,370 : Validation : best param found is reg = 0.001 with score             90.34
2019-02-13 04:58:48,370 : Evaluating...
2019-02-13 04:59:04,598 : 
Dev acc : 90.3 Test acc : 89.3 for TENSE classification

2019-02-13 04:59:04,599 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 04:59:05,185 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 04:59:05,250 : loading BERT mode bert-base-uncased
2019-02-13 04:59:05,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 04:59:05,279 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 04:59:05,279 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpytzw703l
2019-02-13 04:59:07,711 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 04:59:09,206 : Computing embeddings for train/dev/test
2019-02-13 05:01:57,633 : Computed embeddings
2019-02-13 05:01:57,633 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:03:22,870 : [('reg:1e-05', 84.7), ('reg:0.0001', 84.69), ('reg:0.001', 84.71), ('reg:0.01', 82.95)]
2019-02-13 05:03:22,870 : Validation : best param found is reg = 0.001 with score             84.71
2019-02-13 05:03:22,870 : Evaluating...
2019-02-13 05:03:42,237 : 
Dev acc : 84.7 Test acc : 84.7 for SUBJNUMBER classification

2019-02-13 05:03:42,238 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 05:03:42,828 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 05:03:42,895 : loading BERT mode bert-base-uncased
2019-02-13 05:03:42,895 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:03:42,921 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:03:42,921 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmwt4tvm8
2019-02-13 05:03:45,354 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:03:46,835 : Computing embeddings for train/dev/test
2019-02-13 05:06:31,740 : Computed embeddings
2019-02-13 05:06:31,740 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:07:32,570 : [('reg:1e-05', 80.05), ('reg:0.0001', 79.85), ('reg:0.001', 80.1), ('reg:0.01', 79.24)]
2019-02-13 05:07:32,570 : Validation : best param found is reg = 0.001 with score             80.1
2019-02-13 05:07:32,570 : Evaluating...
2019-02-13 05:07:49,916 : 
Dev acc : 80.1 Test acc : 80.8 for OBJNUMBER classification

2019-02-13 05:07:49,917 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 05:07:50,334 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 05:07:50,402 : loading BERT mode bert-base-uncased
2019-02-13 05:07:50,403 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:07:50,429 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:07:50,430 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl5h3ytag
2019-02-13 05:07:52,880 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:07:54,385 : Computing embeddings for train/dev/test
2019-02-13 05:10:31,879 : Computed embeddings
2019-02-13 05:10:31,879 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:11:42,456 : [('reg:1e-05', 65.14), ('reg:0.0001', 65.12), ('reg:0.001', 65.1), ('reg:0.01', 64.23)]
2019-02-13 05:11:42,456 : Validation : best param found is reg = 1e-05 with score             65.14
2019-02-13 05:11:42,457 : Evaluating...
2019-02-13 05:11:56,480 : 
Dev acc : 65.1 Test acc : 65.1 for ODDMANOUT classification

2019-02-13 05:11:56,481 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 05:11:56,888 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 05:11:56,964 : loading BERT mode bert-base-uncased
2019-02-13 05:11:56,964 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:11:57,089 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:11:57,089 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn00l30ip
2019-02-13 05:11:59,526 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:12:00,975 : Computing embeddings for train/dev/test
2019-02-13 05:14:21,488 : Computed embeddings
2019-02-13 05:14:21,488 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:15:46,935 : [('reg:1e-05', 69.41), ('reg:0.0001', 69.37), ('reg:0.001', 68.91), ('reg:0.01', 66.93)]
2019-02-13 05:15:46,936 : Validation : best param found is reg = 1e-05 with score             69.41
2019-02-13 05:15:46,936 : Evaluating...
2019-02-13 05:16:03,888 : 
Dev acc : 69.4 Test acc : 69.4 for COORDINATIONINVERSION classification

2019-02-13 05:16:03,890 : {'STS12': {'MSRpar': {'pearson': (0.3727095667593454, 3.9596299931259383e-26), 'spearman': SpearmanrResult(correlation=0.41876266143179286, pvalue=3.331594344982482e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4332361253636729, 1.15051849264387e-35), 'spearman': SpearmanrResult(correlation=0.45630312680945373, pvalue=7.639038079993273e-40), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5116008319491429, 5.580004510886832e-32), 'spearman': SpearmanrResult(correlation=0.6016258362864673, pvalue=1.5830708708838964e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5699934004347451, 7.820989789068989e-66), 'spearman': SpearmanrResult(correlation=0.5779932288617731, pvalue=4.5978848132958986e-68), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5530105138953818, 2.4503858273253974e-33), 'spearman': SpearmanrResult(correlation=0.5570492843584547, pvalue=6.7229177023961166e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48811008768045766, 'wmean': 0.4785811764244647}, 'spearman': {'mean': 0.5223468275495884, 'wmean': 0.5110048861460028}}}, 'STS13': {'FNWN': {'pearson': (0.32177085147682594, 6.33378417332863e-06), 'spearman': SpearmanrResult(correlation=0.3989381468768162, pvalue=1.300883157892006e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6145097960792294, 4.483049894079435e-79), 'spearman': SpearmanrResult(correlation=0.5847331997850223, pvalue=5.4294827068766644e-70), 'nsamples': 750}, 'OnWN': {'pearson': (0.5643187054722828, 1.7238118664274123e-48), 'spearman': SpearmanrResult(correlation=0.5790412431574151, pvalue=1.547884460804139e-51), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5001997843427793, 'wmean': 0.5588532211723286}, 'spearman': {'mean': 0.520904196606418, 'wmean': 0.5591942313398632}}}, 'STS14': {'deft-forum': {'pearson': (0.3093400814837979, 1.9583031906093414e-11), 'spearman': SpearmanrResult(correlation=0.30243175972397235, pvalue=5.686661845212124e-11), 'nsamples': 450}, 'deft-news': {'pearson': (0.7618896879419697, 3.735220035716604e-58), 'spearman': SpearmanrResult(correlation=0.7240513284811, pvalue=5.404936503661116e-50), 'nsamples': 300}, 'headlines': {'pearson': (0.5810527986442682, 6.208964613620758e-69), 'spearman': SpearmanrResult(correlation=0.5296322025963927, pvalue=1.847553501231274e-55), 'nsamples': 750}, 'images': {'pearson': (0.4475822620251421, 3.1648585612655135e-38), 'spearman': SpearmanrResult(correlation=0.4436376708236111, pvalue=1.6467927012255007e-37), 'nsamples': 750}, 'OnWN': {'pearson': (0.6964211639659849, 6.862688561044599e-110), 'spearman': SpearmanrResult(correlation=0.7189876175740287, pvalue=2.64796373502888e-120), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6446591974386136, 2.5606856648023767e-89), 'spearman': SpearmanrResult(correlation=0.5693918808146343, pvalue=1.1442643118026033e-65), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5734908652499627, 'wmean': 0.5720150692282151}, 'spearman': {'mean': 0.5480220766689565, 'wmean': 0.5465457918070981}}}, 'STS15': {'answers-forums': {'pearson': (0.5766060474551941, 1.3057866187078612e-34), 'spearman': SpearmanrResult(correlation=0.5892628940732605, pvalue=1.9770403071093515e-36), 'nsamples': 375}, 'answers-students': {'pearson': (0.5470171728937847, 9.359394986207948e-60), 'spearman': SpearmanrResult(correlation=0.573504508317169, pvalue=8.351507493699605e-67), 'nsamples': 750}, 'belief': {'pearson': (0.6770424013428654, 1.3089468271549008e-51), 'spearman': SpearmanrResult(correlation=0.7064090963233781, pvalue=6.0685676584761764e-58), 'nsamples': 375}, 'headlines': {'pearson': (0.6407857340589851, 6.150937849564486e-88), 'spearman': SpearmanrResult(correlation=0.6345383917516699, pvalue=9.42164922708702e-86), 'nsamples': 750}, 'images': {'pearson': (0.634463377125025, 1.0001373075395758e-85), 'spearman': SpearmanrResult(correlation=0.6435299597822182, pvalue=6.499903808466876e-89), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6151829465751708, 'wmean': 0.612272627119206}, 'spearman': {'mean': 0.6294489700495391, 'wmean': 0.624852213762344}}}, 'STS16': {'answer-answer': {'pearson': (0.5357751252758719, 2.8362120679404524e-20), 'spearman': SpearmanrResult(correlation=0.5214389997915273, pvalue=4.1239509444388844e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.6445526917826722, 1.2477173691309513e-30), 'spearman': SpearmanrResult(correlation=0.6518797553957988, pvalue=1.6313889009854508e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7591483129615296, 2.1122113712595788e-44), 'spearman': SpearmanrResult(correlation=0.7712375019184672, pvalue=1.2807283570216581e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.8020770615916597, 4.2559296082647035e-56), 'spearman': SpearmanrResult(correlation=0.8328135441479744, pvalue=4.29435254489274e-64), 'nsamples': 244}, 'question-question': {'pearson': (0.47985988747192027, 1.9584524104617907e-13), 'spearman': SpearmanrResult(correlation=0.47269092886229475, pvalue=4.95690806614973e-13), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6442826158167307, 'wmean': 0.6468652053668676}, 'spearman': {'mean': 0.6500121460232124, 'wmean': 0.6527378578298269}}}, 'MR': {'devacc': 80.99, 'acc': 80.2, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.08, 'acc': 85.38, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.81, 'acc': 88.22, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.52, 'acc': 95.14, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 85.44, 'acc': 85.34, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.87, 'acc': 46.11, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 84.21, 'acc': 91.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.72, 'acc': 73.33, 'f1': 80.57, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.8, 'acc': 77.23, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8057024050424405, 'pearson': 0.8129063299277852, 'spearman': 0.7428656341923232, 'mse': 0.34691890003171844, 'yhat': array([3.21954153, 3.83916256, 1.49827067, ..., 3.39341568, 4.67290041,
       4.79184747]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7190162624647348, 'pearson': 0.6504388803995269, 'spearman': 0.64292642411981, 'mse': 1.48381559769896, 'yhat': array([1.62349783, 1.19161571, 2.82833047, ..., 3.81651634, 3.41112515,
       3.51477407]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 78.63, 'acc': 79.16, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 53.21, 'acc': 52.92, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 34.0, 'acc': 33.04, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.47, 'acc': 70.01, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.39, 'acc': 88.76, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.34, 'acc': 89.29, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.71, 'acc': 84.71, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.1, 'acc': 80.82, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.14, 'acc': 65.07, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.41, 'acc': 69.37, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 05:16:03,890 : ********************************************************************************
2019-02-13 05:16:03,890 : ********************************************************************************
2019-02-13 05:16:03,890 : ********************************************************************************
2019-02-13 05:16:03,890 : layer 12
2019-02-13 05:16:03,890 : ********************************************************************************
2019-02-13 05:16:03,890 : ********************************************************************************
2019-02-13 05:16:03,890 : ********************************************************************************
2019-02-13 05:16:03,977 : ***** Transfer task : STS12 *****


2019-02-13 05:16:03,989 : loading BERT mode bert-base-uncased
2019-02-13 05:16:03,989 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:16:04,006 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:16:04,007 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpig7nl87p
2019-02-13 05:16:06,440 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:16:10,113 : MSRpar : pearson = 0.3508, spearman = 0.4008
2019-02-13 05:16:11,312 : MSRvid : pearson = 0.3575, spearman = 0.3877
2019-02-13 05:16:12,233 : SMTeuroparl : pearson = 0.4790, spearman = 0.5790
2019-02-13 05:16:13,855 : surprise.OnWN : pearson = 0.5792, spearman = 0.5796
2019-02-13 05:16:14,741 : surprise.SMTnews : pearson = 0.5833, spearman = 0.5418
2019-02-13 05:16:14,741 : ALL (weighted average) : Pearson = 0.4563,             Spearman = 0.4852
2019-02-13 05:16:14,741 : ALL (average) : Pearson = 0.4700,             Spearman = 0.4978

2019-02-13 05:16:14,741 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 05:16:14,751 : loading BERT mode bert-base-uncased
2019-02-13 05:16:14,751 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:16:14,769 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:16:14,769 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2ewz_t5d
2019-02-13 05:16:17,204 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:16:19,312 : FNWN : pearson = 0.4203, spearman = 0.4471
2019-02-13 05:16:20,632 : headlines : pearson = 0.6471, spearman = 0.6215
2019-02-13 05:16:21,621 : OnWN : pearson = 0.5585, spearman = 0.5710
2019-02-13 05:16:21,621 : ALL (weighted average) : Pearson = 0.5854,             Spearman = 0.5806
2019-02-13 05:16:21,621 : ALL (average) : Pearson = 0.5420,             Spearman = 0.5465

2019-02-13 05:16:21,622 : ***** Transfer task : STS14 *****


2019-02-13 05:16:21,665 : loading BERT mode bert-base-uncased
2019-02-13 05:16:21,665 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:16:21,682 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:16:21,682 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplixxl1qf
2019-02-13 05:16:24,114 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:16:26,431 : deft-forum : pearson = 0.3352, spearman = 0.3277
2019-02-13 05:16:27,331 : deft-news : pearson = 0.7513, spearman = 0.7168
2019-02-13 05:16:28,751 : headlines : pearson = 0.5995, spearman = 0.5421
2019-02-13 05:16:30,141 : images : pearson = 0.4518, spearman = 0.4449
2019-02-13 05:16:31,544 : OnWN : pearson = 0.6977, spearman = 0.7181
2019-02-13 05:16:33,254 : tweet-news : pearson = 0.6614, spearman = 0.5933
2019-02-13 05:16:33,254 : ALL (weighted average) : Pearson = 0.5824,             Spearman = 0.5564
2019-02-13 05:16:33,254 : ALL (average) : Pearson = 0.5828,             Spearman = 0.5572

2019-02-13 05:16:33,254 : ***** Transfer task : STS15 *****


2019-02-13 05:16:33,286 : loading BERT mode bert-base-uncased
2019-02-13 05:16:33,286 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:16:33,303 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:16:33,304 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzi0n89kq
2019-02-13 05:16:35,736 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:16:38,242 : answers-forums : pearson = 0.6046, spearman = 0.6016
2019-02-13 05:16:39,624 : answers-students : pearson = 0.5642, spearman = 0.5680
2019-02-13 05:16:40,725 : belief : pearson = 0.7071, spearman = 0.7154
2019-02-13 05:16:42,187 : headlines : pearson = 0.6711, spearman = 0.6600
2019-02-13 05:16:43,600 : images : pearson = 0.6188, spearman = 0.6244
2019-02-13 05:16:43,600 : ALL (weighted average) : Pearson = 0.6275,             Spearman = 0.6277
2019-02-13 05:16:43,600 : ALL (average) : Pearson = 0.6332,             Spearman = 0.6339

2019-02-13 05:16:43,600 : ***** Transfer task : STS16 *****


2019-02-13 05:16:43,669 : loading BERT mode bert-base-uncased
2019-02-13 05:16:43,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:16:43,688 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:16:43,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5wntymx8
2019-02-13 05:16:46,123 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:16:48,081 : answer-answer : pearson = 0.5535, spearman = 0.5656
2019-02-13 05:16:48,538 : headlines : pearson = 0.6538, spearman = 0.6575
2019-02-13 05:16:49,071 : plagiarism : pearson = 0.7692, spearman = 0.7857
2019-02-13 05:16:49,857 : postediting : pearson = 0.8058, spearman = 0.8243
2019-02-13 05:16:50,267 : question-question : pearson = 0.5275, spearman = 0.5162
2019-02-13 05:16:50,267 : ALL (weighted average) : Pearson = 0.6637,             Spearman = 0.6721
2019-02-13 05:16:50,267 : ALL (average) : Pearson = 0.6619,             Spearman = 0.6699

2019-02-13 05:16:50,267 : ***** Transfer task : MR *****


2019-02-13 05:16:50,284 : loading BERT mode bert-base-uncased
2019-02-13 05:16:50,284 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:16:50,305 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:16:50,306 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphlrx6qlw
2019-02-13 05:16:52,741 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:16:54,195 : Generating sentence embeddings
2019-02-13 05:17:10,946 : Generated sentence embeddings
2019-02-13 05:17:10,946 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:17:36,607 : Best param found at split 1: l2reg = 0.001                 with score 80.14
2019-02-13 05:18:13,753 : Best param found at split 2: l2reg = 1e-05                 with score 80.47
2019-02-13 05:18:53,995 : Best param found at split 3: l2reg = 1e-05                 with score 80.6
2019-02-13 05:19:23,395 : Best param found at split 4: l2reg = 0.01                 with score 80.25
2019-02-13 05:19:41,730 : Best param found at split 5: l2reg = 0.001                 with score 80.88
2019-02-13 05:19:42,316 : Dev acc : 80.47 Test acc : 79.53

2019-02-13 05:19:42,317 : ***** Transfer task : CR *****


2019-02-13 05:19:42,325 : loading BERT mode bert-base-uncased
2019-02-13 05:19:42,325 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:19:42,344 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:19:42,344 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp98hzlfbj
2019-02-13 05:19:44,778 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:19:46,295 : Generating sentence embeddings
2019-02-13 05:19:51,042 : Generated sentence embeddings
2019-02-13 05:19:51,042 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:19:56,449 : Best param found at split 1: l2reg = 0.01                 with score 86.72
2019-02-13 05:20:06,286 : Best param found at split 2: l2reg = 0.0001                 with score 86.52
2019-02-13 05:20:13,856 : Best param found at split 3: l2reg = 0.001                 with score 87.05
2019-02-13 05:20:19,367 : Best param found at split 4: l2reg = 0.0001                 with score 86.96
2019-02-13 05:20:26,308 : Best param found at split 5: l2reg = 1e-05                 with score 86.89
2019-02-13 05:20:26,678 : Dev acc : 86.83 Test acc : 85.72

2019-02-13 05:20:26,679 : ***** Transfer task : MPQA *****


2019-02-13 05:20:26,685 : loading BERT mode bert-base-uncased
2019-02-13 05:20:26,685 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:20:26,706 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:20:26,706 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5rv9rzzu
2019-02-13 05:20:29,145 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:20:30,595 : Generating sentence embeddings
2019-02-13 05:20:37,377 : Generated sentence embeddings
2019-02-13 05:20:37,378 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:21:04,864 : Best param found at split 1: l2reg = 0.0001                 with score 87.95
2019-02-13 05:21:35,606 : Best param found at split 2: l2reg = 0.0001                 with score 87.28
2019-02-13 05:22:05,754 : Best param found at split 3: l2reg = 0.001                 with score 86.92
2019-02-13 05:22:39,732 : Best param found at split 4: l2reg = 1e-05                 with score 87.98
2019-02-13 05:23:04,663 : Best param found at split 5: l2reg = 0.001                 with score 87.65
2019-02-13 05:23:05,257 : Dev acc : 87.56 Test acc : 87.63

2019-02-13 05:23:05,258 : ***** Transfer task : SUBJ *****


2019-02-13 05:23:05,272 : loading BERT mode bert-base-uncased
2019-02-13 05:23:05,273 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:23:05,294 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:23:05,294 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcxpl_2sf
2019-02-13 05:23:07,731 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:23:09,175 : Generating sentence embeddings
2019-02-13 05:23:27,666 : Generated sentence embeddings
2019-02-13 05:23:27,667 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 05:23:46,278 : Best param found at split 1: l2reg = 0.0001                 with score 95.38
2019-02-13 05:24:01,123 : Best param found at split 2: l2reg = 0.001                 with score 95.5
2019-02-13 05:24:19,588 : Best param found at split 3: l2reg = 1e-05                 with score 95.22
2019-02-13 05:24:51,668 : Best param found at split 4: l2reg = 0.0001                 with score 95.67
2019-02-13 05:25:26,000 : Best param found at split 5: l2reg = 1e-05                 with score 95.38
2019-02-13 05:25:28,053 : Dev acc : 95.43 Test acc : 95.05

2019-02-13 05:25:28,053 : ***** Transfer task : SST Binary classification *****


2019-02-13 05:25:28,180 : loading BERT mode bert-base-uncased
2019-02-13 05:25:28,181 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:25:28,204 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:25:28,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6yjv11bu
2019-02-13 05:25:30,634 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:25:32,220 : Computing embedding for train
2019-02-13 05:27:40,382 : Computed train embeddings
2019-02-13 05:27:40,383 : Computing embedding for dev
2019-02-13 05:27:41,641 : Computed dev embeddings
2019-02-13 05:27:41,641 : Computing embedding for test
2019-02-13 05:27:44,281 : Computed test embeddings
2019-02-13 05:27:44,281 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:28:07,032 : [('reg:1e-05', 86.12), ('reg:0.0001', 86.24), ('reg:0.001', 86.12), ('reg:0.01', 84.17)]
2019-02-13 05:28:07,032 : Validation : best param found is reg = 0.0001 with score             86.24
2019-02-13 05:28:07,032 : Evaluating...
2019-02-13 05:28:10,971 : 
Dev acc : 86.24 Test acc : 85.06 for             SST Binary classification

2019-02-13 05:28:10,971 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 05:28:11,022 : loading BERT mode bert-base-uncased
2019-02-13 05:28:11,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:28:11,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:28:11,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcqvft869
2019-02-13 05:28:13,481 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:28:14,891 : Computing embedding for train
2019-02-13 05:28:27,506 : Computed train embeddings
2019-02-13 05:28:27,506 : Computing embedding for dev
2019-02-13 05:28:29,223 : Computed dev embeddings
2019-02-13 05:28:29,224 : Computing embedding for test
2019-02-13 05:28:32,828 : Computed test embeddings
2019-02-13 05:28:32,829 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:28:38,040 : [('reg:1e-05', 44.14), ('reg:0.0001', 44.23), ('reg:0.001', 44.23), ('reg:0.01', 44.87)]
2019-02-13 05:28:38,040 : Validation : best param found is reg = 0.01 with score             44.87
2019-02-13 05:28:38,040 : Evaluating...
2019-02-13 05:28:39,518 : 
Dev acc : 44.87 Test acc : 46.38 for             SST Fine-Grained classification

2019-02-13 05:28:39,518 : ***** Transfer task : TREC *****


2019-02-13 05:28:39,531 : loading BERT mode bert-base-uncased
2019-02-13 05:28:39,531 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:28:39,550 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:28:39,550 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ebnn6s_
2019-02-13 05:28:42,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:28:50,486 : Computed train embeddings
2019-02-13 05:28:51,107 : Computed test embeddings
2019-02-13 05:28:51,107 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 05:29:06,652 : [('reg:1e-05', 80.37), ('reg:0.0001', 80.28), ('reg:0.001', 79.69), ('reg:0.01', 72.45)]
2019-02-13 05:29:06,652 : Cross-validation : best param found is reg = 1e-05             with score 80.37
2019-02-13 05:29:06,652 : Evaluating...
2019-02-13 05:29:08,042 : 
Dev acc : 80.37 Test acc : 89.8             for TREC

2019-02-13 05:29:08,043 : ***** Transfer task : MRPC *****


2019-02-13 05:29:08,064 : loading BERT mode bert-base-uncased
2019-02-13 05:29:08,065 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:29:08,085 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:29:08,085 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprxsfnkmf
2019-02-13 05:29:10,525 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:29:12,118 : Computing embedding for train
2019-02-13 05:29:30,577 : Computed train embeddings
2019-02-13 05:29:30,577 : Computing embedding for test
2019-02-13 05:29:38,555 : Computed test embeddings
2019-02-13 05:29:38,571 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 05:29:59,956 : [('reg:1e-05', 73.38), ('reg:0.0001', 73.48), ('reg:0.001', 73.01), ('reg:0.01', 73.31)]
2019-02-13 05:29:59,957 : Cross-validation : best param found is reg = 0.0001             with score 73.48
2019-02-13 05:29:59,957 : Evaluating...
2019-02-13 05:30:01,021 : Dev acc : 73.48 Test acc 73.45; Test F1 80.79 for MRPC.

2019-02-13 05:30:01,022 : ***** Transfer task : SICK-Entailment*****


2019-02-13 05:30:01,085 : loading BERT mode bert-base-uncased
2019-02-13 05:30:01,085 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:30:01,105 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:30:01,105 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv3zlea6x
2019-02-13 05:30:03,544 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:30:05,092 : Computing embedding for train
2019-02-13 05:30:22,874 : Computed train embeddings
2019-02-13 05:30:22,874 : Computing embedding for dev
2019-02-13 05:30:25,018 : Computed dev embeddings
2019-02-13 05:30:25,018 : Computing embedding for test
2019-02-13 05:30:45,666 : Computed test embeddings
2019-02-13 05:30:45,694 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:30:50,726 : [('reg:1e-05', 79.2), ('reg:0.0001', 79.2), ('reg:0.001', 78.8), ('reg:0.01', 76.2)]
2019-02-13 05:30:50,726 : Validation : best param found is reg = 1e-05 with score             79.2
2019-02-13 05:30:50,726 : Evaluating...
2019-02-13 05:30:52,107 : 
Dev acc : 79.2 Test acc : 77.15 for                        SICK entailment

2019-02-13 05:30:52,108 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 05:30:52,135 : loading BERT mode bert-base-uncased
2019-02-13 05:30:52,136 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:30:52,191 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:30:52,192 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpace1gkqh
2019-02-13 05:30:54,623 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:30:56,164 : Computing embedding for train
2019-02-13 05:31:13,600 : Computed train embeddings
2019-02-13 05:31:13,600 : Computing embedding for dev
2019-02-13 05:31:15,731 : Computed dev embeddings
2019-02-13 05:31:15,731 : Computing embedding for test
2019-02-13 05:31:32,982 : Computed test embeddings
2019-02-13 05:32:17,506 : Dev : Pearson 0.8171522572469139
2019-02-13 05:32:17,506 : Test : Pearson 0.8094042913214028 Spearman 0.7361093992751818 MSE 0.35301990357161456                        for SICK Relatedness

2019-02-13 05:32:17,507 : 

***** Transfer task : STSBenchmark*****


2019-02-13 05:32:17,546 : loading BERT mode bert-base-uncased
2019-02-13 05:32:17,547 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:32:17,576 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:32:17,577 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcpc8ua0e
2019-02-13 05:32:20,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:32:21,454 : Computing embedding for train
2019-02-13 05:32:34,391 : Computed train embeddings
2019-02-13 05:32:34,391 : Computing embedding for dev
2019-02-13 05:32:38,090 : Computed dev embeddings
2019-02-13 05:32:38,090 : Computing embedding for test
2019-02-13 05:32:40,725 : Computed test embeddings
2019-02-13 05:33:43,323 : Dev : Pearson 0.7140077083391813
2019-02-13 05:33:43,323 : Test : Pearson 0.6605640727130131 Spearman 0.651771579777153 MSE 1.4713919112985543                        for SICK Relatedness

2019-02-13 05:33:43,323 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 05:33:43,640 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 05:33:43,650 : loading BERT mode bert-base-uncased
2019-02-13 05:33:43,650 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:33:43,673 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:33:43,673 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn9uxkgc2
2019-02-13 05:33:46,102 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:33:47,558 : Computing embeddings for train/dev/test
2019-02-13 05:37:11,554 : Computed embeddings
2019-02-13 05:37:11,554 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:38:08,034 : [('reg:1e-05', 77.03), ('reg:0.0001', 76.26), ('reg:0.001', 70.35), ('reg:0.01', 52.81)]
2019-02-13 05:38:08,035 : Validation : best param found is reg = 1e-05 with score             77.03
2019-02-13 05:38:08,035 : Evaluating...
2019-02-13 05:38:22,857 : 
Dev acc : 77.0 Test acc : 77.6 for LENGTH classification

2019-02-13 05:38:22,864 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 05:38:23,212 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 05:38:23,257 : loading BERT mode bert-base-uncased
2019-02-13 05:38:23,257 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:38:23,286 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:38:23,286 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcx1lwr7o
2019-02-13 05:38:25,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:38:28,000 : Computing embeddings for train/dev/test
2019-02-13 05:41:22,778 : Computed embeddings
2019-02-13 05:41:22,779 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:42:12,223 : [('reg:1e-05', 45.09), ('reg:0.0001', 17.89), ('reg:0.001', 2.02), ('reg:0.01', 0.82)]
2019-02-13 05:42:12,223 : Validation : best param found is reg = 1e-05 with score             45.09
2019-02-13 05:42:12,223 : Evaluating...
2019-02-13 05:42:27,583 : 
Dev acc : 45.1 Test acc : 45.0 for WORDCONTENT classification

2019-02-13 05:42:27,590 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 05:42:27,949 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 05:42:28,017 : loading BERT mode bert-base-uncased
2019-02-13 05:42:28,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:42:28,128 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:42:28,128 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplm5oj491
2019-02-13 05:42:30,609 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:42:32,100 : Computing embeddings for train/dev/test
2019-02-13 05:45:18,481 : Computed embeddings
2019-02-13 05:45:18,481 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:45:54,423 : [('reg:1e-05', 33.96), ('reg:0.0001', 33.88), ('reg:0.001', 32.65), ('reg:0.01', 28.1)]
2019-02-13 05:45:54,423 : Validation : best param found is reg = 1e-05 with score             33.96
2019-02-13 05:45:54,423 : Evaluating...
2019-02-13 05:46:02,868 : 
Dev acc : 34.0 Test acc : 33.8 for DEPTH classification

2019-02-13 05:46:02,875 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 05:46:03,440 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 05:46:03,503 : loading BERT mode bert-base-uncased
2019-02-13 05:46:03,503 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:46:03,530 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:46:03,530 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6br0wnwf
2019-02-13 05:46:05,960 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:46:07,371 : Computing embeddings for train/dev/test
2019-02-13 05:48:17,691 : Computed embeddings
2019-02-13 05:48:17,691 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:49:51,849 : [('reg:1e-05', 67.37), ('reg:0.0001', 65.0), ('reg:0.001', 58.57), ('reg:0.01', 47.12)]
2019-02-13 05:49:51,850 : Validation : best param found is reg = 1e-05 with score             67.37
2019-02-13 05:49:51,850 : Evaluating...
2019-02-13 05:50:03,528 : 
Dev acc : 67.4 Test acc : 66.8 for TOPCONSTITUENTS classification

2019-02-13 05:50:03,530 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 05:50:03,876 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 05:50:03,943 : loading BERT mode bert-base-uncased
2019-02-13 05:50:03,943 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:50:04,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:50:04,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa34ysc99
2019-02-13 05:50:06,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:50:07,946 : Computing embeddings for train/dev/test
2019-02-13 05:52:53,436 : Computed embeddings
2019-02-13 05:52:53,437 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:54:04,511 : [('reg:1e-05', 88.51), ('reg:0.0001', 88.49), ('reg:0.001', 88.21), ('reg:0.01', 86.35)]
2019-02-13 05:54:04,511 : Validation : best param found is reg = 1e-05 with score             88.51
2019-02-13 05:54:04,511 : Evaluating...
2019-02-13 05:54:14,981 : 
Dev acc : 88.5 Test acc : 88.2 for BIGRAMSHIFT classification

2019-02-13 05:54:14,989 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 05:54:15,579 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 05:54:15,645 : loading BERT mode bert-base-uncased
2019-02-13 05:54:15,646 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:54:15,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:54:15,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiui84bnf
2019-02-13 05:54:18,128 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:54:19,610 : Computing embeddings for train/dev/test
2019-02-13 05:56:58,393 : Computed embeddings
2019-02-13 05:56:58,393 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 05:58:24,346 : [('reg:1e-05', 89.73), ('reg:0.0001', 89.83), ('reg:0.001', 90.14), ('reg:0.01', 90.04)]
2019-02-13 05:58:24,346 : Validation : best param found is reg = 0.001 with score             90.14
2019-02-13 05:58:24,346 : Evaluating...
2019-02-13 05:58:40,669 : 
Dev acc : 90.1 Test acc : 88.2 for TENSE classification

2019-02-13 05:58:40,677 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 05:58:41,259 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 05:58:41,323 : loading BERT mode bert-base-uncased
2019-02-13 05:58:41,323 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 05:58:41,353 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 05:58:41,353 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp0600gx_
2019-02-13 05:58:43,790 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 05:58:45,281 : Computing embeddings for train/dev/test
2019-02-13 06:01:06,260 : Computed embeddings
2019-02-13 06:01:06,260 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:02:51,925 : [('reg:1e-05', 84.6), ('reg:0.0001', 84.43), ('reg:0.001', 84.13), ('reg:0.01', 83.17)]
2019-02-13 06:02:51,925 : Validation : best param found is reg = 1e-05 with score             84.6
2019-02-13 06:02:51,925 : Evaluating...
2019-02-13 06:03:16,869 : 
Dev acc : 84.6 Test acc : 84.6 for SUBJNUMBER classification

2019-02-13 06:03:16,876 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 06:03:17,308 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 06:03:17,375 : loading BERT mode bert-base-uncased
2019-02-13 06:03:17,375 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:03:17,402 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:03:17,402 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdo0figjl
2019-02-13 06:03:19,830 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:03:21,318 : Computing embeddings for train/dev/test
2019-02-13 06:06:06,250 : Computed embeddings
2019-02-13 06:06:06,250 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:07:34,171 : [('reg:1e-05', 80.0), ('reg:0.0001', 80.05), ('reg:0.001', 80.03), ('reg:0.01', 78.52)]
2019-02-13 06:07:34,171 : Validation : best param found is reg = 0.0001 with score             80.05
2019-02-13 06:07:34,171 : Evaluating...
2019-02-13 06:07:55,630 : 
Dev acc : 80.0 Test acc : 80.4 for OBJNUMBER classification

2019-02-13 06:07:55,637 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 06:07:56,033 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 06:07:56,103 : loading BERT mode bert-base-uncased
2019-02-13 06:07:56,103 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:07:56,229 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:07:56,230 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq3y5kq0g
2019-02-13 06:07:58,667 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:08:00,147 : Computing embeddings for train/dev/test
2019-02-13 06:10:52,844 : Computed embeddings
2019-02-13 06:10:52,844 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:11:42,956 : [('reg:1e-05', 64.68), ('reg:0.0001', 64.57), ('reg:0.001', 64.32), ('reg:0.01', 65.45)]
2019-02-13 06:11:42,956 : Validation : best param found is reg = 0.01 with score             65.45
2019-02-13 06:11:42,956 : Evaluating...
2019-02-13 06:12:04,947 : 
Dev acc : 65.5 Test acc : 64.5 for ODDMANOUT classification

2019-02-13 06:12:04,954 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 06:12:05,355 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 06:12:05,432 : loading BERT mode bert-base-uncased
2019-02-13 06:12:05,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 06:12:05,559 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 06:12:05,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp33bfe9qk
2019-02-13 06:12:08,005 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 06:12:09,468 : Computing embeddings for train/dev/test
2019-02-13 06:14:35,340 : Computed embeddings
2019-02-13 06:14:35,340 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 06:15:49,277 : [('reg:1e-05', 69.08), ('reg:0.0001', 69.01), ('reg:0.001', 69.13), ('reg:0.01', 66.42)]
2019-02-13 06:15:49,277 : Validation : best param found is reg = 0.001 with score             69.13
2019-02-13 06:15:49,277 : Evaluating...
2019-02-13 06:16:10,150 : 
Dev acc : 69.1 Test acc : 68.1 for COORDINATIONINVERSION classification

2019-02-13 06:16:10,159 : {'STS12': {'MSRpar': {'pearson': (0.35080611007412454, 3.8509397697871774e-23), 'spearman': SpearmanrResult(correlation=0.4008028075384826, pvalue=2.600350039258681e-30), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3574974469821697, 4.980765183966073e-24), 'spearman': SpearmanrResult(correlation=0.3876519061632382, pvalue=2.6515414058605344e-28), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47895325858574384, 1.0696520322533151e-27), 'spearman': SpearmanrResult(correlation=0.5790452258483904, pvalue=1.8972858221541758e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.579227905979551, 2.0547811960456315e-68), 'spearman': SpearmanrResult(correlation=0.5796082436605509, pvalue=1.6021836917087366e-68), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5833381830425967, 9.477792856074256e-38), 'spearman': SpearmanrResult(correlation=0.5418387768776708, pvalue=8.011118562566017e-32), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4699645809328371, 'wmean': 0.45631920141626003}, 'spearman': {'mean': 0.49778939201766653, 'wmean': 0.48520677241322574}}}, 'STS13': {'FNWN': {'pearson': (0.42027695078457045, 1.7369187958770378e-09), 'spearman': SpearmanrResult(correlation=0.44706410341349023, pvalue=1.127214813064322e-10), 'nsamples': 189}, 'headlines': {'pearson': (0.6471272790637843, 3.297436370324476e-90), 'spearman': SpearmanrResult(correlation=0.6214995473872796, pvalue=2.378033101697607e-81), 'nsamples': 750}, 'OnWN': {'pearson': (0.5584891342237713, 2.5186058231640175e-47), 'spearman': SpearmanrResult(correlation=0.5709919911435829, pvalue=7.490799700775691e-50), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5419644546907088, 'wmean': 0.5853934715304385}, 'spearman': {'mean': 0.5465185473147843, 'wmean': 0.5806308554114396}}}, 'STS14': {'deft-forum': {'pearson': (0.33517344658242065, 2.8221797349044877e-13), 'spearman': SpearmanrResult(correlation=0.3277425344689775, pvalue=9.961699853695555e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7512845294591148, 1.0172324277558216e-55), 'spearman': SpearmanrResult(correlation=0.7167636208178597, pvalue=1.4118964428312582e-48), 'nsamples': 300}, 'headlines': {'pearson': (0.599546608117327, 2.1640877173086343e-74), 'spearman': SpearmanrResult(correlation=0.5421053873616071, pvalue=1.6231026312930347e-58), 'nsamples': 750}, 'images': {'pearson': (0.45178731422362517, 5.3254700661549696e-39), 'spearman': SpearmanrResult(correlation=0.4449333417950941, pvalue=9.602813917283093e-38), 'nsamples': 750}, 'OnWN': {'pearson': (0.6977044107045871, 1.8641386901213634e-110), 'spearman': SpearmanrResult(correlation=0.7181222547272003, pvalue=6.935189463119958e-120), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6613557625942664, 1.662782500912532e-95), 'spearman': SpearmanrResult(correlation=0.5932714516885115, pvalue=1.685963704398521e-72), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5828086786135568, 'wmean': 0.5824023950745808}, 'spearman': {'mean': 0.5571564318098751, 'wmean': 0.5563566809161886}}}, 'STS15': {'answers-forums': {'pearson': (0.604621376837723, 9.490032616618947e-39), 'spearman': SpearmanrResult(correlation=0.6016348976758708, pvalue=2.7415764251583826e-38), 'nsamples': 375}, 'answers-students': {'pearson': (0.5641964122454406, 2.9640317150703668e-64), 'spearman': SpearmanrResult(correlation=0.5679701297522847, pvalue=2.8041053842547244e-65), 'nsamples': 750}, 'belief': {'pearson': (0.7071331680987738, 4.140029596376785e-58), 'spearman': SpearmanrResult(correlation=0.7153901150377215, pvalue=4.857987485564283e-60), 'nsamples': 375}, 'headlines': {'pearson': (0.6711056834764186, 2.629637991690932e-99), 'spearman': SpearmanrResult(correlation=0.6600259181469976, pvalue=5.3484735708935256e-95), 'nsamples': 750}, 'images': {'pearson': (0.6187577796030145, 1.8861846688906345e-80), 'spearman': SpearmanrResult(correlation=0.6244125179508605, pvalue=2.5754458030205485e-82), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6331628840522742, 'wmean': 0.6274842869482805}, 'spearman': {'mean': 0.633886715712747, 'wmean': 0.6277302680517347}}}, 'STS16': {'answer-answer': {'pearson': (0.553468988693486, 8.696745208819695e-22), 'spearman': SpearmanrResult(correlation=0.5656283486651251, pvalue=7.015113546336354e-23), 'nsamples': 254}, 'headlines': {'pearson': (0.6537779600151308, 9.542377841938514e-32), 'spearman': SpearmanrResult(correlation=0.6574766500311147, pvalue=3.318975284972055e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7692302676644358, 3.0545856657446224e-46), 'spearman': SpearmanrResult(correlation=0.7857314358961456, pvalue=1.829635252464683e-49), 'nsamples': 230}, 'postediting': {'pearson': (0.8057528659945581, 5.610354568038406e-57), 'spearman': SpearmanrResult(correlation=0.824265134032028, pvalue=1.0283332861930353e-61), 'nsamples': 244}, 'question-question': {'pearson': (0.5274997481312688, 2.2836776102210805e-16), 'spearman': SpearmanrResult(correlation=0.5162063366811672, pvalue=1.2470251147478226e-15), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6619459660997759, 'wmean': 0.6636980972991912}, 'spearman': {'mean': 0.6698615810611162, 'wmean': 0.6720972459907097}}}, 'MR': {'devacc': 80.47, 'acc': 79.53, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 86.83, 'acc': 85.72, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.56, 'acc': 87.63, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.43, 'acc': 95.05, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.24, 'acc': 85.06, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 44.87, 'acc': 46.38, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.37, 'acc': 89.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.48, 'acc': 73.45, 'f1': 80.79, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.2, 'acc': 77.15, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8171522572469139, 'pearson': 0.8094042913214028, 'spearman': 0.7361093992751818, 'mse': 0.35301990357161456, 'yhat': array([3.32618537, 4.0652178 , 2.40994995, ..., 3.43443938, 4.50666863,
       4.73824627]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7140077083391813, 'pearson': 0.6605640727130131, 'spearman': 0.651771579777153, 'mse': 1.4713919112985543, 'yhat': array([2.14575773, 1.58156135, 2.25255849, ..., 3.84748113, 3.76941291,
       3.73962837]), 'ndev': 1500, 'ntest': 1379}, 'Length': {'devacc': 77.03, 'acc': 77.61, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 45.09, 'acc': 45.03, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 33.96, 'acc': 33.77, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 67.37, 'acc': 66.77, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.51, 'acc': 88.23, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.14, 'acc': 88.17, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.6, 'acc': 84.61, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.05, 'acc': 80.4, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.45, 'acc': 64.51, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.13, 'acc': 68.1, 'ndev': 10002, 'ntest': 10002}}
